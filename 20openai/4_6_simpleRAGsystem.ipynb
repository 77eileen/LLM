{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b3f7ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "래퍼 클래스 테스트: \n",
      "질문: VectorDB의 종류를 알려주세요\n",
      "답변: 주요 VectorDB 솔루션은 다음과 같습니다:\n",
      "\n",
      "- ChromaDB: 로컬 개발에 적합한 오픈소스 솔루션으로, 파이썬 네이티브로 설치가 간편합니다.\n",
      "- Pinecone: 완전 관리형 클라우드 서비스로, 대규모 프로덕션 환경에 적합합니다.\n",
      "- Weaviate: 그래프 기반 벡터 데이터베이스로, 하이브리드 검색을 지원합니다.\n",
      "출처: ['vectordb_intro.txt', 'vectordb_intro.txt', 'rag_concept.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle    # chunk, vectorDB 저장한것 사용\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 경고메세지 삭제\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# openapi key 확인\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError('.env확인,  key없음')\n",
    "\n",
    "# 필수 라이브러리 로드\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "\n",
    "class SimpleRAGSystem:\n",
    "    '''간단한 RAG 시스템 래퍼 클래스'''\n",
    "    def __init__(self, vectorstore, llm, retriever_k=3):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.retriever = vectorstore.as_retriever(search_kwargs={'k':retriever_k})\n",
    "        self.chain = self._build_chain()\n",
    "    \n",
    "    \n",
    "    def _build_chain(self):\n",
    "        '''RAG 체인 구성'''\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            ('system', \"당신은 제공된 문맥을 바탕으로 질문에 답변하는 AI입니다.\\\n",
    "            문맥에 없는 정보는 답하지 마세요\"),\n",
    "            ('human', '문맥:\\n{context}\\n\\n질문:{question}\\n\\n답변:')\n",
    "        ])\n",
    "        return(\n",
    "            {'context': self.retriever | self._format_docs, 'question':RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @staticmethod   # 클래스 안에 있지만, 그 클래스의 인스턴스(self)도, 클래스(cls)도 필요 없이 쓸 수 있는 함수를 만들 때 사용하는 데코레이터\n",
    "    def _format_docs(docs):\n",
    "        return '\\n\\n'.join([doc.page_content for doc in docs])\n",
    "    \n",
    "    def ask(self, question:str) -> str:\n",
    "        '''질문에 답변'''\n",
    "        return self.chain.invoke(question)\n",
    "    \n",
    "    def ask_with_sources(self, question:str) -> dict : \n",
    "        '''질문에 답변 + 출처 반환'''\n",
    "        answer = self.chain.invoke(question)\n",
    "        sources = self.retriever.invoke(question)\n",
    "        return {\n",
    "            'answer' : answer,\n",
    "            'source' : [ doc.metadata.get('source', 'unknown') for doc in sources]\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    # 이전단계에서 저장한 vectordb로드\n",
    "    persist_dir = './chroma_db_reg2'\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory = persist_dir,\n",
    "        collection_name = 'persistent_rag',\n",
    "        embedding_function = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
    "    )\n",
    "    llm = ChatOpenAI( model = 'gpt-4o-mini', temperature=0 )\n",
    "\n",
    "    rag_system = SimpleRAGSystem(vectorstore, llm)\n",
    "\n",
    "    print('래퍼 클래스 테스트: ')\n",
    "    result = rag_system.ask_with_sources(\"VectorDB의 종류를 알려주세요\")\n",
    "    print(f'질문: VectorDB의 종류를 알려주세요')\n",
    "    print(f\"답변: {result['answer']}\")  #일부만 보고 싶으면 ['answer'][:100]\n",
    "    print(f\"출처: {result['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8bc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c2947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068ddad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c171e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e997083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b742ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- 문서 1 ----\n",
      "source: langchain_intro.txt\n",
      "page: 없음\n",
      "내용: LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n",
      "\n",
      "        LangChain의 주요 구성 요소:\n",
      "        1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합\n",
      "        2. Prompts: 프롬프트 템플릿 관리 및 최적화\n",
      "        3. Ch ...\n",
      "\n",
      "---- 문서 2 ----\n",
      "source: langchain_intro.txt\n",
      "page: 없음\n",
      "내용: 4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\n",
      "        5. Indexes: 문서 검색을 위한 인덱싱 도구\n",
      "        6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트\n",
      "\n",
      "        LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로,\n",
      "        파이프(|) 연산 ...\n",
      "\n",
      "---- 문서 3 ----\n",
      "source: rag_concept.txt\n",
      "page: 없음\n",
      "내용: RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\n",
      "\n",
      "        RAG의 작동 원리:\n",
      "        1. 사용자 질문을 임베딩 벡터로 변환합니다.\n",
      "        2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\n",
      "        3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.\n",
      "\n",
      "       ...\n",
      "\n",
      "---- 문서 4 ----\n",
      "source: rag_concept.txt\n",
      "page: 없음\n",
      "내용: - 환각(Hallucination)을 감소시킵니다. 실제 문서 기반으로 답변하기 때문입니다.\n",
      "        - 출처를 명시할 수 있습니다. 어떤 문서에서 정보를 가져왔는지 추적 가능합니다.\n",
      "        - 도메인 특화가 가능합니다. 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.\n",
      "\n",
      "        RAG의 핵심 구성요소: Retriever(검색기) ...\n",
      "\n",
      "---- 문서 5 ----\n",
      "source: vectordb_intro.txt\n",
      "page: 없음\n",
      "내용: VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.\n",
      "\n",
      "        주요 VectorDB 솔루션:\n",
      "        - ChromaDB: 로컬 개발에 적합한 오픈소스 솔루션. 파이썬 네이티브로 설치가 간편합니다.\n",
      "        - Pinecone: 완전 관리형 클라우드 서비스. 대규모 프로덕션 환경에 적합합니다. ...\n",
      "\n",
      "---- 문서 6 ----\n",
      "source: vectordb_intro.txt\n",
      "page: 없음\n",
      "내용: - FAISS: Facebook에서 개발한 고성능 라이브러리. 대용량 벡터 검색에 최적화되어 있습니다.\n",
      "        - Milvus: 분산 환경을 지원하는 오픈소스 솔루션입니다.\n",
      "\n",
      "        임베딩(Embedding)은 텍스트를 숫자 벡터로 변환하는 과정으로,\n",
      "        의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치됩니다.\n",
      "      ...\n",
      "\n",
      "---- 문서 7 ----\n",
      "source: langchain_intro.txt\n",
      "page: 없음\n",
      "내용: LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n",
      "\n",
      "        LangChain의 주요 구성 요소:\n",
      "        1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합\n",
      "        2. Prompts: 프롬프트 템플릿 관리 및 최적화\n",
      "        3. Ch ...\n",
      "\n",
      "---- 문서 8 ----\n",
      "source: langchain_intro.txt\n",
      "page: 없음\n",
      "내용: 4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\n",
      "        5. Indexes: 문서 검색을 위한 인덱싱 도구\n",
      "        6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트\n",
      "\n",
      "        LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로,\n",
      "        파이프(|) 연산 ...\n",
      "\n",
      "---- 문서 9 ----\n",
      "source: rag_concept.txt\n",
      "page: 없음\n",
      "내용: RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\n",
      "\n",
      "        RAG의 작동 원리:\n",
      "        1. 사용자 질문을 임베딩 벡터로 변환합니다.\n",
      "        2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\n",
      "        3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.\n",
      "\n",
      "       ...\n",
      "\n",
      "---- 문서 10 ----\n",
      "source: rag_concept.txt\n",
      "page: 없음\n",
      "내용: - 환각(Hallucination)을 감소시킵니다. 실제 문서 기반으로 답변하기 때문입니다.\n",
      "        - 출처를 명시할 수 있습니다. 어떤 문서에서 정보를 가져왔는지 추적 가능합니다.\n",
      "        - 도메인 특화가 가능합니다. 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.\n",
      "\n",
      "        RAG의 핵심 구성요소: Retriever(검색기) ...\n",
      "\n",
      "---- 문서 11 ----\n",
      "source: vectordb_intro.txt\n",
      "page: 없음\n",
      "내용: VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.\n",
      "\n",
      "        주요 VectorDB 솔루션:\n",
      "        - ChromaDB: 로컬 개발에 적합한 오픈소스 솔루션. 파이썬 네이티브로 설치가 간편합니다.\n",
      "        - Pinecone: 완전 관리형 클라우드 서비스. 대규모 프로덕션 환경에 적합합니다. ...\n",
      "\n",
      "---- 문서 12 ----\n",
      "source: vectordb_intro.txt\n",
      "page: 없음\n",
      "내용: - FAISS: Facebook에서 개발한 고성능 라이브러리. 대용량 벡터 검색에 최적화되어 있습니다.\n",
      "        - Milvus: 분산 환경을 지원하는 오픈소스 솔루션입니다.\n",
      "\n",
      "        임베딩(Embedding)은 텍스트를 숫자 벡터로 변환하는 과정으로,\n",
      "        의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치됩니다.\n",
      "      ...\n"
     ]
    }
   ],
   "source": [
    "items = vectorstore._collection.get()\n",
    "metas = items['metadatas']\n",
    "contents = items['documents']\n",
    "\n",
    "for i, (meta, content) in enumerate(zip(metas, contents)):\n",
    "    print(f\"\\n---- 문서 {i+1} ----\")\n",
    "    print(\"source:\", meta.get(\"source\", \"없음\"))\n",
    "    print(\"page:\", meta.get(\"page\", \"없음\"))\n",
    "    print(\"내용:\", content[:200], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
