{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r 20251127_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking : ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°(chunck)ìœ¼ë¡œ ë¶„í• \n",
    "# LLMì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œí•œ\n",
    "# ê²€ìƒ‰ì˜ ì •í™•ë„ë¥¼ í–¥ìƒ\n",
    "# ê²€ìƒ‰ì˜ ì •í™•ë„ í–¥ìƒ(í°ë¬¸ì„œ ê´€ë ¨ì—†ëŠ” ì •ë³´ í¬í•¨) : ì‘ì€ ì²­í¬ --> ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¶€ë¶„ë¬¸ ê²€ìƒ‰\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "spliiter = CharacterTextSplitter(\n",
    "    separator= '\\n',  #ë¶„í• ê¸°ì¤€\n",
    "    chunk_size = 1000, # ìµœëŒ€í¬ê¸°\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "    # ìš°ë¦¬ëŠ” ì˜¤ëŠ˜ ì‹ë‹¹ì— ê°€ì„œ ì ì‹¬ì„ ë¨¹ê³  ê·¸ë¦¬ê³  ì¹´í˜ì— ê°€ì„œ ì»¤í”¼ë¥¼ ì‹œì¼°ë‹¤. ê·¸ë¦¬ê³  10ë¶„ì— ì˜ì² ì´ê°€ ì™€ì„œ ... \n",
    "    # ---> \"ìš°ë¦¬ëŠ” ì˜¤ëŠ˜\" // \"ì˜¤ëŠ˜ ì‹ë‹¹ì—\"  // \"ì‹ë‹¹ì— ê°€ì„œ\" ==> overlab\n",
    "\n",
    "# ê¶Œì¥ : ì—¬ëŸ¬êµ¬ë¶„ìë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì‹œë„\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "spliiter = RecursiveCharacterTextSplitter(\n",
    "    separators= ['\\n', '\\n\\n', '.'],  #ë¶„í• ê¸°ì¤€\n",
    "    chunk_size = 1000, # ìµœëŒ€í¬ê¸°\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96efee1a",
   "metadata": {},
   "source": [
    "- ê²°ë¡ ë¶€í„° ë§í•˜ë©´ CharacterTextSplitterëŠ” separatorë¥¼ í•˜ë‚˜ë§Œ ê°€ì§ˆ ìˆ˜ ìˆì–´.\n",
    "- ğŸ‘‰ RecursiveCharacterTextSplitter ì²˜ëŸ¼ ['\\n\\n', '\\n', '.', ' '] ì—¬ëŸ¬ êµ¬ë¶„ìë¥¼ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ì–´."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befb71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ˆì œ Document ê°ì²´\n",
      "page_content: ì´ê²ƒì€ ì˜ˆì œ ë¬¸ì„œì˜ ë‚´ìš©ì…ì´ë‹¤.\n",
      "page_metadata: {'source': 'example.txt', 'page': 1, 'author': 'í™ê¸¸ë™'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (RecursiveCharacterTextSplitter, CharacterTextSplitter)\n",
    "\n",
    "# ìƒ˜í”Œ Document ê°ì²´ ìƒì„±\n",
    "example_doc = Document(\n",
    "    page_content= 'ì´ê²ƒì€ ì˜ˆì œ ë¬¸ì„œì˜ ë‚´ìš©ì…ì´ë‹¤.',\n",
    "    metadata = {'source' : 'example.txt', 'page':1, 'author':'í™ê¸¸ë™'}\n",
    ")\n",
    "print('ì˜ˆì œ Document ê°ì²´')\n",
    "print(f'page_content: {example_doc.page_content}')\n",
    "print(f'page_metadata: {example_doc.metadata}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be22081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
      "ì›ë³¸ ë¬¸ì„œê¸¸ì´ : 485ì\n",
      "CharacterTextSplitter ê²°ê³¼ : 4ê°œ ì²­í¬\n",
      "ì²­í¬ë³„ ë¯¸ë¦¬ë³´ê¸°\n",
      "chunk 1 ( 185ì : LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.         LangChainì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ:         1. Models: ë‹¤ì–‘í•œ LLM ì œê³µì(OpenAI, Anthropic, Google ë“±)ì™€ í†µí•©         2. Prompts: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬ ë° ìµœì í™”)\n",
      "chunk 2 ( 148ì : 2. Prompts: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬ ë° ìµœì í™”         3. Chains: ì—¬ëŸ¬ êµ¬ì„± ìš”ì†Œë¥¼ ì—°ê²°í•˜ëŠ” íŒŒì´í”„ë¼ì¸         4. Memory: ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ         5. Indexes: ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ì‹± ë„êµ¬)\n",
      "chunk 3 ( 189ì : 5. Indexes: ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ì‹± ë„êµ¬         6. Agents: ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸         LangChain Expression Language (LCEL)ì€ ì²´ì¸ì„ êµ¬ì„±í•˜ëŠ” ì„ ì–¸ì  ë°©ì‹ìœ¼ë¡œ,         íŒŒì´í”„(|) ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì§ê´€ì ìœ¼ë¡œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n",
      "chunk 4 ( 41ì : íŒŒì´í”„(|) ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì§ê´€ì ìœ¼ë¡œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ ë¬¸ì„œ ìƒì„± (ì™¸ë¶€ë¬¸ì„œ ì‹œë®¬ë ˆì´ì…˜)\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "        \n",
    "        LangChainì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ:\n",
    "        1. Models: ë‹¤ì–‘í•œ LLM ì œê³µì(OpenAI, Anthropic, Google ë“±)ì™€ í†µí•©\n",
    "        2. Prompts: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬ ë° ìµœì í™”\n",
    "        3. Chains: ì—¬ëŸ¬ êµ¬ì„± ìš”ì†Œë¥¼ ì—°ê²°í•˜ëŠ” íŒŒì´í”„ë¼ì¸\n",
    "        4. Memory: ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ\n",
    "        5. Indexes: ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ì‹± ë„êµ¬\n",
    "        6. Agents: ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸\n",
    "        \n",
    "        LangChain Expression Language (LCEL)ì€ ì²´ì¸ì„ êµ¬ì„±í•˜ëŠ” ì„ ì–¸ì  ë°©ì‹ìœ¼ë¡œ,\n",
    "        íŒŒì´í”„(|) ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì§ê´€ì ìœ¼ë¡œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"langchain_intro.txt\", \"topic\": \"framework\", \"importance\": \"high\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        RAG (Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "        \n",
    "        RAGì˜ ì‘ë™ ì›ë¦¬:\n",
    "        1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "        2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "        3. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        RAGì˜ ì¥ì :\n",
    "        - ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LLMì˜ í•™ìŠµ ë°ì´í„° ì´í›„ ì •ë³´ë„ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "        - í™˜ê°(Hallucination)ì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ì‹¤ì œ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "        - ì¶œì²˜ë¥¼ ëª…ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ ì¶”ì  ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "        - ë„ë©”ì¸ íŠ¹í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŠ¹ì • ë¶„ì•¼ì˜ ë¬¸ì„œë§Œ ì‚¬ìš©í•˜ì—¬ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "        \n",
    "        RAGì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œ: Retriever(ê²€ìƒ‰ê¸°), Generator(ìƒì„±ê¸°), VectorStore(ë²¡í„°ì €ì¥ì†Œ)\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"rag_concept.txt\", \"topic\": \"technique\", \"importance\": \"high\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        VectorDB(ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤)ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.\n",
    "        \n",
    "        ì£¼ìš” VectorDB ì†”ë£¨ì…˜:\n",
    "        - ChromaDB: ë¡œì»¬ ê°œë°œì— ì í•©í•œ ì˜¤í”ˆì†ŒìŠ¤ ì†”ë£¨ì…˜. íŒŒì´ì¬ ë„¤ì´í‹°ë¸Œë¡œ ì„¤ì¹˜ê°€ ê°„í¸í•©ë‹ˆë‹¤.\n",
    "        - Pinecone: ì™„ì „ ê´€ë¦¬í˜• í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤. ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ í™˜ê²½ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "        - Weaviate: ê·¸ë˜í”„ ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "        - FAISS: Facebookì—ì„œ ê°œë°œí•œ ê³ ì„±ëŠ¥ ë¼ì´ë¸ŒëŸ¬ë¦¬. ëŒ€ìš©ëŸ‰ ë²¡í„° ê²€ìƒ‰ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "        - Milvus: ë¶„ì‚° í™˜ê²½ì„ ì§€ì›í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.\n",
    "        \n",
    "        ì„ë² ë”©(Embedding)ì€ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ,\n",
    "        ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë°°ì¹˜ë©ë‹ˆë‹¤.\n",
    "        ì˜ˆë¥¼ ë“¤ì–´, \"ê³ ì–‘ì´\"ì™€ \"ê°•ì•„ì§€\"ëŠ” \"ìë™ì°¨\"ë³´ë‹¤ ë²¡í„° ê³µê°„ì—ì„œ ë” ê°€ê¹ìŠµë‹ˆë‹¤.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"vectordb_intro.txt\", \"topic\": \"database\", \"importance\": \"medium\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "print('í…ìŠ¤íŠ¸ ë¶„í• ê¸°')\n",
    "\n",
    "# ë‹¨ìˆœ ë¬¸ìê¸°ë°˜ ë¶„í• ê¸°\n",
    "char_spl = CharacterTextSplitter(\n",
    "    separator='\\n', #ë¶„í• ê¸°ì¤€\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 70,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "# ì²«ë²ˆì§¸ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸\n",
    "test_doc = sample_documents[0]\n",
    "char_splits = char_spl.split_documents( [test_doc] )\n",
    "print(f'ì›ë³¸ ë¬¸ì„œê¸¸ì´ : {len(test_doc.page_content)}ì')\n",
    "print(f'CharacterTextSplitter ê²°ê³¼ : {len(char_splits)}ê°œ ì²­í¬')\n",
    "print(f'ì²­í¬ë³„ ë¯¸ë¦¬ë³´ê¸°')\n",
    "for i, chunk in enumerate(char_splits[:],1):  # [:3] ì²­í¬3ê°œë§Œ ë´„ / ì „ë¶€ë³´ë ¤ë©´ [:] ì´ë ‡ê²Œ ì“°ë˜ê°€, ì•„ì˜ˆ ì‚­ì œí•´ë„ë¨\n",
    "    preview = chunk.page_content.strip()[:].replace('\\n', ' ')  # [:80] ì´ë ‡ê²Œí•˜ë©´ 80ìë§Œ print ì¶œë ¥ë¨. / ì „ë¶€ë³´ë ¤ë©´ [:]ì´ë ‡ê²Œ ì“°ë˜ê°€, ì•„ì˜ˆ ì‚­ì œí•´ë„ë¨\n",
    "    print(f'chunk {i} ( {len(chunk.page_content)}ì : {preview})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecursiveCharacterTextSplitter\n",
    "print('RecursiveCharacterTextSplitter ì ìš©')\n",
    "recursive_spl = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap=50,\n",
    "    separators=['\\n', '\\n\\n', '.', ',', ' '],\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "\n",
    "# ëª¨ë“  ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• \n",
    "doc_rec_splits = recursive_spl.split_documents(sample_documents)\n",
    "print(f'ì›ë³¸ ë¬¸ì„œê¸¸ì´ : {len(sample_documents)}ê°œ')\n",
    "print(f'RecursiveCharacterTextSplitter ê²°ê³¼ : {len(doc_rec_splits)}ê°œ ì²­í¬')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b065bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveCharacterTextSplitter ì ìš©\n",
      "ì›ë³¸ ë¬¸ì„œê¸¸ì´ : 501ì\n",
      "RecursiveCharacterTextSplitter ê²°ê³¼ : 4ê°œ ì²­í¬\n"
     ]
    }
   ],
   "source": [
    "############ DJ ê°œì¸#########\n",
    "# RecursiveCharacterTextSplitter\n",
    "print('RecursiveCharacterTextSplitter ì ìš©')\n",
    "recursive_spl = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap=70,\n",
    "    separators=['\\n', '\\n\\n', '.', ',', ' '],\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "\n",
    "# ì²«ë²ˆì§¸ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸\n",
    "test_doc_2 = sample_documents[0]\n",
    "doc_rec_splits = recursive_spl.split_documents([test_doc_2])\n",
    "print(f'ì›ë³¸ ë¬¸ì„œê¸¸ì´ : {len(test_doc_2.page_content)}ì')\n",
    "print(f'RecursiveCharacterTextSplitter ê²°ê³¼ : {len(doc_rec_splits)}ê°œ ì²­í¬')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21166f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥ì™„ë£Œ / íŒŒì¼ëª… : chunks_output_4_2_RAG2.pkl / ì²­í¬ìˆ˜ : 6\n"
     ]
    }
   ],
   "source": [
    "# ì²­í‚¹ ê²°ê³¼ ì €ì¥ (pickle ì‚¬ìš©)\n",
    "import pickle\n",
    "# ìµœì¢… ë¶„í• ì„¤ì • (ì¤‘ê°„í¬ê¸°)\n",
    "final_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap=50,\n",
    "    separators=['\\n', '\\n\\n', '.', ',', ' '],\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "final_chunks = final_splitter.split_documents(sample_documents)\n",
    "\n",
    "# íŒŒì¼ë¡œ ì €ì¥\n",
    "output_path = 'chunks_output_4_2_RAG2.pkl'\n",
    "with open(output_path, 'wb') as f:  #wb write binary / rb read binary --> binary ë”•ì…”ë„ˆë¦¬ ë“± ì €ì¥ê°€ëŠ¥\n",
    "    pickle.dump(final_chunks, f)\n",
    "print(f'ì €ì¥ì™„ë£Œ / íŒŒì¼ëª… : {output_path} / ì²­í¬ìˆ˜ : {len(final_chunks)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
