{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33108ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청크데이터 로드\n",
      "입력텍스트 : RAG는 검색 증강 생성 기술입니다.\n",
      "벡터차원: 1536\n",
      "벡터일부: 0.0114 0.0539 ...-0.0661\n",
      "소요시간: 0.657\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 필수 라이브러리 로드\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# 이전 단계 데이터 로드\n",
    "print('청크데이터 로드')\n",
    "chunks_file = 'chunks_output_4_2_RAG2.pkl'\n",
    "\n",
    "if os.path.exists(chunks_file):\n",
    "    with open(chunks_file, 'rb') as f:   # 바이너리 형식..? 불러올때 지정??? \n",
    "        doc_chunks = pickle.load(f)\n",
    "else:\n",
    "    # 파일이 없으면 새로 생성\n",
    "    print('4_2_RAG1.2_chunk.ipynb 실행')\n",
    "\n",
    "# openai 임베딩 모델 초기화\n",
    "embeddng_model = OpenAIEmbeddings(\n",
    "    model = 'text-embedding-3-small'\n",
    ")\n",
    "\n",
    "# 단일 텍스트 임베딩 테스트\n",
    "test_text = 'RAG는 검색 증강 생성 기술입니다.'\n",
    "\n",
    "# 시간비교\n",
    "start_time = time.time()\n",
    "\n",
    "# 텍스트 쿼리 임베딩\n",
    "test_embedding = embeddng_model.embed_query(test_text)\n",
    "\n",
    "# 시간비교\n",
    "elasped = time.time() - start_time\n",
    "\n",
    "print(f'입력텍스트 : {test_text}')\n",
    "print(f'벡터차원: {len(test_embedding)}')\n",
    "print(f'벡터일부: {test_embedding[0]:.4f} {test_embedding[1]:.4f} ...{test_embedding[-1]:.4f}')\n",
    "print(f'소요시간: {elasped:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7723c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기준문장 : RAG는 검색 증강 생성 기술입니다.\n",
      "유사도 비교 결과\n",
      "문서 1번째 : RAG는 문서 검색과 답변 생성을 결합합니다. ---> 0.6779\n",
      "문서 2번째 : 벡터 데이터베이스는 임베딩을 저장합니다. ---> 0.2335\n",
      "문서 3번째 : 오늘 날씨가 매우 좋습니다. ---> 0.0664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 유사도 계산\n",
    "import numpy as np\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1*norm2)\n",
    "\n",
    "test_sentences = [\n",
    "    \"RAG는 검색 증강 생성 기술입니다.\",         # 기준 문장\n",
    "    \"RAG는 문서 검색과 답변 생성을 결합합니다.\",  # 유사한 문장\n",
    "    \"벡터 데이터베이스는 임베딩을 저장합니다.\",   # 관련 있는 문장\n",
    "    \"오늘 날씨가 매우 좋습니다.\",               # 관련 없는 문장\n",
    "]\n",
    "\n",
    "# 모든 문장을 임베딩\n",
    "embeddings = [embeddng_model.embed_query(sent) for sent in test_sentences]\n",
    "\n",
    "# 기존 문장과 유사도 비교\n",
    "base_embedding = embeddings[0]\n",
    "\n",
    "print(f'기준문장 : {test_sentences[0]}')\n",
    "print(f'유사도 비교 결과')\n",
    "for i, (sent, emb) in enumerate(zip(test_sentences[1:], embeddings[1:]), 1) :\n",
    "    similarity= cosine_similarity(base_embedding, emb)\n",
    "    print(f'문서 {i}번째 : {sent} ---> {similarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f8865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB 구축완료\n",
      "저장된 청크수 : 6\n",
      "소요시간: 1.2081890106201172\n"
     ]
    }
   ],
   "source": [
    "# chromaDB (벡터DB)\n",
    "# 동작방식\n",
    "# 저장 : 텍스트(청크) --> 임베딩 (벡터) --> VectorDB(저장)\n",
    "# 검색 : 질문 --> 임베딩(벡터) --> 유사도 검색 --> top-k문서 변환\n",
    "embeddng_model = OpenAIEmbeddings(\n",
    "    model = 'text-embedding-3-small'\n",
    ")\n",
    "# chromaDB에 chunk를 저장\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# chromaDB 생성(인메모리 방식)\n",
    "chunks_file = 'chunks_output_4_2_RAG2.pkl'\n",
    "\n",
    "if os.path.exists(chunks_file):\n",
    "    with open(chunks_file, 'rb') as f:   # 바이너리 형식..? 불러올때 지정??? \n",
    "        doc_chunks = pickle.load(f)\n",
    "else:\n",
    "    # 파일이 없으면 새로 생성\n",
    "    print('4_2_RAG1.2_chunk.ipynb 실행')\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = doc_chunks,\n",
    "    collection_name = '4_2_RAG1.2_chunk', \n",
    "    embedding=embeddng_model\n",
    ")\n",
    "\n",
    "elasped = time.time() - start_time\n",
    "print(f'VectorDB 구축완료')\n",
    "print(f'저장된 청크수 : {len(doc_chunks)}')\n",
    "print(f'소요시간: {elasped}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "227d0533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "질문 1 : RAG란 무엇인가요?\n",
      "-----> 답변1 rag_concept.txt (거리: 0.8739\n",
      " RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.          RAG의 작동 원리:         1. 사용자 질문을 임베딩 벡터로 변환합니다.         2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.         3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.          RAG의 장점:         - 최신 정보를 반영할 수 있습니다. LLM의 학습 데이터 이후 정보도 활용 가능합니다.\n",
      "-----> 답변2 rag_concept.txt (거리: 0.9984\n",
      " - 환각(Hallucination)을 감소시킵니다. 실제 문서 기반으로 답변하기 때문입니다.         - 출처를 명시할 수 있습니다. 어떤 문서에서 정보를 가져왔는지 추적 가능합니다.         - 도메인 특화가 가능합니다. 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.          RAG의 핵심 구성요소: Retriever(검색기), Generator(생성기), VectorStore(벡터저장소)\n",
      "\n",
      "\n",
      "질문 2 : VectorDB에는 어떤 종류가 있나요?\n",
      "-----> 답변1 vectordb_intro.txt (거리: 0.7170\n",
      " VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.          주요 VectorDB 솔루션:         - ChromaDB: 로컬 개발에 적합한 오픈소스 솔루션. 파이썬 네이티브로 설치가 간편합니다.         - Pinecone: 완전 관리형 클라우드 서비스. 대규모 프로덕션 환경에 적합합니다.         - Weaviate: 그래프 기반 벡터 데이터베이스. 하이브리드 검색을 지원합니다.\n",
      "-----> 답변2 vectordb_intro.txt (거리: 1.3665\n",
      " - FAISS: Facebook에서 개발한 고성능 라이브러리. 대용량 벡터 검색에 최적화되어 있습니다.         - Milvus: 분산 환경을 지원하는 오픈소스 솔루션입니다.          임베딩(Embedding)은 텍스트를 숫자 벡터로 변환하는 과정으로,         의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치됩니다.         예를 들어, \"고양이\"와 \"강아지\"는 \"자동차\"보다 벡터 공간에서 더 가깝습니다.\n",
      "\n",
      "\n",
      "질문 3 : LangChain의 구성 요소는?\n",
      "-----> 답변1 langchain_intro.txt (거리: 0.7558\n",
      " LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.          LangChain의 주요 구성 요소:         1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합         2. Prompts: 프롬프트 템플릿 관리 및 최적화         3. Chains: 여러 구성 요소를 연결하는 파이프라인         4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\n",
      "-----> 답변2 langchain_intro.txt (거리: 0.9849\n",
      " 4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템         5. Indexes: 문서 검색을 위한 인덱싱 도구         6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트          LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로,         파이프(|) 연산자를 사용하여 컴포넌트들을 직관적으로 연결할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 테스트 질문\n",
    "test_queries = [\n",
    "    'RAG란 무엇인가요?',\n",
    "    'VectorDB에는 어떤 종류가 있나요?',\n",
    "    'LangChain의 구성 요소는?'\n",
    "]\n",
    "\n",
    "for j, query in enumerate(test_queries, 1):\n",
    "    print(f'\\n\\n질문 {j} : {query}')\n",
    "\n",
    "    # 유사문서 검색 상위 2개\n",
    "    results = vectorstore.similarity_search_with_score(query, k=2)\n",
    "    for i, (doc,score) in enumerate(results, 1):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        preview = doc.page_content.strip()[:].replace('\\n', ' ')\n",
    "        print(f'-----> 답변{i} {source} (거리: {score:.4f}') # 적을수록 좋음. (거리반환이므로, 거리 가까울수록 값 적게나옴)\n",
    "        print(f' {preview}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "692161f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다양한 검색 옵션\n",
      "기본 유사도 검색 (similarity)\n",
      "결과 : 3\n",
      "1 : rag_concept.txt\n",
      "2 : rag_concept.txt\n",
      "3 : vectordb_intro.txt\n"
     ]
    }
   ],
   "source": [
    "# 다양한 검색 옵션\n",
    "print('다양한 검색 옵션')\n",
    "# 리트리버 생성\n",
    "print('기본 유사도 검색 (similarity)')\n",
    "retriver_basic = vectorstore.as_retriever(   #청킨된 다큐먼트 벡터를 가지고 있는게 vectorstore\n",
    "                search_type = 'similarity'   #retriver similarity 이용?\n",
    "                , search_kwargs = {'k': 3}\n",
    ") \n",
    "results = retriver_basic.invoke('RAG의 장점')\n",
    "print(f'결과 : {len(results)}')\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i} : {doc.metadata.get('source', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR 검색 (다양성 고려)\n",
      "결과 : 3\n",
      "1 : rag_concept.txt\n",
      "2 : langchain_intro.txt\n",
      "3 : vectordb_intro.txt\n"
     ]
    }
   ],
   "source": [
    "print(f'MMR 검색 (다양성 고려)')\n",
    "retriver_basic = vectorstore.as_retriever(   #청킨된 다큐먼트 벡터를 가지고 있는게 vectorstore\n",
    "                search_type = 'mmr'   #mmr 이용\n",
    "                , search_kwargs = {'k': 3,\n",
    "                                   'fetch_k': 6,  #먼저 6개의 후보 검색\n",
    "                                   'lambda' : 0.5  # 다양성 가중치 (0=다양성, 1=관련성)\n",
    "                                   }  \n",
    ") \n",
    "results = retriver_basic.invoke('RAG의 장점')\n",
    "print(f'결과 : {len(results)}')\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i} : {doc.metadata.get('source', 'unknown')}\") # source, unkown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a57c64af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata를 필터링\n",
      "결과 : 2\n",
      "1 : rag_concept.txt topic = technique\n",
      "2 : rag_concept.txt topic = technique\n"
     ]
    }
   ],
   "source": [
    "print(f'metadata를 필터링')  # 필터링 하면 조건지정이 가능?\n",
    "results = vectorstore.similarity_search(\n",
    "    '기술에 대해 설명해 주세요',\n",
    "    k=2,\n",
    "    filter={'topic': 'technique'} # 주제가 technique한 것을.. \n",
    ")\n",
    "print(f'결과 : {len(results)}')\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i} : {doc.metadata.get('source', 'unknown')} topic = {doc.metadata.get('topic')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4f4e620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorDB 영구저장\n",
      "저장경로 : ./chroma_db_reg2\n",
      "저장된 청크수 : 6\n",
      "설정정보 저장 완료 파일명 : vectordb_config_4_4_RAG2.2.pkl\n"
     ]
    }
   ],
   "source": [
    "#vectorDB 영구 저장 (옵션임/ 필수아님)\n",
    "persist_dir = './chroma_db_reg2'\n",
    "vectorstore_persistent = Chroma.from_documents(\n",
    "    documents= doc_chunks,\n",
    "    collection_name='persistent_rag',\n",
    "    embedding= embeddng_model,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "print(f'vectorDB 영구저장')\n",
    "print(f'저장경로 : {persist_dir}')\n",
    "print(f'저장된 청크수 : {len(doc_chunks)}')\n",
    "\n",
    "# 설정정보 저장\n",
    "# json 저장해도 되나, json 보다 pickle로 하면 용량이 작아지고 장점이 있음 / json은 딕셔너리만 가능\n",
    "config = {\n",
    "    'persist_directory': persist_dir,\n",
    "    'collection_name' : \"persistent_rag\",\n",
    "    'embedding_model': 'text-embedding-3-small',\n",
    "    'chunk_count' : len(doc_chunks) \n",
    "}\n",
    "with open('vectordb_config_4_4_RAG2.2.pkl', 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "print('설정정보 저장 완료 파일명 : vectordb_config_4_4_RAG2.2.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
