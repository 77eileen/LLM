🔍 RAG(Retrieval-Augmented Generation)의 개념

RAG는 **Retrieval(검색)**과 **Generation(생성)**을 결합한 구조로, LLM이 기존에 학습한 지식만으로 답변하는 것이 아니라 외부 문서나 데이터베이스에서 필요한 정보를 실시간으로 찾아와 답변을 생성하는 방식의 AI 아키텍처이다.
기존 LLM은 학습 데이터의 한계를 가지며, 최신 정보나 특정 도메인 지식을 정확하게 반영하기 어렵다. RAG는 이런 문제를 해결하기 위해, 질문을 받으면 우선 벡터 검색(vector search) 혹은 **키워드 검색(BM25 등)**을 사용하여 관련 문서를 찾아내고, 그 문서 내용을 토대로 LLM이 답변을 생성한다.

🔗 RAG 파이프라인 구성 요소
1) Query → Embedding

사용자가 질문을 하면 이를 임베딩 모델을 통해 고차원 벡터로 변환한다.
이 벡터는 의미 기반 유사도를 계산하는 데 사용된다.

2) Retriever(검색기)

임베딩된 질문 벡터와 벡터 DB(Chroma, Pinecone 등)에 저장된 문서 벡터들을 비교하여 가장 관련 높은 문서 Top-K를 반환한다.

벡터 검색(ex: cosine similarity)

키워드 기반 검색(BM25)

혼합 검색(RRF 등)

3) Context 생성

찾아온 문서 조각들을 하나의 **context(문맥)**로 조합하여 LLM에 전달한다.
이때 context 길이를 너무 크게 넣으면 LLM의 컨텍스트 윈도우를 낭비하므로, 문맥 압축(Contextual Compression) 기술을 활용하기도 한다.

4) LLM이 최종 답변 생성

LLM은 제공된 컨텍스트를 바탕으로

질문 의도

검색된 문서의 정보

자체 언어 이해 능력
을 결합하여 최종 답변을 생성한다.

📌 RAG가 필요한 이유

지식 최신화 문제 해결
LLM은 학습 시점 이후 정보를 모른다.
하지만 RAG를 사용하면 실시간 데이터나 사내 문서를 즉시 연결할 수 있다.

정확도 향상
LLM이 ‘헛소리(Hallucination)’를 줄이고, 문서를 근거로 답변하도록 유도하므로 정확도가 크게 증가한다.

도메인 특화 모델 없이 고성능 달성
특정 산업(의료, 법률, 기업 내부 문서 등)에 대해 LLM을 따로 파인튜닝하지 않아도 되기 때문에 비용이 적게 들고 효율적이다.

🧠 RAG 구현 시 필수 요소

문서 전처리(Chunking)
문서를 적절한 길이로 잘라야 검색 효율이 좋아짐.

Embedding 모델 선택
텍스트 의미를 얼마나 잘 인코딩하는지 중요함.

Vector DB 설계
Large-scale 검색 시 속도와 정확성 균형 필요.

Retriever 최적화
Multi-query, RRF, re-ranking 등을 활용하면 성능 향상.

LLM 프롬프트 디자인
문서 출처 기반 답변하도록 지시해야 Hallucination 금지됨.

🎯 정리

RAG는 AI가 정확하고 신뢰성 있는 답을 만들기 위한 핵심 기술이다.
기존 LLM의 한계(지식 고정, 환각 문제)를 극복하여 최신·정확·근거 기반 답변을 제공할 수 있게 해준다.
현재 대부분의 기업형 AI 시스템(OpenAI의 Assistants, LangChain, LlamaIndex 기반 서비스 등)이 RAG 구조를 채택하고 있으며, 앞으로는 RAG 성능 최적화 기법(여러 retriever 결합, 재랭킹, self-RAG 등)이 더 중요해지는 추세이다.