{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe6f514",
   "metadata": {},
   "source": [
    "### <span style=\"color: Gold\"> **RAG - LangChain**\n",
    "- 사용자 질문\n",
    "- 임베딩 변환 --------> 벡터로 변환\n",
    "- vectorDB 검색 ------> 유사한 문서 검색\n",
    "- 문서 포멧팅 ---------> 검색된 문서를 텍스트로 정리\n",
    "- 프롬프트 구성 -------> 컨텍스트 + 질문 결합\n",
    "- LLM 호출 -----------> 답변생성\n",
    "- 출력피싱 ------------> 문자열로 변환\n",
    "- 최종답변"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "48865737",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7ee219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 템플릿 : 재사용 가능한 프롬프트 구조를 정의\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '당신은 {role} 입니다.'),\n",
    "    ('human', '{question}' )\n",
    "])\n",
    "\n",
    "# role, question  변수채우기\n",
    "prompt = template.invoke({\n",
    "    'role' : 'AI assistant',\n",
    "    'question' : 'RAG란 무엇인가요?'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 프롬프트 유형 : 단일 문자열\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = PromptTemplate.from_template('''\n",
    "                                        다음 질문에 답변하세요.\n",
    "                                        질문 : {question}\n",
    "                                        답변 : ''')\n",
    "\n",
    "\n",
    "\n",
    "# 프롬프트 유형 : 채팅형식\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '시스템 지시사항'),\n",
    "    ('human', '사용자 질문: {question}' ),\n",
    "    ('assistant', '이전답변 (선택)'),\n",
    "    ('human', \"후속질문\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907004ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 설계 원칙\n",
    "\n",
    "'''\n",
    "1. 역할 정의 (Role Definition)\n",
    "     \"당신은 전문적인 기술 문서 Q&A 시스템입니다.\"    \n",
    "2. 컨텍스트 제공 (Context)                          \n",
    "    \"다음은 참조할 문서입니다: {context}\"        \n",
    "3. 명확한 지시 (Instructions)                       \n",
    "    - 컨텍스트 내 정보만 사용                      \n",
    "    - 모르면 모른다고 답변                          \n",
    "    - 한국어로 답변                                  \n",
    "4. 질문 (Question)                                  \n",
    "    \"질문: {question}\"                           \n",
    "5. 출력 형식 (Output Format)                        \n",
    "    \"답변은 구조화된 형태로 작성하세요.\"   \n",
    "'''\n",
    "\n",
    "# 효과적인 RAG 프롬프트 작성 예시\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''당신은 제공된 문맥(context)을 바탕으로 질문에 답변하는 AI 어시스턴트입니다.\n",
    "    ## 규칙\n",
    "    1. 제공된 문맥 내의 정보만을 사용하여 답변하세요.\n",
    "    2. 문맥에 없는 정보는 추측하지 말고 \"제공된 문서에서 해당 정보를 찾을 수 없습니다.\"라고 답하세요.\n",
    "    3. 답변은 한국어로 명확하고 간결하게 작성하세요.\n",
    "    4. 가능하면 구조화된 형태(목록, 번호 등)로 답변하세요.\n",
    "    5. 확실하지 않은 내용은 그 점을 명시하세요\n",
    "     '''),\n",
    "    ('human', '''## 참조문맥\n",
    "     {context}\n",
    "\n",
    "     ## 질문\n",
    "     {question}\n",
    "\n",
    "     ## 답변''')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff56e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL (LangChain Expression Language)\n",
    "# : 파이프연산자를 이용해서 직관적으로 연결\n",
    "# : 가독성이 높고, 재사용이 높고, 스트리밍과 배치를 지원\n",
    "# : Runnable component 입력을 다음단계로 넘김??? \n",
    "\n",
    "# lagecy 방식 (전통적인 방식)\n",
    "    # result = parser.parser(llm.invoke(prompt.format(question= '질문')))\n",
    "\n",
    "# LCEL 방식(체인구성방식)\n",
    "# chain = prompt | llm | parser\n",
    "# result = chain.invoke({'question' : '질문'})\n",
    "\n",
    "# 핵심 : Runnable component (LCEL을 이루는 단위 구성요소) \n",
    "# “프롬프트\", “LLM\", “retriever\", “parser” 같은 기능 블록(모듈)\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# # 질문을 그대로 전달하면서 context는 별도 처리\n",
    "# chain = {\n",
    "#     \"context\" : retriever | format_docs,\n",
    "#     \"question\" : RunnablePassthrough\n",
    "# } | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain 프롬프트 탬플릿\n",
    "# LCEL 사용법\n",
    "# RAG 체인 구성 및 실행\n",
    "# 답변 품질 개선 전략\n",
    "\n",
    "# 파이프라인 구축\n",
    "     # [질문] --> [retriever] --> [관련문서] --> [프롬프트] --> [LLM] --> [답변]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0652b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 전체 과정: loader → embeddings → vectorDB → retriever\n",
    "# 1. loader = 파일에서 문서 읽기\n",
    "# 2. embedding = 문장을 숫자 벡터로 변환\n",
    "# 3. vector store(DB) = 문서 벡터를 저장해두는 공간 (Chroma, FAISS 등)\n",
    "# 4. retriever = 질문 임베딩과 비교해서 “가장 가까운 문서” 찾아오기\n",
    "# 5. LLM = 찾아온 문서를 참고해 답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ecd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서 포맷팅 예시\n",
      "문서1: rag_concept.txt\n",
      "RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\n",
      "\n",
      "        RAG의 작동 원리:\n",
      "        1. 사용자 질문을 임베딩 벡터로 변환합니다.\n",
      "        2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\n",
      "        3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.\n",
      "\n",
      "        RAG의 장점:\n",
      "        - 최신 정보를 반영할 수 있습니다. LLM의 학습 데이터 이후 정보도 활용 가능합니다.\n",
      "\n",
      "---\n",
      "\n",
      "문서2: rag_concept.txt\n",
      "RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\n",
      "\n",
      "        RAG의 작동 원리:\n",
      "        1. 사용자 질문을 임베딩 벡터로 변환합니다.\n",
      "        2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\n",
      "        3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.\n",
      "\n",
      "        RAG의 장점:\n",
      "        - 최신 정보를 반영할 수 있습니다. LLM의 학습 데이터 이후 정보도 활용 가능합니다.\n",
      "\n",
      "---\n",
      "\n",
      "문서3: rag_concept.txt\n",
      "- 환각(Hallucination)을 감소시킵니다. 실제 문서 기반으로 답변하기 때문입니다.\n",
      "        - 출처를 명시할 수 있습니다. 어떤 문서에서 정보를 가져왔는지 추적 가능합니다.\n",
      "        - 도메인 특화가 가능합니다. 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.\n",
      "\n",
      "        RAG의 핵심 구성요소: Retriever(검색기), Generator(생성기), VectorStore(벡터저장소)\n",
      "기본 RAG 체인 구성 완료\n",
      "출처 포함 RAG 체인 구성 완료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle    # chunk, vectorDB 저장한것 사용\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 경고메세지 삭제\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# openapi key 확인\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError('.env확인,  key없음')\n",
    "\n",
    "# 필수 라이브러리 로드\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "\n",
    "# vectorDB 로드\n",
    "# 임베딩 모델 초기화\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "# 이전단계에서 저장한 vectordb로드\n",
    "persist_dir = './chroma_db_reg2'\n",
    "config_file = 'vectordb_config_4_4_RAG2.2.pkl'\n",
    "if os.path.exists(persist_dir):\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name='persistent_rag',\n",
    "        embedding_function=embedding_model\n",
    "    )\n",
    "else:\n",
    "    raise ValueError('이전단계에서 chroma_db 디렉터리 생성, vectordb_config 생성 필요')\n",
    "\n",
    "\n",
    "\n",
    "#  [retriever] 생성\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k':3})\n",
    "\n",
    "\n",
    "# LLM 모델 생성\n",
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature=0\n",
    "    )\n",
    "\n",
    "\n",
    "# prompt templete 생성\n",
    "# 기본 RAG 프롬프트\n",
    "basic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"당신은 제공된 문맥(Context)을 바탕으로 질문에 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "규칙:\n",
    "1. 제공된 문맥 내의 정보만을 사용하여 답변하세요.\n",
    "2. 문맥에 없는 정보는 \"제공된 문서에서 해당 정보를 찾을 수 없습니다.\"라고 답하세요.\n",
    "3. 답변은 한국어로 명확하고 간결하게 작성하세요.\n",
    "4. 가능하면 구조화된 형태(목록, 번호 등)로 답변하세요.\"\"\"),\n",
    "    (\"human\", \"\"\"문맥(Context):\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\")\n",
    "])\n",
    "\n",
    "\n",
    "# 상세 RAG 프롬프트\n",
    "detailed_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"당신은 전문적인 지식 기반 Q&A 시스템입니다.\n",
    "\n",
    "## 역할\n",
    "제공된 문맥을 분석하여 사용자의 질문에 정확하게 답변합니다.\n",
    "\n",
    "## 답변 규칙\n",
    "1. **출처 기반**: 반드시 제공된 문맥의 정보만 사용합니다.\n",
    "2. **정확성**: 문맥에 없는 내용은 추측하지 않습니다.\n",
    "3. **명확성**: 답변은 이해하기 쉽게 구조화합니다.\n",
    "4. **언어**: 한국어로 답변합니다.\n",
    "\n",
    "## 답변 불가 시\n",
    "문맥에서 정보를 찾을 수 없으면:\n",
    "\"제공된 문서에서 해당 정보를 찾을 수 없습니다. 다른 질문을 해주세요.\"\n",
    "라고 답변합니다.\"\"\"),\n",
    "    (\"human\", \"\"\"## 참조 문맥\n",
    "{context}\n",
    "\n",
    "## 질문\n",
    "{question}\n",
    "\n",
    "## 답변\"\"\")\n",
    "])\n",
    "\n",
    "\n",
    "# 문서 포맷 작성 (검색된 문서를 포맷..????)\n",
    "def format_docs(docs):\n",
    "    '''검색된 문서들을 하나의 문자열로 포맷팅'''\n",
    "    return '\\n\\n---\\n\\n'.join([doc.page_content for doc in docs])\n",
    "\n",
    "def format_docs_with_source(docs):\n",
    "    '''출처 정보를 포함하여 문서 포맷팅'''\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        formatted.append(f\"문서{i}: {source}\\n{doc.page_content}\")\n",
    "    return '\\n\\n---\\n\\n'.join(formatted)\n",
    "\n",
    "#테스트\n",
    "test_docs = retriever.invoke('RAG란 무엇인가요?')\n",
    "print('검색된 문서 포맷팅 예시')\n",
    "print(format_docs_with_source(test_docs[:2]))\n",
    "\n",
    "\n",
    "\n",
    "# RAG 체인 구성 \n",
    "# 기본 RAG 체인 (LCEL 사용)\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question':RunnablePassthrough()}  #'question':RunnablePassthrough() 재활용 가능\n",
    "    | basic_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()  # 순수 문자열만..\n",
    ")\n",
    "print('기본 RAG 체인 구성 완료')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 출처 포함 RAG 체인\n",
    "rag_chain_with_source = (\n",
    "    {'context': retriever | format_docs_with_source, 'question':RunnablePassthrough()}  #'question':RunnablePassthrough() 재활용 가능\n",
    "    | basic_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()  # 순수 문자열만..\n",
    ")\n",
    "print('출처 포함 RAG 체인 구성 완료')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7116eab",
   "metadata": {},
   "source": [
    "#### 상기 chain 구조\n",
    "질문 --------------> retriever ---------------> 관련 문서 검색\n",
    "                    format_docs --------------> 문자열로 변환\n",
    "                    prompt -------------------> context + question 결합\n",
    "                    LLM  ---------------------> 답변 생성\n",
    "                    Strparser  ---------------> 문자열 출력  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dbda66",
   "metadata": {},
   "source": [
    "### <span style=\"color: Gold\"> **RAG chain 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aed16bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 체인 테스트\n",
      "테스트 질문1 : RAG란 무엇이고 어떤 장점이 있나요?\n",
      "답변 : RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술로, 다음과 같은 장점이 있습니다:\n",
      "\n",
      "1. **최신 정보 반영**: LLM의 학습 데이터 이후의 정보도 활용할 수 있습니다.\n",
      "2. **환각 감소**: 실제 문서 기반으로 답변하기 때문에 잘못된 정보 제공을 줄입니다.\n",
      "3. **출처 명시 가능**: 정보를 가져온 문서를 추적할 수 있어 출처를 명확히 할 수 있습니다.\n",
      "4. **도메인 특화**: 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.\n",
      "참조문서 : ['rag_concept.txt', 'rag_concept.txt', 'rag_concept.txt']\n",
      "소요된 시간 : 5.082186460494995\n",
      "=========================\n",
      "\n",
      "\n",
      "테스트 질문2 : LangChain의 주요 구성 요소를 설명해주세요.\n",
      "답변 : LangChain의 주요 구성 요소는 다음과 같습니다:\n",
      "\n",
      "1. **Models**: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합하여 모델을 활용합니다.\n",
      "2. **Prompts**: 프롬프트 템플릿을 관리하고 최적화하는 기능을 제공합니다.\n",
      "3. **Chains**: 여러 구성 요소를 연결하는 파이프라인을 형성합니다.\n",
      "4. **Memory**: 대화 맥락을 유지하기 위한 메모리 시스템입니다.\n",
      "5. **Indexes**: 문서 검색을 위한 인덱싱 도구를 제공합니다.\n",
      "6. **Agents**: 도구를 사용하여 복잡한 작업을 수행하는 에이전트입니다. \n",
      "\n",
      "또한, LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로, 파이프(|) 연산자를 사용하여 컴포넌트들을 직관적으로 연결할 수 있습니다.\n",
      "참조문서 : ['langchain_intro.txt', 'langchain_intro.txt', 'langchain_intro.txt']\n",
      "소요된 시간 : 4.901137113571167\n",
      "=========================\n",
      "\n",
      "\n",
      "테스트 질문3 : VectorDB에는 어떤 종류가 있나요?\n",
      "답변 : VectorDB에는 다음과 같은 주요 종류가 있습니다:\n",
      "\n",
      "1. **ChromaDB**: \n",
      "   - 로컬 개발에 적합한 오픈소스 솔루션\n",
      "   - 파이썬 네이티브로 설치가 간편함\n",
      "\n",
      "2. **Pinecone**: \n",
      "   - 완전 관리형 클라우드 서비스\n",
      "   - 대규모 프로덕션 환경에 적합함\n",
      "\n",
      "3. **Weaviate**: \n",
      "   - 그래프 기반 벡터 데이터베이스\n",
      "   - 하이브리드 검색을 지원함\n",
      "\n",
      "4. **FAISS**: \n",
      "   - Facebook에서 개발한 고성능 라이브러리\n",
      "   - 대용량 벡터 검색에 최적화됨\n",
      "\n",
      "5. **Milvus**: \n",
      "   - 분산 환경을 지원하는 오픈소스 솔루션\n",
      "참조문서 : ['vectordb_intro.txt', 'vectordb_intro.txt', 'vectordb_intro.txt']\n",
      "소요된 시간 : 5.00413966178894\n",
      "=========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('RAG 체인 테스트')\n",
    "test_questions = [\n",
    "    \"RAG란 무엇이고 어떤 장점이 있나요?\",\n",
    "    \"LangChain의 주요 구성 요소를 설명해주세요.\",\n",
    "    \"VectorDB에는 어떤 종류가 있나요?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions,1) :\n",
    "    print(f'테스트 질문{i} : {question}')\n",
    "    start_time = time.time()\n",
    "    # RAG체인 실행\n",
    "    answer = rag_chain.invoke(question)  #LCEL로 하면 전부 invoke.. invoke가 뭔데?\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'답변 : {answer}')\n",
    "    # 참조문서\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    sources = [doc.metadata.get('source', 'unknown') for doc in retrieved_docs]\n",
    "    print(f'참조문서 : {sources}')\n",
    "    print(f'소요된 시간 : {elapsed}')\n",
    "    print('=========================\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4cfd4",
   "metadata": {},
   "source": [
    "### 고급 RAG 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e22344c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 성능향상을 위한 고급패턴\n",
      "query transformation\n",
      "원본 : RAG가 뭔지 좀 알려주세요\n",
      "변환 : RAG 정의 및 설명\n"
     ]
    }
   ],
   "source": [
    "### RAG 성능향상을 위한 고급패턴\n",
    "print('RAG 성능향상을 위한 고급패턴')\n",
    "### query transformation\n",
    "print('query transformation')\n",
    "query_transform_prompt = ChatPromptTemplate.from_template(\n",
    "    '''다음질문을 검색에 더 적합한 형태로 변환해주세요.\n",
    "    키워드 중심으로 명확하게 바꿔주세요\n",
    "    \n",
    "    원본질문:{question}\n",
    "    \n",
    "    변환된 검색어 (한줄로)'''\n",
    ")\n",
    "query_chain = query_transform_prompt | llm | StrOutputParser()\n",
    "\n",
    "original_question = 'RAG가 뭔지 좀 알려주세요'\n",
    "\n",
    "transformed =query_chain.invoke({'question':original_question})\n",
    "print(f'원본 : {original_question}')\n",
    "print(f'변환 : {transformed}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea2e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923c02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bde22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde3cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab32d2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d13531e1-268b-4ffb-99c1-6b3360cd4131', '7072aa96-1294-49f0-8e34-a3f4bc1dc921', '3136e3a2-ddd2-41c3-815e-7f1fc92db27d', '2b8ca3fd-da77-4a64-8b05-e01c8e45734e', '88ea2a77-4efc-4ca2-91f5-12ff9cc449a4', 'b91ccc24-0a8f-4a0b-942a-c7499a1417c1', '59d7a612-1694-48a5-a761-2798e3ba6f22', '68bebb16-8df5-4f0e-a9e0-615fdfdb081c', 'c38d4c05-8d19-4d30-a04c-c3f45fc838bf', '94c83a20-97f6-4af6-bf14-524270df32ea', '7c032f8a-124f-41d2-b975-9785322b7632', 'f641e887-66ff-4b90-83a8-aff9b85518b7']\n"
     ]
    }
   ],
   "source": [
    "# 벡터스토어 내부 문서 ID 조회\n",
    "ids = vectorstore._collection.get()['ids']\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c7608a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- 문서 1 ----\n",
      "source: langchain_intro.txt\n",
      "page: 없음\n",
      "내용: LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n",
      "\n",
      "        LangChain의 주요 구성 요소:\n",
      "        1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합\n",
      "        2. Prompts: 프롬프트 템플릿 관리 및 최적화\n",
      "        3. Ch ...\n",
      "\n",
      "---- 문서 2 ----\n",
      "source: langchain_intro.txt\n",
      "page: 없음\n",
      "내용: 4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\n",
      "        5. Indexes: 문서 검색을 위한 인덱싱 도구\n",
      "        6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트\n",
      "\n",
      "        LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로,\n",
      "        파이프(|) 연산 ...\n",
      "\n",
      "---- 문서 3 ----\n",
      "source: rag_concept.txt\n",
      "page: 없음\n",
      "내용: RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\n",
      "\n",
      "        RAG의 작동 원리:\n",
      "        1. 사용자 질문을 임베딩 벡터로 변환합니다.\n",
      "        2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\n",
      "        3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.\n",
      "\n",
      "       ...\n",
      "\n",
      "---- 문서 4 ----\n",
      "source: rag_concept.txt\n",
      "page: 없음\n",
      "내용: - 환각(Hallucination)을 감소시킵니다. 실제 문서 기반으로 답변하기 때문입니다.\n",
      "        - 출처를 명시할 수 있습니다. 어떤 문서에서 정보를 가져왔는지 추적 가능합니다.\n",
      "        - 도메인 특화가 가능합니다. 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.\n",
      "\n",
      "        RAG의 핵심 구성요소: Retriever(검색기) ...\n",
      "\n",
      "---- 문서 5 ----\n",
      "source: vectordb_intro.txt\n",
      "page: 없음\n",
      "내용: VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.\n",
      "\n",
      "        주요 VectorDB 솔루션:\n",
      "        - ChromaDB: 로컬 개발에 적합한 오픈소스 솔루션. 파이썬 네이티브로 설치가 간편합니다.\n",
      "        - Pinecone: 완전 관리형 클라우드 서비스. 대규모 프로덕션 환경에 적합합니다. ...\n",
      "\n",
      "---- 문서 6 ----\n",
      "source: vectordb_intro.txt\n",
      "page: 없음\n",
      "내용: - FAISS: Facebook에서 개발한 고성능 라이브러리. 대용량 벡터 검색에 최적화되어 있습니다.\n",
      "        - Milvus: 분산 환경을 지원하는 오픈소스 솔루션입니다.\n",
      "\n",
      "        임베딩(Embedding)은 텍스트를 숫자 벡터로 변환하는 과정으로,\n",
      "        의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치됩니다.\n",
      "      ...\n",
      "\n",
      "---- 문서 7 ----\n",
      "source: langchain_intro.txt\n",
      "page: 없음\n",
      "내용: LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n",
      "\n",
      "        LangChain의 주요 구성 요소:\n",
      "        1. Models: 다양한 LLM 제공자(OpenAI, Anthropic, Google 등)와 통합\n",
      "        2. Prompts: 프롬프트 템플릿 관리 및 최적화\n",
      "        3. Ch ...\n",
      "\n",
      "---- 문서 8 ----\n",
      "source: langchain_intro.txt\n",
      "page: 없음\n",
      "내용: 4. Memory: 대화 맥락을 유지하기 위한 메모리 시스템\n",
      "        5. Indexes: 문서 검색을 위한 인덱싱 도구\n",
      "        6. Agents: 도구를 사용하여 복잡한 작업을 수행하는 에이전트\n",
      "\n",
      "        LangChain Expression Language (LCEL)은 체인을 구성하는 선언적 방식으로,\n",
      "        파이프(|) 연산 ...\n",
      "\n",
      "---- 문서 9 ----\n",
      "source: rag_concept.txt\n",
      "page: 없음\n",
      "내용: RAG (Retrieval-Augmented Generation)는 검색 증강 생성 기술입니다.\n",
      "\n",
      "        RAG의 작동 원리:\n",
      "        1. 사용자 질문을 임베딩 벡터로 변환합니다.\n",
      "        2. 벡터 데이터베이스에서 유사한 문서를 검색합니다.\n",
      "        3. 검색된 문서를 컨텍스트로 사용하여 LLM이 답변을 생성합니다.\n",
      "\n",
      "       ...\n",
      "\n",
      "---- 문서 10 ----\n",
      "source: rag_concept.txt\n",
      "page: 없음\n",
      "내용: - 환각(Hallucination)을 감소시킵니다. 실제 문서 기반으로 답변하기 때문입니다.\n",
      "        - 출처를 명시할 수 있습니다. 어떤 문서에서 정보를 가져왔는지 추적 가능합니다.\n",
      "        - 도메인 특화가 가능합니다. 특정 분야의 문서만 사용하여 전문적인 답변을 제공합니다.\n",
      "\n",
      "        RAG의 핵심 구성요소: Retriever(검색기) ...\n",
      "\n",
      "---- 문서 11 ----\n",
      "source: vectordb_intro.txt\n",
      "page: 없음\n",
      "내용: VectorDB(벡터 데이터베이스)는 고차원 벡터를 효율적으로 저장하고 검색하는 데이터베이스입니다.\n",
      "\n",
      "        주요 VectorDB 솔루션:\n",
      "        - ChromaDB: 로컬 개발에 적합한 오픈소스 솔루션. 파이썬 네이티브로 설치가 간편합니다.\n",
      "        - Pinecone: 완전 관리형 클라우드 서비스. 대규모 프로덕션 환경에 적합합니다. ...\n",
      "\n",
      "---- 문서 12 ----\n",
      "source: vectordb_intro.txt\n",
      "page: 없음\n",
      "내용: - FAISS: Facebook에서 개발한 고성능 라이브러리. 대용량 벡터 검색에 최적화되어 있습니다.\n",
      "        - Milvus: 분산 환경을 지원하는 오픈소스 솔루션입니다.\n",
      "\n",
      "        임베딩(Embedding)은 텍스트를 숫자 벡터로 변환하는 과정으로,\n",
      "        의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치됩니다.\n",
      "      ...\n"
     ]
    }
   ],
   "source": [
    "items = vectorstore._collection.get()\n",
    "metas = items['metadatas']\n",
    "contents = items['documents']\n",
    "\n",
    "for i, (meta, content) in enumerate(zip(metas, contents)):\n",
    "    print(f\"\\n---- 문서 {i+1} ----\")\n",
    "    print(\"source:\", meta.get(\"source\", \"없음\"))\n",
    "    print(\"page:\", meta.get(\"page\", \"없음\"))\n",
    "    print(\"내용:\", content[:200], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
