{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb9e225",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px\">\n",
    "\n",
    "#### â–¶ ìì—°ì–´ ê°ì„±ë¶„ì„\n",
    "- ê°ì„±ì‚¬ì „ ê¸°ë°˜ : ë¯¸ë¦¬ ì •ì˜ëœ ê°ì„± ë‹¨ì–´ ì‚¬ì „ ì‚¬ìš©í•´ì„œ ë‹¨ì–´ë³„ ê°ì • ì ìˆ˜ë¥¼ ëª¨ì•„ íŒë‹¨ (ê·œì¹™ê¸°ë°˜) \n",
    "    - TextBlob, AFINN, VADER\n",
    "    - ë¹ ë¥´ê²Œ í° í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼(ì˜ˆ: íŠ¸ìœ„í„°, í˜ì´ìŠ¤ë¶ ëŒ“ê¸€)ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•  ë•Œ.\n",
    "    - ë¼ë²¨ì´ ì—†ëŠ” ë°ì´í„°ì—ì„œ ë¹ ë¥¸ íƒìƒ‰(whatâ€™s the overall sentiment?)ì´ í•„ìš”í•  ë•Œ.\n",
    "    - ë¦¬ì†ŒìŠ¤(ë¼ë²¨ë§ ì‹œê°„)ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ê²½ìš°.\n",
    "- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ : í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”(TF-IDF ë“±)í•˜ê³  ë¶„ë¥˜ê¸°(ì˜ˆ: Naive Bayes, SVM, ë¡œì§€ìŠ¤í‹± íšŒê·€, ì‹ ê²½ë§)ë¥¼ í•™ìŠµ (í†µê³„ê¸°ë°˜)\n",
    "    - TF-IDF ë²¡í„°í™”\n",
    "    - ì„ í˜•íšŒê·€, ë¡œì§€ìŠ¤í‹±íšŒê·€\n",
    "    - F1 Score, Recison, Recall -> classification report\n",
    "    - ë„ë©”ì¸ íŠ¹í™” ì–¸ì–´(ì œí’ˆ ë¦¬ë·°, ì˜ë£Œ ë¬¸ì„œ ë“±)ì—ì„œ ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•  ë•Œ.\n",
    "    - ë¼ë²¨(í•™ìŠµ ë°ì´í„°)ì„ í™•ë³´í•  ìˆ˜ ìˆê³  ëª¨ë¸ íŠœë‹/ê²€ì¦ì„ í•  ìˆ˜ ìˆì„ ë•Œ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1ac2c",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px\">\n",
    "\n",
    "#### â–¶ ì‚¬ìš© ë°ì´í„°\n",
    "- NLTK ì˜í™” ë¦¬ë·° (2000ê°œ)\n",
    "- ë‹¤ìŒ ì˜í™” ë¦¬ë·°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf93006",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"font-size:12px\">\n",
    "\n",
    "#### â–¶ ì•Œê³ ë¦¬ì¦˜\n",
    "- TextBlob ì‚¬ì „ê¸°ë°˜ ê°ì„±ë¶„ì„\n",
    "- AFINN ê°ì • ì ìˆ˜ ë§µí•‘\n",
    "- VADER(Valence Aware Dictionary) ì†Œì…œë¯¸ë””ì–´ ìµœì í™” ê°ì„±ë¶„ì„\n",
    "- TF-IDF í…ìŠ¤íŠ¸ ë²¡í„°í™”\n",
    "-  Multinomial Naive Bayes í™•ë¥  ê¸°ë°˜ ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21596be6",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px\">\n",
    "\n",
    "#### â–¶ TextBlob\n",
    "- ì´ ì˜í™”ëŠ” ì •ë§ ì¢‹ê³  ì¬ë¯¸ìˆë‹¤\n",
    "    - ì¢‹ë‹¤ +1 (ê¸ì •)\n",
    "    - ì¬ë¯¸ìˆë‹¤ +1 (ê¸ì •)\n",
    "    - +2 > 0 ----> ê¸ì •(pos) ë¶„ë¥˜\n",
    "- polarity (ê·¹ì„±ë„) = (ê¸ì •ë‹¨ì–´ ê°œìˆ˜ - ë¶€ì •ë‹¨ì–´ ê°œìˆ˜) / ì „ì²´ë‹¨ì–´ê°œìˆ˜\n",
    "- -1.0 ~ +1.0\n",
    "- 0 ì€ ì¤‘ë¦½\n",
    "- Subjectivity (ì£¼ê´€ì„±) í‰ê°€ëŒ€ìƒ ë‹¨ì–´ ë¹„ìœ¨\n",
    "- 0.0 ~ 1.0\n",
    "- 0 : ê°ê´€ì   / 1 : ì£¼ê´€ì \n",
    "- ë¬¸ë§¥ë¬´ì‹œí•˜ê³  ë‹¨ì–´ ê·¹ì„±ë§Œ ê³ ë ¤\n",
    "- ì´ ì˜í™”ëŠ” ë‚˜ì˜ì§€ ì•Šë‹¤ ---> ë‚˜ì˜ë‹¤(-) ì•Šë‹¤(-) ë¡œ ì¸ì‹\n",
    "- ë‹¨ì  : ë¹ ë¥¸ ì†ë„, í•™ìŠµ ë¶ˆí•„ìš”\n",
    "- ì‚¬ìš© : ì‹¤ì‹œê°„ ê°ì„±ë¶„ì„, ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db152031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channel Terms of Service accepted\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %conda install textBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab7684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"TextBlob is amazingly simple to use.\"), Sentence(\"What a wonderful library for NLP!\")]\n",
      "['TextBlob', 'is', 'amazingly', 'simple', 'to', 'use', 'What', 'a', 'wonderful', 'library', 'for', 'NLP']\n",
      "[('TextBlob', 'NNP'), ('is', 'VBZ'), ('amazingly', 'RB'), ('simple', 'JJ'), ('to', 'TO'), ('use', 'VB'), ('What', 'WP'), ('a', 'DT'), ('wonderful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('NLP', 'NNP')]\n",
      "['textblob', 'wonderful library', 'nlp']\n",
      "===========ê°ì„±ë¶„ì„===========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6785714285714286)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "text = \"TextBlob is amazingly simple to use. What a wonderful library for NLP!\"\n",
    "blob = TextBlob(text)\n",
    "print(blob.sentences)  #ë¬¸ì¥ ì¶”ì¶œ\n",
    "print(blob.words)  #ë‹¨ì–´ ì¶”ì¶œ\n",
    "print(blob.tags)   #í’ˆì‚¬ ì¶”ì¶œ\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('brown')\n",
    "print(blob.noun_phrases)  #ëª…ì‚¬ ì¶”ì¶œ\n",
    "\n",
    "\n",
    "print('===========ê°ì„±ë¶„ì„===========')\n",
    "blob.sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a301b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px\"> \n",
    "==========================================\n",
    "\n",
    "#### â–¶ AFINN (Lexicon-Based)\n",
    "- ê° ë‹¨ì–´ì˜ -5 ~ +5 ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  í•©ì‚°\n",
    "- ì´ ì˜í™”ëŠ” ì¢‹ì§€ë§Œ ì¢‹ì§€ ì•Šì€ ë¶€ë¶„ë„ ìˆë‹¤\n",
    "    - ì¢‹ë‹¤ +3 / ì¢‹ì§€ +3 / ë‚˜ì˜ë‹¤ -3 ----> +3 > 0 ê¸ì •\n",
    "- score = sum(word_sentiment_value)\n",
    "    - ë¶„ë¥˜ê·œì¹™ score > 0 ê¸ì • score < 0 ë¶€ì •\n",
    "- ì´ëª¨í‹°ì½˜ ì§€ì›\n",
    "- ê°•ë„í‘œí˜„ ì¸ì‹ very, really ë“±\n",
    "\n",
    "- ê°•ì¡° ìˆ˜ì •ì (intensifiers)\n",
    "    - ë§¤ìš°ì¢‹ë‹¤ = 1.5x (ì¢‹ë‹¤ì˜ ì ìˆ˜)\n",
    "\n",
    "- AFINN vs TextBlob\n",
    "    - AFINN : ë” ì •í™•í•  ì ìˆ˜ ë§¤í•‘\n",
    "    - TextBlob : ë” ì¼ë°˜ì ì¸ ì ‘ê·¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "af = Afinn()\n",
    "text1 = \"TextBlob is amazingly simple to use.\"\n",
    "text2 = \"What a wonderful library for NLP!\"\n",
    "score1 = af.score(text1)\n",
    "score2 = af.score(text2)\n",
    "score1, score2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78b05b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px\"> \n",
    "==========================================\n",
    "\n",
    "#### â–¶ VADER  ì†Œì…œ ë¯¸ë””ì–´ í…ìŠ¤íŠ¸ì— ìµœì í™”\n",
    "- ì´ ì˜í™”ëŠ” ì •ë§ì •ë§ í›Œë¥­í•´ !!\n",
    "- í›Œë¥­í•˜ë‹¤ (ê¸°ë³¸) +0.7  / ì •ë§ì •ë§ (ê°•ì¡°) x1.5\n",
    "- !!!(ë¬¸ì¥ë¶€í˜¸ê°•ì¢…) x1.2\n",
    "- 4ê°œì˜ ê°ì • ì§€ìˆ˜\n",
    "    - positive ê¸ì • í™•ë¥  0 ~ 1\n",
    "    - negative ë¶€ì • í™•ë¥ \n",
    "    - neutral ì¤‘ë¦½ í™•ë¥ \n",
    "    - compound ì¢…í•©ì ìˆ˜ -1 ~ 1\n",
    "- score = compound_score / sqrt(compound_score**2 + 0.0625)   #0.0625 : ê³ ì •ê°’?\n",
    "    - score >= 0.05 ê¸ì •\n",
    "    - score <= -0.05 ë¶€ì •\n",
    "    - ê·¸ ì‚¬ì´ëŠ” ì¤‘ë¦½\n",
    "- ëŒ€ì†Œë¬¸ì êµ¬ë¶„ AMAZING amazing ë‹¤ë¥¸ ì ìˆ˜ë¥¼ ê°€ì§\n",
    "- :-) ê¸ì • /  :-( ë¶€ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f80a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\playdata2\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa79cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ë¬¸ì¥: I love this product! It's absolutely amazing ğŸ˜, \n",
      " ì ìˆ˜: {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.862}\n",
      " ë¬¸ì¥: This is the worst movie I've ever seen..., \n",
      " ì ìˆ˜: {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
      " ë¬¸ì¥: The food was okay, not great but not bad either., \n",
      " ì ìˆ˜: {'neg': 0.149, 'neu': 0.487, 'pos': 0.364, 'compound': 0.4728}\n",
      " ë¬¸ì¥: Iâ€™m REALLY happy with the results!!!, \n",
      " ì ìˆ˜: {'neg': 0.0, 'neu': 0.472, 'pos': 0.528, 'compound': 0.7651}\n",
      " ë¬¸ì¥: Not good at all. Iâ€™m disappointed., \n",
      " ì ìˆ˜: {'neg': 0.579, 'neu': 0.421, 'pos': 0.0, 'compound': -0.6711}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyer = SentimentIntensityAnalyzer()\n",
    "sentences = [\n",
    "    \"I love this product! It's absolutely amazing ğŸ˜\",\n",
    "    \"This is the worst movie I've ever seen...\",\n",
    "    \"The food was okay, not great but not bad either.\",\n",
    "    \"Iâ€™m REALLY happy with the results!!!\",\n",
    "    \"Not good at all. Iâ€™m disappointed.\",\n",
    "]\n",
    "for s in sentences:\n",
    "    scores = analyer.polarity_scores(s)\n",
    "    print(f' ë¬¸ì¥: {s}, \\n ì ìˆ˜: {scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# nltk ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41c4ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000, 1000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì˜í™” ë¦¬ë·° ë°ì´í„° ë¡œë“œ\n",
    "fileids = movie_reviews.fileids()\n",
    "\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "\n",
    "# ì†ë„ë¥¼ ìœ„í•´ ë°ì´í„° í•œì •\n",
    "# reviews = [movie_reviews.raw(fileid) for fileid in fileids[:50]] + \\\n",
    "#         [movie_reviews.raw(fileid) for fileid in fileids[-50:]]\n",
    "# categories = [movie_reviews.categories(fileid)[0] for fileid in fileids[:50]]+\\\n",
    "#         [movie_reviews.categories(fileid)[0] for fileid in fileids[-50:]]\n",
    "\n",
    "# ë¦¬ë·° ì´ê°œìˆ˜ / ê¸ì • ë¶€ì • ë¦¬ë·° ê°¯ìˆ˜\n",
    "len(reviews), categories.count('pos'), categories.count('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74b10a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ì •í™•ë„ :  0.60\n"
     ]
    }
   ],
   "source": [
    "#### 1 TextBlob\n",
    "def sentiment_textblob(docs):\n",
    "    return ['pos' if TextBlob(doc).sentiment.polarity>0 else 'neg' for doc in docs]\n",
    "pred_textblob = sentiment_textblob(reviews)\n",
    "accuracy_textblob = accuracy_score(categories, pred_textblob)\n",
    "print ( f' ì •í™•ë„ : {accuracy_textblob: .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e8b6f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ì •í™•ë„ :  0.66\n"
     ]
    }
   ],
   "source": [
    "#### 2 AFINN\n",
    "def sentiment_afinn(docs):\n",
    "    afn = Afinn (emoticons=True)\n",
    "    return [ 'pos' if afn.score(doc)>0 else 'neg' for doc in docs]\n",
    "pred = sentiment_afinn(reviews)\n",
    "accuracy = accuracy_score(categories, pred)\n",
    "print ( f' ì •í™•ë„ : {accuracy: .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3590ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ì •í™•ë„ :  0.64\n"
     ]
    }
   ],
   "source": [
    "#### 3 VADER\n",
    "def sentiment_vader(docs):\n",
    "    vad = SentimentIntensityAnalyzer()\n",
    "    return ['pos' if vad.polarity_scores(doc) ['compound']>0 else 'neg' for doc in docs]\n",
    "pred = sentiment_vader(reviews)\n",
    "accuracy = accuracy_score(categories, pred)\n",
    "print ( f' ì •í™•ë„ : {accuracy: .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1496e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„  / ë”¥ëŸ¬ë‹ í•´ë³´ì„¸ì—¬ - MLP ? softmax, crossentropy, loss, entropy, ê³¼ì í•© ë“± í™•ì¸\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c1387f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ\n",
    "# ë² ì´ì¦ˆ ì •ë¦¬\n",
    "# \"ì¢‹ë‹¤\" ë‹¨ì–´ë¥¼ ë³¸ í›„ ì´ ë¦¬ë·°ê°€ ê¸ì •ì¼ í™•ë¥ \n",
    "# p(ê¸ì • | \"ì¢‹ë‹¤\") = p(\"ì¢‹ë‹¤\" | ì „ì²´ ê¸ì •) x p(ê¸ì •) / p(\"ì¢‹ë‹¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a75c94a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1600, 400, 400)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° ë¶„í• \n",
    "dataset = train_test_split(reviews, categories, test_size=0.2, random_state=42, stratify=categories)\n",
    "len(dataset[0]), len(dataset[2]), len(dataset[1]), len(dataset[3])\n",
    "\n",
    "\n",
    "# ì¸ë±ìŠ¤ ë³€ìˆ˜ ì´ë¦„\tì„¤ëª…\n",
    "#   0\tX_train\tí•™ìŠµìš© ì…ë ¥ ë°ì´í„°\n",
    "#   1\tX_test\tí…ŒìŠ¤íŠ¸ìš© ì…ë ¥ ë°ì´í„°\n",
    "#   2\ty_train\tí•™ìŠµìš© ë¼ë²¨\n",
    "#   3\ty_test\tí…ŒìŠ¤íŠ¸ìš© ë¼ë²¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77a34ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf ë²¡í„°í™”\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "x_train = vectorizer.fit_transform(dataset[0])\n",
    "x_test = vectorizer.transform(dataset[1])\n",
    "y_train =dataset[2]\n",
    "y_test = dataset[3]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eed35250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.81      0.82       200\n",
      "         pos       0.81      0.83      0.82       200\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 1. MNB\n",
    "mnb_clf = MultinomialNB()\n",
    "mnb_clf.fit(x_train,y_train)\n",
    "pred = mnb_clf.predict (x_test)\n",
    "print (classification_report (y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f2d5a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.84      0.81      0.82       200\n",
      "         pos       0.81      0.85      0.83       200\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "pred = lr.predict (x_test)\n",
    "print (classification_report (y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d987c992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ì„±ëŠ¥í–¥ìƒ - ì „ì²˜ë¦¬\n",
    "# ì†Œë¬¸ìë³€í™˜ - ì—°ì†ëœ ë¬¸ìì—´ ì¤‘ì— 3ê¸€ì ì´ìƒ - ì–´ê°„ì¶”ì¶œ(í˜•íƒœì†Œ ë¶„ì„) - ë¶ˆìš©ì–´ ì œê±°\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    return [porter.stem(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "vector = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    max_features=1000,\n",
    "    min_df=5,\n",
    "    max_df=0.5,\n",
    "    ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "x_train = vector.fit_transform(dataset[0])\n",
    "x_test = vector.transform(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea714456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.03746766, 0.        ,\n",
       "        0.03645065],\n",
       "       [0.06204417, 0.05176801, 0.        , ..., 0.04337023, 0.        ,\n",
       "        0.04219301],\n",
       "       [0.        , 0.        , 0.        , ..., 0.0768619 , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.06536913, 0.        , 0.        , ..., 0.04569445, 0.        ,\n",
       "        0.08890828],\n",
       "       [0.10722645, 0.        , 0.        , ..., 0.07495363, 0.        ,\n",
       "        0.        ]], shape=(1600, 1000))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train\n",
    "x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "494587ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í•¨ìˆ˜\n",
    "def evaluate_model(model):\n",
    "    model.fit(x_train,y_train)\n",
    "    pred = model.predict (x_test)\n",
    "    print (f' {str(model).split(\"(\")[0]} ëª¨ë¸:\\n{classification_report (y_test,pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8f8f74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LogisticRegression ëª¨ë¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.80      0.84       200\n",
      "         pos       0.82      0.89      0.85       200\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.84      0.84      0.84       400\n",
      "weighted avg       0.84      0.84      0.84       400\n",
      "\n",
      " MultinomialNB ëª¨ë¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.80      0.82      0.81       200\n",
      "         pos       0.82      0.80      0.81       200\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.81      0.81      0.81       400\n",
      "weighted avg       0.81      0.81      0.81       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ì „ì²˜ë¦¬ í›„ ëª¨ë¸ í‰ê°€\n",
    "evaluate_model(LogisticRegression())  #ìƒê¸° ì „ì²˜ë¦¬ì—ì„œ ngram ì¶”ê°€í›„ lrëª¨ë¸ì—ì„œ 0.01 ìƒìŠ¹\n",
    "evaluate_model(MultinomialNB())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "175e4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° - í† í°í™” & ì ìˆ˜ ì¸ì½”ë”© - ì‹œí€€ìŠ¤ íŒ¨ë”©\n",
    "# baseline ì¼ë°˜ ì‹ ê²½ë§ : Dense\n",
    "# simple RNN : ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ\n",
    "# Bidirectional LSTM : ì–‘ë°©í–¥ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9375c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜\n",
    "# Word Embedding ë‹¨ì–´ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜\n",
    "# Sequence Padding ê¸¸ì´ê°€ ë‹¤ë¥¸ ë¬¸ì¥ì„ ê°™ì€ í¬ê¸°ë¡œ ê³ ì •\n",
    "# simple RNN\n",
    "# LSTM  : RNNì˜ ê²½ì‚¬ì†Œì‹¤ë¬¸ì œë¥¼ í•´ê²°\n",
    "# Bidiretional LSTM : ì–‘ë°©í–¥ ì»¨í…ì¸  í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a6869a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í°í™” : ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •\n",
    "# í† í¬ë‚˜ì´ì € 3ë‹¨ê³„\n",
    "    # 1. fit_on_text (texts) ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•´ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜\n",
    "    # 2. texts_to_sequence (texts) ê°ë¬¸ì„œë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë°˜í™˜\n",
    "    # 3. pad_sequence() ê¸¸ì´ë¥¼ ì •ê·œí™”(ê°™ì€ ê¸¸ì´)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "83e5ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë”¥ëŸ¬ë‹ì—ì„œ ì›Œë“œ ì„ë² ë”© ë ˆì´ì–´ : ê° ë‹¨ì–´ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ì‹¤ìˆ˜ ë²¡í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5babba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dffdd24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì´í† ì¹˜ ë²„ì „ (ì±…ì€ tensorflow)\n",
    "\n",
    "# í† í°í™”\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "sample_reviews = [\n",
    "    \"this movie is great and wonderful\",\n",
    "    \"bad movie with poor acting\",\n",
    "    \"great movie absolutely wonderful\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b621c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì–´ ë¶„í•  ë° ë¹ˆë„ ê³„ì‚°\n",
    "all_words = []\n",
    "for i in [review.split() for review in sample_reviews]:\n",
    "    all_words.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a7a6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì–´ë¹ˆë„\n",
    "word_freq = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7087660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 1,\n",
       " 'movie': 2,\n",
       " 'great': 3,\n",
       " 'wonderful': 4,\n",
       " 'this': 5,\n",
       " 'is': 6,\n",
       " 'and': 7,\n",
       " 'bad': 8,\n",
       " 'with': 9,\n",
       " 'poor': 10}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Tokenizer êµ¬í˜„\n",
    "class SimpleTokenizer :\n",
    "    def __init__ (self, num_words=10, oov_token = 'UNK'):\n",
    "        self.num_words = num_words  #ë‹¨ì–´ë¥¼ ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ëª‡ê°œê¹Œì§€ ì“¸ì§€ (ê°€ì¥ ìì£¼ ë“±ì¥í•œ ë‹¨ì–´ë§Œ ë‚¨ê¹€)\n",
    "        self.oov_token = oov_token  #ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´(Out-Of-Vocabulary, OOV)ë¥¼ ëŒ€ì²´í•  íŠ¹ìˆ˜ í† í° ì´ë¦„ (â€˜UNKâ€™ = unknown)\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "\n",
    "\n",
    "        # oov_token ì„¤ëª…\n",
    "        # tokenizer.fit_on_texts([\"i love apple\"])\n",
    "        # tokenizer.texts_to_sequences([\"i love banana\"])\n",
    "        # â†’ \"banana\"ëŠ” ì‚¬ì „ì— ì—†ì£ ? -> OOVí† í° -> \"banana\"ì²˜ëŸ¼ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ëŠ” \"UNK\"ë¼ëŠ” íŠ¹ìˆ˜ ë‹¨ì–´ë¡œ ëŒ€ì²´\n",
    "\n",
    "\n",
    "\n",
    "    # ë‹¨ì–´ì‚¬ì „ ë§Œë“¤ê¸° : ë¬¸ì¥ì—ì„œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³ , ë¹ˆë„ ë†’ì€ ìˆœì„œë¡œ ì¸ë±ìŠ¤ ë¶€ì—¬\n",
    "    def fit_on_texts(self, texts):\n",
    "        '''ë¬¸ì¥ì„ ë‹¨ì–´ ì¸ë±ìŠ¤ë¡œ ë°˜í™˜'''\n",
    "        all_words = []\n",
    "        for i in [review.split() for review in sample_reviews]:\n",
    "            all_words.extend(i)\n",
    "        word_freq = Counter(all_words)\n",
    "        \n",
    "        # oov í† í°ì„ 1ë¡œ ì„¤ì •\n",
    "        self.word_index[self.oov_token] = 1  # 'UNK' (ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ìš©) â†’ 1ë²ˆìœ¼ë¡œ ì„¤ì •\n",
    "        self.index_word[1] = self.oov_token\n",
    "        # ë‚˜ë¨¸ì§€ ë‹¨ì–´ëŠ” ë§ì´ ë“±ì¥í•œ ìˆœì„œëŒ€ë¡œ 2,3,4,... ìˆœì„œ\n",
    "        idx = 2\n",
    "        for word, _ in word_freq.most_common(self.num_words-1) :\n",
    "            self.word_index[word] = idx\n",
    "            self.index_word[idx] = word\n",
    "            idx += 1\n",
    "                        # most_common() : Counter ê°ì²´ì˜ ë‚´ì¥ í•¨ìˆ˜\n",
    "                        # ğŸ‘‰ ë“±ì¥ ë¹ˆë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ë‹¨ì–´ì™€ ë¹ˆë„ë¥¼ íŠœí”Œ í˜•íƒœë¡œ ë°˜í™˜\n",
    "                        # ì˜ˆ. ord_freq.most_common(2) --> ê°€ì¥ ìì£¼ ë‚˜ì˜¨ ë‹¨ì–´ 2ê°œë§Œ ë°˜í™˜\n",
    "                        # most_common(n) â†’ ê°€ì¥ ìì£¼ ë‚˜ì˜¨ nê°œ ë‹¨ì–´ë¥¼ ë°˜í™˜\n",
    "\n",
    "                        # word_freq.most_common(self.num_words-1) \n",
    "                        # ìœ„ì— oov_token ì´ 1ë²ˆ ìë¦¬ë¥´ ì°¨ì§€í•˜ê³  ìˆì–´ì„œ -1ë¡œ ê³„ì‚°í•¨ (OOVí† í° ì œì™¸)\n",
    "                        \n",
    "\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜\n",
    "    # ì˜ˆì‹œ\n",
    "    # {'UNK': 1, 'i': 2, 'movie': 3, 'love': 4, 'hate': 5}\n",
    "    # texts = [\"i love movie\", \"i like movie\"]\n",
    "    # tokenizer.texts_to_sequences(texts)\n",
    "    # [[2, 4, 3], [2, 1, 3]]\n",
    "    def texts_to_sequences (self, texts):\n",
    "        '''í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜'''\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = []\n",
    "            for word in text.split():\n",
    "                # ë‹¨ì–´ê°€ vocabularyì— ìˆìœ¼ë©´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©, ì—†ìœ¼ë©´ oov\n",
    "                word_index = self.word_index.get(word,1)\n",
    "                seq.append(word_index)\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "                                            # ë”•ì…”ë„ˆë¦¬.get()ë¬¸ë²•\n",
    "                                            # dict.\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer ìƒì„± ë° í•™ìŠµ\n",
    "tokenizer = SimpleTokenizer(num_words=10, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(sample_reviews)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3a62ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 2, 6, 3, 7, 4], [8, 2, 9, 10, 1], [3, 2, 1, 4]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02df6a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  5,  2,  6,  3,  7,  4],\n",
       "       [ 0,  0,  0,  0,  0,  8,  2,  9, 10,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  3,  2,  1,  4]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íŒ¨ë”© êµ¬í˜„ - ë¬¸ìì—´ì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶˜ë‹¤\n",
    "def pad_sequence_manual (sequences, max_len=10, padding='pre', value=0):\n",
    "    '''íŒ¨ë”©êµ¬í˜„'''\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= max_len :\n",
    "            if padded == 'pre':\n",
    "                padded_seq = seq[-max_len]\n",
    "            else :\n",
    "                padded_seq = seq [:max_len]\n",
    "        else :\n",
    "            pad_length = max_len-len(seq)\n",
    "            if padding == 'pre':\n",
    "                padded_seq = [value]*pad_length + seq\n",
    "            else :\n",
    "                padded_seq = seq + [value]*pad_length\n",
    "        padded.append(padded_seq)\n",
    "    return np.array(padded)\n",
    "padded = pad_sequence_manual(sequences)\n",
    "padded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e65a0fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  5,  2,  6,  3,  7,  4],\n",
       "        [ 0,  0,  0,  0,  0,  8,  2,  9, 10,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  3,  2,  1,  4]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch tensor ë³€í™˜\n",
    "sequences_tensor = torch.LongTensor(padded)\n",
    "sequences_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd13e7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " íŒ¨ë”©ëœ ì‹œí€€ìŠ¤ í˜•íƒœ : torch.Size([3, 10])\n",
      " ì²«ë²ˆì§¸ : tensor([0, 0, 0, 0, 5, 2, 6, 3, 7, 4])\n",
      "ì…ë ¥í˜•íƒœ : torch.Size([3, 10])\n",
      "ì¶œë ¥í˜•íƒœ : torch.Size([3, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# 2. ì›Œë“œ ì„ë² ë”©\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f' íŒ¨ë”©ëœ ì‹œí€€ìŠ¤ í˜•íƒœ : {sequences_tensor.shape}')\n",
    "print (f' ì²«ë²ˆì§¸ : {sequences_tensor[0]}')\n",
    "\n",
    "# pytorch embedding ë ˆì´ì–´ ìƒì„±\n",
    "    # num_ebeddings : ê° ë‹¨ì–´ë¥¼ ëª‡ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„í•  ê²ƒì¸ì§€\n",
    "    # padding_idx ê°™ì´ ë§ì¶œë•Œ ì±„ìš°ëŠ” ê°’\n",
    "embedding_layer = nn.Embedding(num_embeddings=1000, embedding_dim=8, padding_idx=0)\n",
    "embedded = embedding_layer(sequences_tensor)\n",
    "print(f'ì…ë ¥í˜•íƒœ : {sequences_tensor.shape}')\n",
    "print(f'ì¶œë ¥í˜•íƒœ : {embedded.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30de6f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ë‹¨ì–´ idf 0: [0. 0. 0. 0. 0. 0. 0. 0.] -- 8ì°¨ì› ì¤‘ ì²˜ìŒ 4ê°œ\n",
      " ë‹¨ì–´ idf 0: [0. 0. 0. 0. 0. 0. 0. 0.] -- 8ì°¨ì› ì¤‘ ì²˜ìŒ 4ê°œ\n",
      " ë‹¨ì–´ idf 0: [0. 0. 0. 0. 0. 0. 0. 0.] -- 8ì°¨ì› ì¤‘ ì²˜ìŒ 4ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© ë²¡í„° ìƒì„¸ ë¶„ì„\n",
    "# ìƒ˜í”Œ ë°ì´í„°ì˜ ì²« 3ê°œë‹¨ì–´ ì„ë² ë”©\n",
    "for word_idx in range(3):\n",
    "    embedding_vec = embedded[0,word_idx].detach().numpy()\n",
    "    word_id = sequences_tensor[0, word_idx].item()\n",
    "    print(f' ë‹¨ì–´ idf {word_id}: {embedding_vec} -- 8ì°¨ì› ì¤‘ ì²˜ìŒ 4ê°œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "471b5d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© í–‰ë ¬ í˜•íƒœ : (1000, 8)\n",
      "íŒ¨ë”©(id=0)ì˜ ì„ë² ë”© [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "ë‹¨ì–´ id=5 [ 0.43877324 -1.3464957   0.95847356 -1.3694234  -0.8483197  -1.313673\n",
      "  0.08624984  0.55934185]\n"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© í–‰ë ¬\n",
    "embedding_matrix = embedding_layer.weight.detach().numpy()\n",
    "print (f'ì„ë² ë”© í–‰ë ¬ í˜•íƒœ : {embedding_matrix.shape}')\n",
    "print (f'íŒ¨ë”©(id=0)ì˜ ì„ë² ë”© {embedding_matrix[0]}')\n",
    "print (f'ë‹¨ì–´ id=5 {embedding_matrix[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02698ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rnn ì ìš©\n",
    "class RnnModule(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim ):\n",
    "        super(RnnModule, self).__init__()\n",
    "        self.embdding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward (self, x):\n",
    "        x_emb = self.embdding(x)  #(batch, seq_len, embedding_dim) ì¶œë ¥\n",
    "        rnn_out, h_n = self.rnn(x_emb)  \n",
    "            # rnn_out(batch, seq_len, hidden_dim)\n",
    "            # h_n(1, batch, hidden_dim) \n",
    "\n",
    "        #ë§ˆì§€ë§‰ ìŠ¤í…ì˜ ì¶œë ¥\n",
    "        last_output = rnn_out[:, -1, :]  #(batch, hidden_dim) ì¶œë ¥\n",
    "        output = self.sigmoid(self.fc(last_output))\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
