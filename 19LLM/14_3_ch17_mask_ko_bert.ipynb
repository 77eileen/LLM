{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH8JuxxYusWN"
      },
      "source": [
        "- Transformer는 encoder --> decoder\n",
        "- encoder를 이용해서 만든 언어모델 BERT : 감성분류, 스팸, 개체명인식, 유사도 측정 --> BERT는 이해, 분류를 잘함\n",
        "- decoder를 이용해서 만든 언어모델 GPT : 언어 추론 요약, QA챗봇 --> GPT는 생성을 잘함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvjPJY911psd"
      },
      "source": [
        "KoBERT를 이용해서 빈 단어 맞추기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# colab에서 해보기!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lvbCQwRcv6Oj"
      },
      "outputs": [],
      "source": [
        "# BERT가 잘하는 것 : 분류, 빈칸 추론, 추측, 문장 임베딩 (생성XX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouBtNP5ywW1i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: \"'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer\": Expected package name at the start of dependency specifier\n",
            "    'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer\n",
            "    ^\n",
            "Hint: = is not a valid operator. Did you mean == ?\n",
            "'subdirectory'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "# %pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OoAOAwxweOb",
        "outputId": "6f0efe62-87cf-4fbd-ffd8-b12c56872663"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kobert_tokenizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkobert_tokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KoBERTTokenizer\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m KoBERTTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskt/kobert-base-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kobert_tokenizer'"
          ]
        }
      ],
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBFJrmr0w7ET"
      },
      "outputs": [],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7pjxtRkswpV6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=8002, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# mask를 씌우는 모델\n",
        "\n",
        "from transformers import BertForMaskedLM\n",
        "model = BertForMaskedLM.from_pretrained('skt/kobert-base-v1')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-xn9FDVxYE6",
        "outputId": "105adae7-643c-4fa2-8a0c-df9d0c9bb1a5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m한국의 수도는 [MASK]입니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id, input_ids)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 출력 : (4, tensor([[   2, 4962, 2874, 5760,    4,  517, 7139,   54,    3]]))\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# tokenizer.mask_token_id ==> 4\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# mask_token_id 위치 찾기\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# torch.where(input_ids == tokenizer.mask_token_id) ==> (tensor([0]), tensor([4]))\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "# 1. 상식 추론\n",
        "import torch\n",
        "text = \"한국의 수도는 [MASK]입니다.\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "print(tokenizer.mask_token_id, input_ids)\n",
        "# 출력 : (4, tensor([[   2, 4962, 2874, 5760,    4,  517, 7139,   54,    3]]))\n",
        "# tokenizer.mask_token_id ==> 4\n",
        "\n",
        "# mask_token_id 위치 찾기\n",
        "# torch.where(input_ids == tokenizer.mask_token_id) ==> (tensor([0]), tensor([4]))\n",
        "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "\n",
        "# 마스크위치에서 가까운 상위 5개 추출\n",
        "# 추론\n",
        "with torch.no_grad():\n",
        "    output = model(input_ids)\n",
        "    print(output) # ==> logits 가 정답임\n",
        "    predictions = output.logits # 정렬해서 예측함..???\n",
        "print(predictions.shape) # ==> 출력: torch.Size([1, 9, 8002]) ==> 9: 문장길이, 8002 : 임베딩갯수(차원)\n",
        "print('===============')\n",
        "\n",
        "print(predictions[0, mask_token_index, :])\n",
        "print('===============')\n",
        "masked_prediciton = predictions[0, mask_token_index, :].topk(5) # 상위 5개\n",
        "print(masked_prediciton)\n",
        "\n",
        "for i, index_t in enumerate(masked_prediciton.indices[0]):\n",
        "    index = index_t.item()\n",
        "    print(tokenizer.decode([index]))\n",
        "\n",
        "# 결과 좋지않음.\n",
        "# skt/kobert-base-v1 단어맞추는 출력 헤더가 붙어 있어서 MASK를 맞추는게 가능함.\n",
        "# kobert 모델에 가중치가 없는 것 같음.\n",
        "# skt/kobert-base-v1 모델은 Masked Language Model(MLM)으로 파인튜닝된 적이 없어서 정확한 마스크 예측 능력이 떨어짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABv94xvL2Jal"
      },
      "source": [
        "### huggingface의 klue/bert-base 이용\n",
        "- https://huggingface.co/klue/bert-base#how-to-get-started-with-the-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdM2mE9r2Jxl",
        "outputId": "56043c2b-d003-4b5e-8373-147916461ef3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "name=\"klue/bert-base\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "\n",
        "\n",
        "# AutoModelForMAskedLM 이라서 추론이 가능함?\n",
        "# 그냥 bert모델은 추론할 수 있는 헤드가 없음..?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ernt_4T2PpN",
        "outputId": "9dcc2720-76c5-4c44-9e0e-d48144233c73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-5.1762,  6.2289, -4.8118,  ..., -5.0996, -3.0230, -3.9644]])\n",
            "서울\n",
            "광화문\n",
            "부산\n",
            "평양\n",
            "인천\n"
          ]
        }
      ],
      "source": [
        "# 1. 상식 추론\n",
        "import torch\n",
        "text = \"대한민국의 수도는 [MASK]입니다.\"\n",
        "inputs = tokenizer(text,return_tensors='pt')\n",
        "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "# 추론\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "predictions = outputs.logits\n",
        "print(predictions[0,mask_token_index,:])\n",
        "masked_prediction = predictions[0,mask_token_index,:].topk(5)\n",
        "for i, index_t in enumerate(masked_prediction.indices[0]):\n",
        "  index = index_t.item()\n",
        "  print(tokenizer.decode([index]))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "P10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
