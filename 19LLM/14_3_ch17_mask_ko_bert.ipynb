{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer는 encoder --> decoder\n",
        "- encoder를 이용해서 만든 언어모델 BERT : 감성분류, 스팸, 개체명인식, 유사도 측정, --> BERT는 이해, 분류를 잘함\n",
        "- decoder를 이용해서 만든 언어모델 GPT : 언어 추론 요약, QA챗봇 --> GPT는 생성을 잘함"
      ],
      "metadata": {
        "id": "WH8JuxxYusWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KoBERT를 이용해서 빈 단어 맞추기"
      ],
      "metadata": {
        "id": "PvjPJY911psd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT가 잘하는 것 : 분류, 빈칸 추론, 추측, 문장 임베딩 (생성XX)\n"
      ],
      "metadata": {
        "id": "lvbCQwRcv6Oj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "id": "ouBtNP5ywW1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OoAOAwxweOb",
        "outputId": "6f0efe62-87cf-4fbd-ffd8-b12c56872663"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "WBFJrmr0w7ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mask를 씌우는 모델\n",
        "\n",
        "from transformers import BertForMaskedLM\n",
        "model = BertForMaskedLM.from_pretrained('skt/kobert-base-v1')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "7pjxtRkswpV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 상식 추론\n",
        "import torch\n",
        "text = \"한국의 수도는 [MASK]입니다.\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "print(tokenizer.mask_token_id, input_ids)\n",
        "# 출력 : (4, tensor([[   2, 4962, 2874, 5760,    4,  517, 7139,   54,    3]]))\n",
        "# tokenizer.mask_token_id ==> 4\n",
        "\n",
        "# mask_token_id 위치 찾기\n",
        "# torch.where(input_ids == tokenizer.mask_token_id) ==> (tensor([0]), tensor([4]))\n",
        "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "\n",
        "# 마스크위치에서 가까운 상위 5개 추출\n",
        "# 추론\n",
        "with torch.no_grad():\n",
        "    output = model(input_ids)\n",
        "    print(output) # ==> logits 가 정답임\n",
        "    predictions = output.logits # 정렬해서 예측함..???\n",
        "print(predictions.shape) # ==> 출력: torch.Size([1, 9, 8002]) ==> 9: 문장길이, 8002 : 임베딩갯수(차원)\n",
        "print('===============')\n",
        "\n",
        "print(predictions[0, mask_token_index, :])\n",
        "print('===============')\n",
        "masked_prediciton = predictions[0, mask_token_index, :].topk(5) # 상위 5개\n",
        "print(masked_prediciton)\n",
        "\n",
        "for i, index_t in enumerate(masked_prediciton.indices[0]):\n",
        "    index = index_t.item()\n",
        "    print(tokenizer.decode([index]))\n",
        "\n",
        "# 결과 좋지않음.\n",
        "# skt/kobert-base-v1 단어맞추는 출력 헤더가 붙어 있어서 MASK를 맞추는게 가능함.\n",
        "# kobert 모델에 가중치가 없는 것 같음.\n",
        "# skt/kobert-base-v1 모델은 Masked Language Model(MLM)으로 파인튜닝된 적이 없어서 정확한 마스크 예측 능력이 떨어짐"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-xn9FDVxYE6",
        "outputId": "105adae7-643c-4fa2-8a0c-df9d0c9bb1a5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 tensor([[   2, 4962, 2874, 5760,    4,  517, 7139,   54,    3]])\n",
            "MaskedLMOutput(loss=None, logits=tensor([[[ 1.1842,  3.2349, -0.3676,  ..., -0.8630,  3.9171, -1.3264],\n",
            "         [ 0.6766,  0.9119, -0.1036,  ...,  0.7129, -0.8652, -1.1968],\n",
            "         [ 1.1335, -0.7245, -1.1014,  ...,  0.9701, -0.2373, -0.2842],\n",
            "         ...,\n",
            "         [-0.1160,  1.9224, -0.0723,  ...,  2.3727, -0.8437, -1.0156],\n",
            "         [ 0.5218,  1.6724, -0.3923,  ...,  0.6417,  3.3373,  0.3521],\n",
            "         [-1.1021,  0.7542, -0.2761,  ...,  1.0267,  1.4209, -1.1078]]]), hidden_states=None, attentions=None)\n",
            "torch.Size([1, 9, 8002])\n",
            "===============\n",
            "tensor([[ 0.7010,  3.3383, -0.1068,  ..., -1.1605,  2.3259, -0.3215]])\n",
            "===============\n",
            "torch.return_types.topk(\n",
            "values=tensor([[6.8893, 6.4605, 5.7966, 5.7783, 5.7722]]),\n",
            "indices=tensor([[3512, 5249, 1701, 7705, 1031]]))\n",
            "우즈\n",
            "京\n",
            "덕\n",
            "팽\n",
            "공격수\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### huggingface의 klue/bert-base 이용\n",
        "- https://huggingface.co/klue/bert-base#how-to-get-started-with-the-model"
      ],
      "metadata": {
        "id": "ABv94xvL2Jal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "name=\"klue/bert-base\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "\n",
        "\n",
        "# AutoModelForMAskedLM 이라서 추론이 가능함?\n",
        "# 그냥 bert모델은 추론할 수 있는 헤드가 없음..?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdM2mE9r2Jxl",
        "outputId": "56043c2b-d003-4b5e-8373-147916461ef3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 상식 추론\n",
        "import torch\n",
        "text = \"대한민국의 수도는 [MASK]입니다.\"\n",
        "inputs = tokenizer(text,return_tensors='pt')\n",
        "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "# 추론\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "predictions = outputs.logits\n",
        "print(predictions[0,mask_token_index,:])\n",
        "masked_prediction = predictions[0,mask_token_index,:].topk(5)\n",
        "for i, index_t in enumerate(masked_prediction.indices[0]):\n",
        "  index = index_t.item()\n",
        "  print(tokenizer.decode([index]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ernt_4T2PpN",
        "outputId": "9dcc2720-76c5-4c44-9e0e-d48144233c73"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-5.1762,  6.2289, -4.8118,  ..., -5.0996, -3.0230, -3.9644]])\n",
            "서울\n",
            "광화문\n",
            "부산\n",
            "평양\n",
            "인천\n"
          ]
        }
      ]
    }
  ]
}