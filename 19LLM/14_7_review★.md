1. 데이터
- 문장 ----> 컴퓨터가 인식못함
- 숫자로 변환
- 문장 ----> 단어단위로 분리 ----> 단어별 정수인코딩
		- 단어별정수인코딩{나는: 1, 너를:2,  ... } =====> <문제점> 단지 문자를 고유한 숫자로 매핑한 것에 불과 / 즉, one-hot ===> 단어간 관계가 전혀 없다. 모델이 학습해도 의미 없음
- =====> 벡터 도입: 유사한 단어들은 서로 비슷한 크기와 방향을 갖도록 함/ 이러한 기능은 embedding 벡터 기능을 제공(by 라이브러리) == 토크나이저

<br>
- "토크나이저" 
	- 문장을 벡터화까지 하는게 토크나이저
	- 토크나이저 출력 : 모델의 학습용 데이터

<br>

- "토크나이저+모델의 학습용 데이터"
	- 허깅페이스라는 애들이 해당모델과 해당 토크나이저를 가지고 있음
<br>

- 셀프어텐션을 인코딩하면..?
- input_ids, token_type_id, attention_mask

<br>

- [입력 문장] --> [인코더] --> [컨텍스트 벡터 / 숨은 상태] --> [디코더] --> [출력 문장]
- 항상 먼저 토크나이즈를 하고 난 뒤에 인코더/디코더에 넣는다.
<br>

| 모델 유형          | 구조      | 용도             | 흐름 요약                                                                                     |
|-----------------|---------|----------------|--------------------------------------------------------------------------------------------|
| Seq2Seq        | 인코더+디코더 | 요약, 번역, 조건부 생성 | [입력 문장] → 토크나이즈 → [인코더] → 숨은 상태(hidden states / context) → [디코더] → 다음 토큰 예측 반복 → 토큰 ID → 디코딩 → "사람이 읽을 수 있는 문장" |
| Autoregressive (GPT) | 디코더만    | 자유 생성, 채팅, 글쓰기 | [프롬프트 문장] → 토크나이즈 → [디코더] → 다음 토큰 예측 반복 → 토큰 ID → 디코딩 → "사람이 읽을 수 있는 문장" |
| Encoder-only (Bert)   | 인코더만    | 분류, QA, 태깅     | [입력 문장] → 토크나이즈 → [인코더(BERT)] → hidden states → [CLS 토큰 추출] → [선형 레이어] → 로짓 → softmax → 클래스 예측 |

<br>

- 토크나이즈가 항상 먼저
- 인코더와 디코더는 모델 내부에서 연결
- 대부분 model.generate()를 호출하면 모델 내부에서 인코더와 디코더를 자동으로 연결되어 처리
![alt text]({C2857F97-6975-41D5-9909-A3EB11591652}.png)

- 우리가 신경 쓰는 부분
	- 토크나이즈만 먼저 해주면 됨
	- 모델 내부에서 인코더/디코더 연결
	- 디코딩(토큰 → 문자열)은 tokenizer.decode()로 처리










<br>

[자연어 학습]
- seq2seq
- seq2seq : 인코더 --> 디코더
	- 인코더 특징 : LSTM을 이용해서 마지막 hidden_state에 문장의 의미를 압축해서 저장 --> 특성을 잘 파악함
	- 디코더의 특징: 인코더의 마지막 레이어의 정보를 이용해서 단어를 하나씩 생성 ---> 언제까지? 모르니까, 종료를 나타내는 특수 토큰을 만날때까지 --> 생성을 잘함.
        - LSTM의 태생적한계 --> RNN 기반 --> 오래된 기억은 점차 소멸 --> 문맥의 의미가 왜곡될 수 있음
	- ===> 이것을 보완하기 위해 나온게 Attention : 문장에서 중요한 단어에 집중
	- ======> 한단계 더 업그레이드한게 self-attention : 문장이 단어들간의 중요도 (집중)

<br> 그래서, 인코더와 디코더 사이에 attetion이 들어감.
<br>

- BERT : 인코더를 이용한 모델 --> 특성 (이해) 잘 파악 하는데 유리 /있는문장의 특징을 파악해서 있는 단어를 가지고 예측.. 소스기반 
- GPT : 디코더를 이용한 모델 --> 생성, 유사도 측정에 유리 / 앞문장을 파악해서 새로운 내용을 생성. ... 성능을 높이려면 GPT 이용..




<br>
<br>

2. 학습하려면,
- 모델선택 (내가 생성할지, 에측할지 등에 맞춰서) ---> 학습용 데이터 확보 ----> 데이터셋/데이터로더------------------>최종 학습루프 
- <모델선택> : AutoTokenizer, AutoModel --> IF 모델이 출력기능이 없다면 모델클래스를 만들어서 선형데이터를 붙여준다(대표: KoBert)
- <데이터셋/데이터로더> : 클래스로 설계 




