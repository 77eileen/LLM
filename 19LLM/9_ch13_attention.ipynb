{
 "cells": [
  {
   "attachments": {
    "{41C2ABC1-C7B0-48D4-AB4F-F875F4E4EC09}.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "49f5351a",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px\">\n",
    "\n",
    "Attention Mechanism — “집중의 기술”\n",
    "\n",
    "---\n",
    "\n",
    "## Seq2Seq의 한계 — 왜 Attention이 등장했는가?\n",
    "\n",
    "### 개념 설명  \n",
    "Seq2Seq(Sequence-to-Sequence) 모델은 “입력 문장을 인코더가 요약해서 벡터로 만든 뒤,  \n",
    "디코더가 그 벡터를 이용해 출력 문장을 하나씩 만들어내는 구조”입니다.  \n",
    "예: 영어 문장 “I love you”를 한국어로 번역한다면,  \n",
    "인코더는 이 문장을 벡터 하나로 요약하고, 디코더는 “나는 너를 사랑해”를 단어별로 생성합니다.\n",
    "\n",
    "### 왜 필요한가  \n",
    "초기 Seq2Seq는 “고정된 하나의 벡터(=문장 전체 요약)”에 모든 정보를 담으려 했습니다.  \n",
    "짧은 문장은 괜찮지만, 문장이 길어질수록 문제가 생깁니다.  \n",
    "중간 내용이 ‘압축’ 과정에서 잃어버려지는 거죠.  \n",
    "즉, **기억력이 짧은 번역가**처럼 앞부분 내용을 잊어버립니다.\n",
    "\n",
    "###  어떻게 동작하는가 (한계점)  \n",
    "- 인코더는 입력 문장을 끝까지 읽고, 그 결과를 하나의 벡터에 저장합니다.  \n",
    "- 디코더는 이 벡터 하나만 보고 전체 번역을 수행합니다.  \n",
    "결과적으로, **문장의 길이가 길면 정보 손실**이 발생합니다.  \n",
    "\n",
    "###  비유  \n",
    "학생이 긴 문장을 외워서 요약한 뒤 발표한다고 생각해보세요.  \n",
    "10단어짜리 문장은 외울 수 있지만, 100단어짜리는 요약하다가 빠뜨립니다.  \n",
    "Attention은 이 문제를 해결하기 위해 등장한 **‘집중 메커니즘’**입니다.  \n",
    "\n",
    "###  핵심 요약  \n",
    "Seq2Seq는 긴 문장에서 정보 손실이 생긴다 →  \n",
    "Attention은 “필요할 때, 필요한 부분만 집중해서 보는 방법”을 제공한다.  \n",
    "\n",
    "---\n",
    "\n",
    "##  Attention의 기본 개념 — 중요한 정보에 집중하기\n",
    "\n",
    "###  개념 설명  \n",
    "Attention은 “모든 입력 단어를 다 보되, 현재 내가 예측하려는 단어와 **가장 관련 있는 부분**에 더 주목하는 방법”입니다.  \n",
    "즉, 문장의 전체 맥락을 보면서도, 특정 순간에는 일부 단어에 집중합니다.\n",
    "\n",
    "###  왜 필요한가  \n",
    "디코더가 단어를 생성할 때,  \n",
    "“이 단어를 예측할 때는 입력 문장의 어떤 단어가 가장 중요할까?”를 판단해야 합니다.  \n",
    "예를 들어, “나는 사과를 먹는다” → “I eat an apple”을 번역할 때,  \n",
    "‘사과’를 번역할 때는 ‘apple’에 집중해야 하고, ‘먹는다’를 번역할 때는 ‘eat’에 집중해야 하죠.\n",
    "\n",
    "###  어떻게 동작하는가  \n",
    "1. 디코더가 현재 단어를 예측하려고 할 때,  \n",
    "2. 인코더의 모든 단어에 대해 “관련도 점수(score)”를 계산합니다.  \n",
    "3. 이 점수를 **Softmax**를 통해 확률처럼 정규화합니다.  \n",
    "4. 높은 점수를 받은 단어일수록 **더 크게 반영**합니다.  \n",
    "\n",
    "###  비유  \n",
    "학생이 시험 문제를 풀 때, 교과서 전체를 다 보는 대신  \n",
    "“이 문제와 관련된 부분”만 집중해서 보는 것과 같습니다.  \n",
    "Attention은 모델에게 **‘집중력’을 주는 기술**입니다.\n",
    "\n",
    "###  핵심 요약  \n",
    "Attention은 “모든 단어를 다 보지만, 중요한 단어에 가중치를 높이는 메커니즘”이다.  \n",
    "\n",
    "---\n",
    "\n",
    "##  Query, Key, Value의 역할 이해\n",
    "\n",
    "###  개념 설명  \n",
    "Attention은 세 가지 요소로 구성됩니다:  \n",
    "- **Query(질문)**: “지금 내가 예측하려는 단어는 어떤 정보를 원할까?”  \n",
    "- **Key(열쇠)**: “각 입력 단어가 가지고 있는 주제나 의미의 특징”  \n",
    "- **Value(값)**: “그 단어가 실제로 가진 정보”  \n",
    "\n",
    "###  왜 필요한가  \n",
    "Query와 Key를 비교하면, 어떤 입력 단어가 지금 예측하려는 단어와 관련 있는지를 판단할 수 있습니다.  \n",
    "즉, Query ↔ Key의 유사도를 계산하면 **집중할 대상**이 정해집니다.  \n",
    "그 뒤 Value를 조합해 최종 문맥(Context)을 만듭니다.  \n",
    "\n",
    "###  어떻게 동작하는가  \n",
    "1. 디코더에서 Query 벡터를 만든다.  \n",
    "2. 인코더의 각 단어에서 Key와 Value 벡터를 만든다.  \n",
    "3. Query와 각 Key의 유사도를 계산해 점수를 매긴다.  \n",
    "4. 점수를 Softmax로 변환하여 확률처럼 만든다.  \n",
    "5. Value들을 이 확률로 가중합(weighted sum)하여 문맥(Context) 벡터를 만든다.\n",
    "\n",
    "###  비유  \n",
    "Query는 “지금 찾고 싶은 질문”,  \n",
    "Key는 “책의 목차”,  \n",
    "Value는 “책의 실제 내용”입니다.  \n",
    "Query가 “사과란 무엇인가?”일 때, Key 중 ‘과일’과 관련된 부분이 높게 매칭되고,  \n",
    "그 Value(‘사과는 달콤한 과일이다’)가 문맥으로 선택됩니다.\n",
    "\n",
    "###  핵심 요약  \n",
    "Query는 “무엇을 찾을지”, Key는 “무엇을 가지고 있는지”, Value는 “그 내용”이다.  \n",
    "\n",
    "---\n",
    "\n",
    "##  유사도 점수(Score) 계산과 Softmax의 의미\n",
    "\n",
    "###  개념 설명  \n",
    "Attention은 “Query와 Key가 얼마나 비슷한가?”를 점수로 계산합니다.  \n",
    "이 점수가 높을수록 그 단어에 더 집중합니다.  \n",
    "이때 Softmax는 점수를 0~1 사이의 **확률처럼 변환**해 전체 비중을 정리합니다.  \n",
    "\n",
    "###  왜 필요한가  \n",
    "모든 단어의 중요도를 수치로 표현하려면,  \n",
    "전체 비중의 합이 1이 되도록 정규화해야 합니다.  \n",
    "그래야 “가장 중요한 단어가 어디인지” 명확하게 알 수 있습니다.  \n",
    "\n",
    "###  어떻게 동작하는가  \n",
    "- Query와 Key의 유사도를 계산 → 점수(score)  \n",
    "- Softmax를 적용 → 각 단어의 중요도를 확률처럼 변환  \n",
    "- 높은 확률 = 높은 집중도  \n",
    "\n",
    "###  비유  \n",
    "학생이 교과서 내용을 훑으면서 “이 문제랑 관련된 문단이 어디 있지?” 하고  \n",
    "각 문단에 0~1 사이의 집중도를 매긴다고 생각해보세요.  \n",
    "“이 단락이 제일 중요해 (0.7), 저건 조금 덜 중요해 (0.2), 나머지는 거의 상관없어 (0.1)”  \n",
    "\n",
    "###  핵심 요약  \n",
    "유사도 점수는 “얼마나 관련 있는가”,  \n",
    "Softmax는 “그 비중을 확률로 정리”하는 단계다.  \n",
    "\n",
    "---\n",
    "\n",
    "##  Attention으로 만들어지는 문맥(Context) 벡터\n",
    "\n",
    "###  개념 설명  \n",
    "문맥 벡터(Context vector)는  \n",
    "모든 단어의 Value를 “집중 비율(Softmax 결과)”로 섞은 결과물입니다.  \n",
    "즉, 디코더는 이 문맥 벡터를 이용해 **현재 단어를 예측**합니다.\n",
    "\n",
    "###  왜 필요한가  \n",
    "문맥 벡터는 “필요할 때마다 다시 계산되는 문장 요약”입니다.  \n",
    "Seq2Seq는 단 하나의 고정 요약을 썼다면,  \n",
    "Attention은 단어마다 다른 요약을 만들어냅니다.\n",
    "\n",
    "###  어떻게 동작하는가  \n",
    "각 단어의 Value × 중요도(Softmax score)를 곱하고,  \n",
    "모두 더하면 현재 시점의 문맥 벡터가 됩니다.  \n",
    "이 벡터는 디코더의 다음 단어 예측에 직접 사용됩니다.  \n",
    "\n",
    "###  비유  \n",
    "교사가 학생에게 “이 문장에 ‘먹는다’라는 말이 나올 때,  \n",
    "앞부분의 ‘사과’와 ‘밥’을 중심으로 다시 문맥을 생각해봐”라고 알려주는 것과 같습니다.  \n",
    "즉, 매 단어마다 **다른 시점의 요약본**이 만들어지는 셈이죠.\n",
    "\n",
    "###  핵심 요약  \n",
    "Attention은 “단어별로 새롭게 만들어지는 문맥 요약 벡터”를 제공한다.  \n",
    "\n",
    "---\n",
    "\n",
    "##  Attention이 들어간 Seq2Seq 구조\n",
    "\n",
    "###  개념 설명  \n",
    "Attention이 추가된 Seq2Seq는  \n",
    "디코더가 매 시점마다 **인코더의 전체 단어를 다시 참고**할 수 있게 됩니다.  \n",
    "\n",
    "###  왜 필요한가  \n",
    "고정된 요약 벡터 하나로는 정보 손실이 크지만,  \n",
    "Attention을 통해 “필요할 때마다 입력 전체를 다시 본다”고 하면  \n",
    "긴 문장에서도 안정적인 번역이 가능합니다.\n",
    "\n",
    "###  어떻게 동작하는가  \n",
    "1. 인코더는 각 단어에 대한 Hidden state를 생성.  \n",
    "2. 디코더는 매 단어 생성 시, 인코더의 Hidden state 전체와 Attention을 수행.  \n",
    "3. 그 결과 얻은 Context 벡터와 함께 다음 단어를 예측.  \n",
    "\n",
    "###  흐름 다이어그램\n",
    "![{41C2ABC1-C7B0-48D4-AB4F-F875F4E4EC09}.png](attachment:{41C2ABC1-C7B0-48D4-AB4F-F875F4E4EC09}.png)\n",
    "\n",
    "\n",
    "###  비유  \n",
    "통역사가 한 번 듣고 끝내는 게 아니라,  \n",
    "매 단어를 말할 때마다 다시 녹음한 내용을 돌려 들으면서 필요한 부분만 집중하는 것입니다.  \n",
    "\n",
    "###  핵심 요약  \n",
    "Attention이 추가된 Seq2Seq는 “디코더가 인코더 전체를 반복적으로 참고할 수 있는 구조”다.  \n",
    "\n",
    "---\n",
    "\n",
    "##  Self-Attention으로 확장된 Transformer와의 연결\n",
    "\n",
    "###  개념 설명  \n",
    "Transformer는 Attention을 더 확장해서  \n",
    "**자기 자신(Self-Attention)**에게도 집중할 수 있게 만든 모델입니다.  \n",
    "즉, 입력 문장 안의 단어들끼리 서로 관련성을 계산합니다.  \n",
    "\n",
    "###  왜 필요한가  \n",
    "“나는 오늘 아침에 사과를 먹었다”라는 문장에서  \n",
    "‘먹었다’는 ‘사과’와 관련이 깊지만,  \n",
    "RNN은 순서대로만 정보를 전달하므로 이 관계를 멀리서 보기 어렵습니다.  \n",
    "Self-Attention은 문장 전체 단어들이 한 번에 서로를 바라볼 수 있게 합니다.  \n",
    "\n",
    "###  어떻게 동작하는가  \n",
    "문장 내의 각 단어가 Query, Key, Value를 모두 가지고,  \n",
    "서로의 관련도를 계산해 새로운 표현을 만듭니다.  \n",
    "\n",
    "###  비유  \n",
    "회의 중 모든 참가자가 서로의 의견을 들으면서  \n",
    "“누가 나와 가장 관련 있는 이야기 하는지” 판단해 자신의 생각을 조정하는 구조입니다.  \n",
    "\n",
    "###  핵심 요약  \n",
    "Self-Attention은 “문장 내부 단어들이 서로에게 주목할 수 있는 확장형 Attention”이다.  \n",
    "\n",
    "---\n",
    "\n",
    "##  실생활 응용과 요약\n",
    "\n",
    "###  개념 설명  \n",
    "Attention은 단순히 번역뿐 아니라  \n",
    "요약, 질의응답, 감정 분석, 이미지 캡셔닝 등  \n",
    "“입력의 특정 부분에 집중해야 하는 모든 문제”에서 쓰입니다.  \n",
    "\n",
    "###  왜 필요한가  \n",
    "인간의 집중 메커니즘을 모방해,  \n",
    "모델이 **필요한 정보만 추출**하도록 돕기 때문입니다.  \n",
    "\n",
    "###  비유  \n",
    "뉴스 기사를 요약할 때,  \n",
    "모든 문장을 다 읽지 않고 **핵심 문장에 집중**하는 것과 같습니다.  \n",
    "\n",
    "###  핵심 요약  \n",
    "Attention은 “AI에게 집중력을 부여하는 기술”이며,  \n",
    "그 결과 Transformer와 같은 현대 NLP 모델의 핵심 기반이 되었다.  \n",
    "\n",
    "---\n",
    "\n",
    "## 전체 핵심 요약\n",
    "\n",
    "- Seq2Seq는 하나의 벡터로 모든 정보를 담아 손실이 생김  \n",
    "- Attention은 “필요할 때, 필요한 부분만 집중”하는 기술  \n",
    "- Query–Key–Value 구조로 관련도를 계산  \n",
    "- Softmax로 중요도를 정규화해 문맥 벡터 생성  \n",
    "- Self-Attention은 문장 내부 단어끼리 상호 참조  \n",
    "- Transformer는 이 구조를 기반으로 완성된 모델  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f086cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 실습1-1\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Concatenate, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "english_sentences = [\n",
    "    'i love you',\n",
    "    'he is a student',\n",
    "    'she likes music',\n",
    "    'we are learning attention',\n",
    "    'you are amazing'\n",
    "]\n",
    "\n",
    "french_sentences = [\n",
    "    '<start> je t aime <end>',\n",
    "    '<start> il est etudiant <end>',\n",
    "    '<start> elle aime la musique <end>',\n",
    "    '<start> nous apprenons l attention <end>',\n",
    "    '<start> tu es incroyable <end>'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf183729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 어휘 크기 : 17\n",
      "프랑스어 어휘 크기 : 20\n",
      "영어 최대 길이 : 4\n",
      "프랑스어 최대 길이 : 6\n",
      "영어 패딩: [0 4 5 2]\n",
      "프랑스어 패딩 : [0 2 5 6 4 3]\n"
     ]
    }
   ],
   "source": [
    "######## 실습1-2\n",
    "# Attention\n",
    "    # 데이터\n",
    "        #토큰화 : 문장을 단어 단위로 분리\n",
    "        #특수토큰 : 디코더의 시작/종료 신호\n",
    "        #패딩 : 길이 맞춤\n",
    "\n",
    "\n",
    "# 토크나이저 생성\n",
    "    # I'm fine. I'm fine? --> 문장의 느낌해석이 달라지므로.\n",
    "    # 번역모델 (seq2seq, attention 포함)은 문장부호도 중요한 의미이기 때문에 제거하면 안됨.\n",
    "    # filters='' ---> 제거할 문자가 없다. (fitlers='?!.,' 이렇게하면 제거하게됨)\n",
    "eng_tokenizer = Tokenizer(filters='', oov_token='<OOV>')   \n",
    "fra_tokenizer = Tokenizer(filters='', oov_token='<OOV>')   \n",
    "\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "fra_tokenizer.fit_on_texts(french_sentences)\n",
    "# fra_tokenizer.index_word  # 단어에 인덱스 넣은것을 확인\n",
    "\n",
    "# 시퀀스 변화\n",
    "eng_sequence = eng_tokenizer.texts_to_sequences(english_sentences)\n",
    "fra_sequence = fra_tokenizer.texts_to_sequences(french_sentences)\n",
    "\n",
    "# 패딩\n",
    "max_eng_len = max([ len(seq) for seq in eng_sequence])\n",
    "max_fra_len = max([ len(seq) for seq in fra_sequence])\n",
    "eng_padded = pad_sequences(eng_sequence, maxlen=max_eng_len)\n",
    "fra_padded = pad_sequences(fra_sequence, maxlen=max_fra_len)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1  # keras의 tokenizer 인덱스를 1부터 부여\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1 \n",
    "\n",
    "print(f'영어 어휘 크기 : {eng_vocab_size}')\n",
    "print(f'프랑스어 어휘 크기 : {fra_vocab_size}')\n",
    "print(f'영어 최대 길이 : {max_eng_len}')\n",
    "print(f'프랑스어 최대 길이 : {max_fra_len}')\n",
    "print(f'영어 패딩: {eng_padded[0]}')\n",
    "print(f'프랑스어 패딩 : {fra_padded[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0ba6646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Encoder 구성 완료\n",
      "   - Encoder Outputs Shape: (batch, 4, 128)\n",
      "   - Hidden State Shape: (batch, 128)\n",
      "   - Cell State Shape: (batch, 128)\n",
      "✅ Decoder 구성 완료\n",
      "   - Decoder Outputs Shape: (batch, 6, 128)\n"
     ]
    }
   ],
   "source": [
    "######## 실습1-3\n",
    "## Seq2Seq 기본 구조 (Encoder–Decoder–LSTM 기반)\n",
    "# 인코더는 문맥(Context)을 요약해 전달하고,\n",
    "# 디코더는 그 문맥과 이전 단어를 바탕으로 다음 단어를 생성한다.\n",
    "    # Encoder : 입력 문장을 순차적으로 처리하며 의미를 요약 (Context 생성)\n",
    "    #           마지막 hidden state와 cell state를 디코더의 초기 상태로 전달\n",
    "    #\n",
    "    # Decoder : 인코더로부터 전달받은 초기 상태(context)와\n",
    "    #           이전 시점의 출력을 입력으로 받아 다음 단어를 예측\n",
    "    #\n",
    "    # return_sequences = True : 모든 타임스텝의 hidden state를 반환 (Attention 계산용)\n",
    "    #\n",
    "    # encoder_outputs : 인코더의 각 타임스텝 hidden state들 (Attention의 Key와 Value로 사용)\n",
    "    # encoder_states  : 인코더의 마지막 hidden state와 cell state (디코더의 초기 상태로 사용)\n",
    "\n",
    "\n",
    "\n",
    "# Funtional API 방식 \n",
    "# 하이퍼파라미터\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "\n",
    "# ========== ENCODER ==========\n",
    "encoder_inputs = Input(shape=(max_eng_len,), name='encoder_input')\n",
    "encoder_embedding = Embedding(eng_vocab_size, embedding_dim, name='encoder_embedding')(encoder_inputs)\n",
    "\n",
    "# return_sequences=True: 모든 타임스텝의 hidden state 반환 (Attention 계산에 필요)\n",
    "# return_state=True: 마지막 hidden state와 cell state 반환 (Decoder 초기화에 사용)\n",
    "encoder_lstm = LSTM(units, return_sequences=True, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# encoder_states = [state_h, state_c]는 Decoder의 초기 상태로 전달\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "print(f\"✅ Encoder 구성 완료\")\n",
    "print(f\"   - Encoder Outputs Shape: (batch, {max_eng_len}, {units})\")\n",
    "print(f\"   - Hidden State Shape: (batch, {units})\")\n",
    "print(f\"   - Cell State Shape: (batch, {units})\")\n",
    "\n",
    "\n",
    "\n",
    "# ========== DECODER ==========\n",
    "max_decoder_len = max_fra_len - 1\n",
    "decoder_inputs = Input(shape=(max_decoder_len,), name='decoder_input')\n",
    "decoder_embedding = Embedding(fra_vocab_size, embedding_dim, name='decoder_embedding')(decoder_inputs)\n",
    "\n",
    "# return_sequences=True: 모든 타임스텝 출력 (각 시점마다 Attention 적용)\n",
    "# return_state=True: Inference 시 상태 전달용\n",
    "decoder_lstm = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "print(f\"✅ Decoder 구성 완료\")\n",
    "print(f\"   - Decoder Outputs Shape: (batch, {max_fra_len}, {units})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfa0545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 실습1-3 attention layer 구현\n",
    "from tensorflow.keras.layers import Layer\n",
    "# 디코더의 현재상태 (query)와 인코더의 모든 hidden state (keys/value)를 비교해서\n",
    "# 각 인코더 타임스텝에 대한 중요도 (가중치)를 계산하고, \n",
    "# 그 가중치로 인코더 출력을 가중합해서 context vector를 얻는다\n",
    "\n",
    "class CAttention(Layer):\n",
    "    def __init__ (self, units, **kwargs):\n",
    "        super(CAttention,self).__init__(**kwargs)\n",
    "        self.units = units # 어텐션 내부에서 사용하는 차원\n",
    "    def build(self, input_shape):\n",
    "        # W1: Query 변환 가중치 (decoder hidden state → attention space)\n",
    "        self.W1 = self.add_weight(name='W1',\n",
    "                                   shape=(input_shape[0][-1], self.units), # input_shape[0] Query.shape / input_shape[1] keys,value / input_shape[1]??? \n",
    "                                   initializer='glorot_uniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        # W2: Key 변환 가중치 (encoder hidden states → attention space)\n",
    "        self.W2 = self.add_weight(name='W2',\n",
    "                                   shape=(input_shape[1][-1], self.units),\n",
    "                                   initializer='glorot_uniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        # V: Score를 스칼라로 변환\n",
    "        self.V = self.add_weight(name='V',\n",
    "                                  shape=(self.units, 1),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(CAttention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Decoder hidden state (batch, decoder_units)\n",
    "            values: Encoder hidden states (batch, max_eng_len, encoder_units)\n",
    "        \n",
    "        Returns:\n",
    "            context_vector: (batch, encoder_units)\n",
    "            attention_weights: (batch, max_eng_len, 1)\n",
    "        \"\"\"\n",
    "        query, values = inputs\n",
    "        \n",
    "        # Query 차원 확장: (batch, decoder_units) → (batch, 1, decoder_units)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # Score 계산: tanh(W1*Q + W2*K)\n",
    "        # W1*Q: (batch, 1, units)\n",
    "        # W2*K: (batch, max_eng_len, units)\n",
    "        score = tf.nn.tanh(\n",
    "            tf.matmul(query_with_time_axis, self.W1) + tf.matmul(values, self.W2)\n",
    "        )\n",
    "        # score shape: (batch, max_eng_len, units)\n",
    "        \n",
    "        # V를 곱해서 스칼라 score로 변환\n",
    "        attention_logits = tf.matmul(score, self.V)\n",
    "        # shape: (batch, max_eng_len, 1)\n",
    "        \n",
    "        # Softmax로 확률 분포 변환 (합이 1)\n",
    "        attention_weights = tf.nn.softmax(attention_logits, axis=1)\n",
    "        \n",
    "        # Context vector 계산: 가중 합\n",
    "        # attention_weights: (batch, max_eng_len, 1)\n",
    "        # values: (batch, max_eng_len, encoder_units)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        # shape: (batch, encoder_units)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cdef574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CAttention name=attention, built=False>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## 실습1-4 attention layer 완성\n",
    "# Attention Layer\n",
    "attention_layer = CAttention(units=10, name='attention')\n",
    "attention_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e95ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Context Vector Shape: (batch, 6, 128)\n",
      "   - Combined Shape: (batch, 6, 256)\n",
      "   - Output Shape: (batch, 6, 20)\n"
     ]
    }
   ],
   "source": [
    "######## 실습1-5 attention 통합모델\n",
    "# Attention 통합모델\n",
    "    # decoder에 Attention을 적용해서 Context Vector와 결합\n",
    "    # 각 디코더 타임스텝마다 Attention 계산\n",
    "    # TimeDistributed: 모든 타임스텝에 동일한 Dense Layer 적용\n",
    "\n",
    "    # Lambda 레이어로 각 타입스템의 hidden state 추출 후 Attention 적용\n",
    "\n",
    "\n",
    "# Attention을 각 디코더 타임스텝에 적용\n",
    "def apply_attention(inputs):\n",
    "    \"\"\"\n",
    "    각 타임스텝마다 Attention 계산\n",
    "    \"\"\"\n",
    "    encoder_outputs, decoder_outputs = inputs\n",
    "    \n",
    "    # 타임스텝별로 순회하며 Context Vector 생성\n",
    "    context_vectors = []\n",
    "    attention_weights_list = []\n",
    "    \n",
    "    for t in range(max_decoder_len):\n",
    "        # t 시점의 decoder hidden state 추출\n",
    "        decoder_hidden_t = decoder_outputs[:, t, :]\n",
    "        \n",
    "        # Attention 계산\n",
    "        context_vector, attention_weights = attention_layer([decoder_hidden_t, encoder_outputs])\n",
    "        context_vectors.append(context_vector)\n",
    "        attention_weights_list.append(attention_weights)\n",
    "    \n",
    "    # (batch, max_fra_len, encoder_units)로 재구성\n",
    "    context_vectors = tf.stack(context_vectors, axis=1)\n",
    "    attention_weights_all = tf.stack(attention_weights_list, axis=1)\n",
    "    \n",
    "    return context_vectors, attention_weights_all\n",
    "\n",
    "# Lambda Layer로 래핑\n",
    "attention_result = Lambda(apply_attention, name='apply_attention')([encoder_outputs, decoder_outputs])\n",
    "context_vectors, attention_weights_all = attention_result[0], attention_result[1]  # Lambda에서 context_vectors만 반환\n",
    "\n",
    "# Decoder Output + Context Vector 결합\n",
    "decoder_combined = Concatenate(axis=-1, name='concat')([decoder_outputs, context_vectors])\n",
    "\n",
    "# 최종 출력 레이어 (단어 확률 분포)\n",
    "output_layer = Dense(fra_vocab_size, activation='softmax', name='output')\n",
    "outputs = output_layer(decoder_combined)\n",
    "\n",
    "\n",
    "print(f\"   - Context Vector Shape: (batch, {max_fra_len}, {units})\")\n",
    "print(f\"   - Combined Shape: (batch, {max_fra_len}, {units * 2})\")\n",
    "print(f\"   - Output Shape: (batch, {max_fra_len}, {fra_vocab_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cbb4842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"attention_seq2seq\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"attention_seq2seq\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ encoder_embeddin… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ decoder_embeddin… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ apply_attention     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)]  │            │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ apply_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,140</span> │ concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │      \u001b[38;5;34m1,088\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │      \u001b[38;5;34m1,280\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m),  │     \u001b[38;5;34m98,816\u001b[0m │ encoder_embeddin… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m),  │     \u001b[38;5;34m98,816\u001b[0m │ decoder_embeddin… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ apply_attention     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m),  │          \u001b[38;5;34m0\u001b[0m │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1\u001b[0m)]  │            │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ apply_attention[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m20\u001b[0m)     │      \u001b[38;5;34m5,140\u001b[0m │ concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,140</span> (801.33 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m205,140\u001b[0m (801.33 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,140</span> (801.33 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m205,140\u001b[0m (801.33 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######## 실습1-6 모델 생성\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs, name='attention_seq2seq')\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['acc']\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1e6671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input : (5, 4)\n",
      "decoder input : (5, 5)\n",
      "decoder target : (5, 5)\n",
      "Epoch 1/1000\n",
      "Epoch 2/1000\n",
      "Epoch 3/1000\n",
      "Epoch 4/1000\n",
      "Epoch 5/1000\n",
      "Epoch 6/1000\n",
      "Epoch 7/1000\n",
      "Epoch 8/1000\n",
      "Epoch 9/1000\n",
      "Epoch 10/1000\n",
      "Epoch 11/1000\n",
      "Epoch 12/1000\n",
      "Epoch 13/1000\n",
      "Epoch 14/1000\n",
      "Epoch 15/1000\n",
      "Epoch 16/1000\n",
      "Epoch 17/1000\n",
      "Epoch 18/1000\n",
      "Epoch 19/1000\n",
      "Epoch 20/1000\n",
      "Epoch 21/1000\n",
      "Epoch 22/1000\n",
      "Epoch 23/1000\n",
      "Epoch 24/1000\n",
      "Epoch 25/1000\n",
      "Epoch 26/1000\n",
      "Epoch 27/1000\n",
      "Epoch 28/1000\n",
      "Epoch 29/1000\n",
      "Epoch 30/1000\n",
      "Epoch 31/1000\n",
      "Epoch 32/1000\n",
      "Epoch 33/1000\n",
      "Epoch 34/1000\n",
      "Epoch 35/1000\n",
      "Epoch 36/1000\n",
      "Epoch 37/1000\n",
      "Epoch 38/1000\n",
      "Epoch 39/1000\n",
      "Epoch 40/1000\n",
      "Epoch 41/1000\n",
      "Epoch 42/1000\n",
      "Epoch 43/1000\n",
      "Epoch 44/1000\n",
      "Epoch 45/1000\n",
      "Epoch 46/1000\n",
      "Epoch 47/1000\n",
      "Epoch 48/1000\n",
      "Epoch 49/1000\n",
      "Epoch 50/1000\n",
      "Epoch 51/1000\n",
      "Epoch 52/1000\n",
      "Epoch 53/1000\n",
      "Epoch 54/1000\n",
      "Epoch 55/1000\n",
      "Epoch 56/1000\n",
      "Epoch 57/1000\n",
      "Epoch 58/1000\n",
      "Epoch 59/1000\n",
      "Epoch 60/1000\n",
      "Epoch 61/1000\n",
      "Epoch 62/1000\n",
      "Epoch 63/1000\n",
      "Epoch 64/1000\n",
      "Epoch 65/1000\n",
      "Epoch 66/1000\n",
      "Epoch 67/1000\n",
      "Epoch 68/1000\n",
      "Epoch 69/1000\n",
      "Epoch 70/1000\n",
      "Epoch 71/1000\n",
      "Epoch 72/1000\n",
      "Epoch 73/1000\n",
      "Epoch 74/1000\n",
      "Epoch 75/1000\n",
      "Epoch 76/1000\n",
      "Epoch 77/1000\n",
      "Epoch 78/1000\n",
      "Epoch 79/1000\n",
      "Epoch 80/1000\n",
      "Epoch 81/1000\n",
      "Epoch 82/1000\n",
      "Epoch 83/1000\n",
      "Epoch 84/1000\n",
      "Epoch 85/1000\n",
      "Epoch 86/1000\n",
      "Epoch 87/1000\n",
      "Epoch 88/1000\n",
      "Epoch 89/1000\n",
      "Epoch 90/1000\n",
      "Epoch 91/1000\n",
      "Epoch 92/1000\n",
      "Epoch 93/1000\n",
      "Epoch 94/1000\n",
      "Epoch 95/1000\n",
      "Epoch 96/1000\n",
      "Epoch 97/1000\n",
      "Epoch 98/1000\n",
      "Epoch 99/1000\n",
      "Epoch 100/1000\n",
      "Epoch 101/1000\n",
      "Epoch 102/1000\n",
      "Epoch 103/1000\n",
      "Epoch 104/1000\n",
      "Epoch 105/1000\n",
      "Epoch 106/1000\n",
      "Epoch 107/1000\n",
      "Epoch 108/1000\n",
      "Epoch 109/1000\n",
      "Epoch 110/1000\n",
      "Epoch 111/1000\n",
      "Epoch 112/1000\n",
      "Epoch 113/1000\n",
      "Epoch 114/1000\n",
      "Epoch 115/1000\n",
      "Epoch 116/1000\n",
      "Epoch 117/1000\n",
      "Epoch 118/1000\n",
      "Epoch 119/1000\n",
      "Epoch 120/1000\n",
      "Epoch 121/1000\n",
      "Epoch 122/1000\n",
      "Epoch 123/1000\n",
      "Epoch 124/1000\n",
      "Epoch 125/1000\n",
      "Epoch 126/1000\n",
      "Epoch 127/1000\n",
      "Epoch 128/1000\n",
      "Epoch 129/1000\n",
      "Epoch 130/1000\n",
      "Epoch 131/1000\n",
      "Epoch 132/1000\n",
      "Epoch 133/1000\n",
      "Epoch 134/1000\n",
      "Epoch 135/1000\n",
      "Epoch 136/1000\n",
      "Epoch 137/1000\n",
      "Epoch 138/1000\n",
      "Epoch 139/1000\n",
      "Epoch 140/1000\n",
      "Epoch 141/1000\n",
      "Epoch 142/1000\n",
      "Epoch 143/1000\n",
      "Epoch 144/1000\n",
      "Epoch 145/1000\n",
      "Epoch 146/1000\n",
      "Epoch 147/1000\n",
      "Epoch 148/1000\n",
      "Epoch 149/1000\n",
      "Epoch 150/1000\n",
      "Epoch 151/1000\n",
      "Epoch 152/1000\n",
      "Epoch 153/1000\n",
      "Epoch 154/1000\n",
      "Epoch 155/1000\n",
      "Epoch 156/1000\n",
      "Epoch 157/1000\n",
      "Epoch 158/1000\n",
      "Epoch 159/1000\n",
      "Epoch 160/1000\n",
      "Epoch 161/1000\n",
      "Epoch 162/1000\n",
      "Epoch 163/1000\n",
      "Epoch 164/1000\n",
      "Epoch 165/1000\n",
      "Epoch 166/1000\n",
      "Epoch 167/1000\n",
      "Epoch 168/1000\n",
      "Epoch 169/1000\n",
      "Epoch 170/1000\n",
      "Epoch 171/1000\n",
      "Epoch 172/1000\n",
      "Epoch 173/1000\n",
      "Epoch 174/1000\n",
      "Epoch 175/1000\n",
      "Epoch 176/1000\n",
      "Epoch 177/1000\n",
      "Epoch 178/1000\n",
      "Epoch 179/1000\n",
      "Epoch 180/1000\n",
      "Epoch 181/1000\n",
      "Epoch 182/1000\n",
      "Epoch 183/1000\n",
      "Epoch 184/1000\n",
      "Epoch 185/1000\n",
      "Epoch 186/1000\n",
      "Epoch 187/1000\n",
      "Epoch 188/1000\n",
      "Epoch 189/1000\n",
      "Epoch 190/1000\n",
      "Epoch 191/1000\n",
      "Epoch 192/1000\n",
      "Epoch 193/1000\n",
      "Epoch 194/1000\n",
      "Epoch 195/1000\n",
      "Epoch 196/1000\n",
      "Epoch 197/1000\n",
      "Epoch 198/1000\n",
      "Epoch 199/1000\n",
      "Epoch 200/1000\n",
      "Epoch 201/1000\n",
      "Epoch 202/1000\n",
      "Epoch 203/1000\n",
      "Epoch 204/1000\n",
      "Epoch 205/1000\n",
      "Epoch 206/1000\n",
      "Epoch 207/1000\n",
      "Epoch 208/1000\n",
      "Epoch 209/1000\n",
      "Epoch 210/1000\n",
      "Epoch 211/1000\n",
      "Epoch 212/1000\n",
      "Epoch 213/1000\n",
      "Epoch 214/1000\n",
      "Epoch 215/1000\n",
      "Epoch 216/1000\n",
      "Epoch 217/1000\n",
      "Epoch 218/1000\n",
      "Epoch 219/1000\n",
      "Epoch 220/1000\n",
      "Epoch 221/1000\n",
      "Epoch 222/1000\n",
      "Epoch 223/1000\n",
      "Epoch 224/1000\n",
      "Epoch 225/1000\n",
      "Epoch 226/1000\n",
      "Epoch 227/1000\n",
      "Epoch 228/1000\n",
      "Epoch 229/1000\n",
      "Epoch 230/1000\n",
      "Epoch 231/1000\n",
      "Epoch 232/1000\n",
      "Epoch 233/1000\n",
      "Epoch 234/1000\n",
      "Epoch 235/1000\n",
      "Epoch 236/1000\n",
      "Epoch 237/1000\n",
      "Epoch 238/1000\n",
      "Epoch 239/1000\n",
      "Epoch 240/1000\n",
      "Epoch 241/1000\n",
      "Epoch 242/1000\n",
      "Epoch 243/1000\n",
      "Epoch 244/1000\n",
      "Epoch 245/1000\n",
      "Epoch 246/1000\n",
      "Epoch 247/1000\n",
      "Epoch 248/1000\n",
      "Epoch 249/1000\n",
      "Epoch 250/1000\n",
      "Epoch 251/1000\n",
      "Epoch 252/1000\n",
      "Epoch 253/1000\n",
      "Epoch 254/1000\n",
      "Epoch 255/1000\n",
      "Epoch 256/1000\n",
      "Epoch 257/1000\n",
      "Epoch 258/1000\n",
      "Epoch 259/1000\n",
      "Epoch 260/1000\n",
      "Epoch 261/1000\n",
      "Epoch 262/1000\n",
      "Epoch 263/1000\n",
      "Epoch 264/1000\n",
      "Epoch 265/1000\n",
      "Epoch 266/1000\n",
      "Epoch 267/1000\n",
      "Epoch 268/1000\n",
      "Epoch 269/1000\n",
      "Epoch 270/1000\n",
      "Epoch 271/1000\n",
      "Epoch 272/1000\n",
      "Epoch 273/1000\n",
      "Epoch 274/1000\n",
      "Epoch 275/1000\n",
      "Epoch 276/1000\n",
      "Epoch 277/1000\n",
      "Epoch 278/1000\n",
      "Epoch 279/1000\n",
      "Epoch 280/1000\n",
      "Epoch 281/1000\n",
      "Epoch 282/1000\n",
      "Epoch 283/1000\n",
      "Epoch 284/1000\n",
      "Epoch 285/1000\n",
      "Epoch 286/1000\n",
      "Epoch 287/1000\n",
      "Epoch 288/1000\n",
      "Epoch 289/1000\n",
      "Epoch 290/1000\n",
      "Epoch 291/1000\n",
      "Epoch 292/1000\n",
      "Epoch 293/1000\n",
      "Epoch 294/1000\n",
      "Epoch 295/1000\n",
      "Epoch 296/1000\n",
      "Epoch 297/1000\n",
      "Epoch 298/1000\n",
      "Epoch 299/1000\n",
      "Epoch 300/1000\n",
      "Epoch 301/1000\n",
      "Epoch 302/1000\n",
      "Epoch 303/1000\n",
      "Epoch 304/1000\n",
      "Epoch 305/1000\n",
      "Epoch 306/1000\n",
      "Epoch 307/1000\n",
      "Epoch 308/1000\n",
      "Epoch 309/1000\n",
      "Epoch 310/1000\n",
      "Epoch 311/1000\n",
      "Epoch 312/1000\n",
      "Epoch 313/1000\n",
      "Epoch 314/1000\n",
      "Epoch 315/1000\n",
      "Epoch 316/1000\n",
      "Epoch 317/1000\n",
      "Epoch 318/1000\n",
      "Epoch 319/1000\n",
      "Epoch 320/1000\n",
      "Epoch 321/1000\n",
      "Epoch 322/1000\n",
      "Epoch 323/1000\n",
      "Epoch 324/1000\n",
      "Epoch 325/1000\n",
      "Epoch 326/1000\n",
      "Epoch 327/1000\n",
      "Epoch 328/1000\n",
      "Epoch 329/1000\n",
      "Epoch 330/1000\n",
      "Epoch 331/1000\n",
      "Epoch 332/1000\n",
      "Epoch 333/1000\n",
      "Epoch 334/1000\n",
      "Epoch 335/1000\n",
      "Epoch 336/1000\n",
      "Epoch 337/1000\n",
      "Epoch 338/1000\n",
      "Epoch 339/1000\n",
      "Epoch 340/1000\n",
      "Epoch 341/1000\n",
      "Epoch 342/1000\n",
      "Epoch 343/1000\n",
      "Epoch 344/1000\n",
      "Epoch 345/1000\n",
      "Epoch 346/1000\n",
      "Epoch 347/1000\n",
      "Epoch 348/1000\n",
      "Epoch 349/1000\n",
      "Epoch 350/1000\n",
      "Epoch 351/1000\n",
      "Epoch 352/1000\n",
      "Epoch 353/1000\n",
      "Epoch 354/1000\n",
      "Epoch 355/1000\n",
      "Epoch 356/1000\n",
      "Epoch 357/1000\n",
      "Epoch 358/1000\n",
      "Epoch 359/1000\n",
      "Epoch 360/1000\n",
      "Epoch 361/1000\n",
      "Epoch 362/1000\n",
      "Epoch 363/1000\n",
      "Epoch 364/1000\n",
      "Epoch 365/1000\n",
      "Epoch 366/1000\n",
      "Epoch 367/1000\n",
      "Epoch 368/1000\n",
      "Epoch 369/1000\n",
      "Epoch 370/1000\n",
      "Epoch 371/1000\n",
      "Epoch 372/1000\n",
      "Epoch 373/1000\n",
      "Epoch 374/1000\n",
      "Epoch 375/1000\n",
      "Epoch 376/1000\n",
      "Epoch 377/1000\n",
      "Epoch 378/1000\n",
      "Epoch 379/1000\n",
      "Epoch 380/1000\n",
      "Epoch 381/1000\n",
      "Epoch 382/1000\n",
      "Epoch 383/1000\n",
      "Epoch 384/1000\n",
      "Epoch 385/1000\n",
      "Epoch 386/1000\n",
      "Epoch 387/1000\n",
      "Epoch 388/1000\n",
      "Epoch 389/1000\n",
      "Epoch 390/1000\n",
      "Epoch 391/1000\n",
      "Epoch 392/1000\n",
      "Epoch 393/1000\n",
      "Epoch 394/1000\n",
      "Epoch 395/1000\n",
      "Epoch 396/1000\n",
      "Epoch 397/1000\n",
      "Epoch 398/1000\n",
      "Epoch 399/1000\n",
      "Epoch 400/1000\n",
      "Epoch 401/1000\n",
      "Epoch 402/1000\n",
      "Epoch 403/1000\n",
      "Epoch 404/1000\n",
      "Epoch 405/1000\n",
      "Epoch 406/1000\n",
      "Epoch 407/1000\n",
      "Epoch 408/1000\n",
      "Epoch 409/1000\n",
      "Epoch 410/1000\n",
      "Epoch 411/1000\n",
      "Epoch 412/1000\n",
      "Epoch 413/1000\n",
      "Epoch 414/1000\n",
      "Epoch 415/1000\n",
      "Epoch 416/1000\n",
      "Epoch 417/1000\n",
      "Epoch 418/1000\n",
      "Epoch 419/1000\n",
      "Epoch 420/1000\n",
      "Epoch 421/1000\n",
      "Epoch 422/1000\n",
      "Epoch 423/1000\n",
      "Epoch 424/1000\n",
      "Epoch 425/1000\n",
      "Epoch 426/1000\n",
      "Epoch 427/1000\n",
      "Epoch 428/1000\n",
      "Epoch 429/1000\n",
      "Epoch 430/1000\n",
      "Epoch 431/1000\n",
      "Epoch 432/1000\n",
      "Epoch 433/1000\n",
      "Epoch 434/1000\n",
      "Epoch 435/1000\n",
      "Epoch 436/1000\n",
      "Epoch 437/1000\n",
      "Epoch 438/1000\n",
      "Epoch 439/1000\n",
      "Epoch 440/1000\n",
      "Epoch 441/1000\n",
      "Epoch 442/1000\n",
      "Epoch 443/1000\n",
      "Epoch 444/1000\n",
      "Epoch 445/1000\n",
      "Epoch 446/1000\n",
      "Epoch 447/1000\n",
      "Epoch 448/1000\n",
      "Epoch 449/1000\n",
      "Epoch 450/1000\n",
      "Epoch 451/1000\n",
      "Epoch 452/1000\n",
      "Epoch 453/1000\n",
      "Epoch 454/1000\n",
      "Epoch 455/1000\n",
      "Epoch 456/1000\n",
      "Epoch 457/1000\n",
      "Epoch 458/1000\n",
      "Epoch 459/1000\n",
      "Epoch 460/1000\n",
      "Epoch 461/1000\n",
      "Epoch 462/1000\n",
      "Epoch 463/1000\n",
      "Epoch 464/1000\n",
      "Epoch 465/1000\n",
      "Epoch 466/1000\n",
      "Epoch 467/1000\n",
      "Epoch 468/1000\n",
      "Epoch 469/1000\n",
      "Epoch 470/1000\n",
      "Epoch 471/1000\n",
      "Epoch 472/1000\n",
      "Epoch 473/1000\n",
      "Epoch 474/1000\n",
      "Epoch 475/1000\n",
      "Epoch 476/1000\n",
      "Epoch 477/1000\n",
      "Epoch 478/1000\n",
      "Epoch 479/1000\n",
      "Epoch 480/1000\n",
      "Epoch 481/1000\n",
      "Epoch 482/1000\n",
      "Epoch 483/1000\n",
      "Epoch 484/1000\n",
      "Epoch 485/1000\n",
      "Epoch 486/1000\n",
      "Epoch 487/1000\n",
      "Epoch 488/1000\n",
      "Epoch 489/1000\n",
      "Epoch 490/1000\n",
      "Epoch 491/1000\n",
      "Epoch 492/1000\n",
      "Epoch 493/1000\n",
      "Epoch 494/1000\n",
      "Epoch 495/1000\n",
      "Epoch 496/1000\n",
      "Epoch 497/1000\n",
      "Epoch 498/1000\n",
      "Epoch 499/1000\n",
      "Epoch 500/1000\n",
      "Epoch 501/1000\n",
      "Epoch 502/1000\n",
      "Epoch 503/1000\n",
      "Epoch 504/1000\n",
      "Epoch 505/1000\n",
      "Epoch 506/1000\n",
      "Epoch 507/1000\n",
      "Epoch 508/1000\n",
      "Epoch 509/1000\n",
      "Epoch 510/1000\n",
      "Epoch 511/1000\n",
      "Epoch 512/1000\n",
      "Epoch 513/1000\n",
      "Epoch 514/1000\n",
      "Epoch 515/1000\n",
      "Epoch 516/1000\n",
      "Epoch 517/1000\n",
      "Epoch 518/1000\n",
      "Epoch 519/1000\n",
      "Epoch 520/1000\n",
      "Epoch 521/1000\n",
      "Epoch 522/1000\n",
      "Epoch 523/1000\n",
      "Epoch 524/1000\n",
      "Epoch 525/1000\n",
      "Epoch 526/1000\n",
      "Epoch 527/1000\n",
      "Epoch 528/1000\n",
      "Epoch 529/1000\n",
      "Epoch 530/1000\n",
      "Epoch 531/1000\n",
      "Epoch 532/1000\n",
      "Epoch 533/1000\n",
      "Epoch 534/1000\n",
      "Epoch 535/1000\n",
      "Epoch 536/1000\n",
      "Epoch 537/1000\n",
      "Epoch 538/1000\n",
      "Epoch 539/1000\n",
      "Epoch 540/1000\n",
      "Epoch 541/1000\n",
      "Epoch 542/1000\n",
      "Epoch 543/1000\n",
      "Epoch 544/1000\n",
      "Epoch 545/1000\n",
      "Epoch 546/1000\n",
      "Epoch 547/1000\n",
      "Epoch 548/1000\n",
      "Epoch 549/1000\n",
      "Epoch 550/1000\n",
      "Epoch 551/1000\n",
      "Epoch 552/1000\n",
      "Epoch 553/1000\n",
      "Epoch 554/1000\n",
      "Epoch 555/1000\n",
      "Epoch 556/1000\n",
      "Epoch 557/1000\n",
      "Epoch 558/1000\n",
      "Epoch 559/1000\n",
      "Epoch 560/1000\n",
      "Epoch 561/1000\n",
      "Epoch 562/1000\n",
      "Epoch 563/1000\n",
      "Epoch 564/1000\n",
      "Epoch 565/1000\n",
      "Epoch 566/1000\n",
      "Epoch 567/1000\n",
      "Epoch 568/1000\n",
      "Epoch 569/1000\n",
      "Epoch 570/1000\n",
      "Epoch 571/1000\n",
      "Epoch 572/1000\n",
      "Epoch 573/1000\n",
      "Epoch 574/1000\n",
      "Epoch 575/1000\n",
      "Epoch 576/1000\n",
      "Epoch 577/1000\n",
      "Epoch 578/1000\n",
      "Epoch 579/1000\n",
      "Epoch 580/1000\n",
      "Epoch 581/1000\n",
      "Epoch 582/1000\n",
      "Epoch 583/1000\n",
      "Epoch 584/1000\n",
      "Epoch 585/1000\n",
      "Epoch 586/1000\n",
      "Epoch 587/1000\n",
      "Epoch 588/1000\n",
      "Epoch 589/1000\n",
      "Epoch 590/1000\n",
      "Epoch 591/1000\n",
      "Epoch 592/1000\n",
      "Epoch 593/1000\n",
      "Epoch 594/1000\n",
      "Epoch 595/1000\n",
      "Epoch 596/1000\n",
      "Epoch 597/1000\n",
      "Epoch 598/1000\n",
      "Epoch 599/1000\n",
      "Epoch 600/1000\n",
      "Epoch 601/1000\n",
      "Epoch 602/1000\n",
      "Epoch 603/1000\n",
      "Epoch 604/1000\n",
      "Epoch 605/1000\n",
      "Epoch 606/1000\n",
      "Epoch 607/1000\n",
      "Epoch 608/1000\n",
      "Epoch 609/1000\n",
      "Epoch 610/1000\n",
      "Epoch 611/1000\n",
      "Epoch 612/1000\n",
      "Epoch 613/1000\n",
      "Epoch 614/1000\n",
      "Epoch 615/1000\n",
      "Epoch 616/1000\n",
      "Epoch 617/1000\n",
      "Epoch 618/1000\n",
      "Epoch 619/1000\n",
      "Epoch 620/1000\n",
      "Epoch 621/1000\n",
      "Epoch 622/1000\n",
      "Epoch 623/1000\n",
      "Epoch 624/1000\n",
      "Epoch 625/1000\n",
      "Epoch 626/1000\n",
      "Epoch 627/1000\n",
      "Epoch 628/1000\n",
      "Epoch 629/1000\n",
      "Epoch 630/1000\n",
      "Epoch 631/1000\n",
      "Epoch 632/1000\n",
      "Epoch 633/1000\n",
      "Epoch 634/1000\n",
      "Epoch 635/1000\n",
      "Epoch 636/1000\n",
      "Epoch 637/1000\n",
      "Epoch 638/1000\n",
      "Epoch 639/1000\n",
      "Epoch 640/1000\n",
      "Epoch 641/1000\n",
      "Epoch 642/1000\n",
      "Epoch 643/1000\n",
      "Epoch 644/1000\n",
      "Epoch 645/1000\n",
      "Epoch 646/1000\n",
      "Epoch 647/1000\n",
      "Epoch 648/1000\n",
      "Epoch 649/1000\n",
      "Epoch 650/1000\n",
      "Epoch 651/1000\n",
      "Epoch 652/1000\n",
      "Epoch 653/1000\n",
      "Epoch 654/1000\n",
      "Epoch 655/1000\n",
      "Epoch 656/1000\n",
      "Epoch 657/1000\n",
      "Epoch 658/1000\n",
      "Epoch 659/1000\n",
      "Epoch 660/1000\n",
      "Epoch 661/1000\n",
      "Epoch 662/1000\n",
      "Epoch 663/1000\n",
      "Epoch 664/1000\n",
      "Epoch 665/1000\n",
      "Epoch 666/1000\n",
      "Epoch 667/1000\n",
      "Epoch 668/1000\n",
      "Epoch 669/1000\n",
      "Epoch 670/1000\n",
      "Epoch 671/1000\n",
      "Epoch 672/1000\n",
      "Epoch 673/1000\n",
      "Epoch 674/1000\n",
      "Epoch 675/1000\n",
      "Epoch 676/1000\n",
      "Epoch 677/1000\n",
      "Epoch 678/1000\n",
      "Epoch 679/1000\n",
      "Epoch 680/1000\n",
      "Epoch 681/1000\n",
      "Epoch 682/1000\n",
      "Epoch 683/1000\n",
      "Epoch 684/1000\n",
      "Epoch 685/1000\n",
      "Epoch 686/1000\n",
      "Epoch 687/1000\n",
      "Epoch 688/1000\n",
      "Epoch 689/1000\n",
      "Epoch 690/1000\n",
      "Epoch 691/1000\n",
      "Epoch 692/1000\n",
      "Epoch 693/1000\n",
      "Epoch 694/1000\n",
      "Epoch 695/1000\n",
      "Epoch 696/1000\n",
      "Epoch 697/1000\n",
      "Epoch 698/1000\n",
      "Epoch 699/1000\n",
      "Epoch 700/1000\n",
      "Epoch 701/1000\n",
      "Epoch 702/1000\n",
      "Epoch 703/1000\n",
      "Epoch 704/1000\n",
      "Epoch 705/1000\n",
      "Epoch 706/1000\n",
      "Epoch 707/1000\n",
      "Epoch 708/1000\n",
      "Epoch 709/1000\n",
      "Epoch 710/1000\n",
      "Epoch 711/1000\n",
      "Epoch 712/1000\n",
      "Epoch 713/1000\n",
      "Epoch 714/1000\n",
      "Epoch 715/1000\n",
      "Epoch 716/1000\n",
      "Epoch 717/1000\n",
      "Epoch 718/1000\n",
      "Epoch 719/1000\n",
      "Epoch 720/1000\n",
      "Epoch 721/1000\n",
      "Epoch 722/1000\n",
      "Epoch 723/1000\n",
      "Epoch 724/1000\n",
      "Epoch 725/1000\n",
      "Epoch 726/1000\n",
      "Epoch 727/1000\n",
      "Epoch 728/1000\n",
      "Epoch 729/1000\n",
      "Epoch 730/1000\n",
      "Epoch 731/1000\n",
      "Epoch 732/1000\n",
      "Epoch 733/1000\n",
      "Epoch 734/1000\n",
      "Epoch 735/1000\n",
      "Epoch 736/1000\n",
      "Epoch 737/1000\n",
      "Epoch 738/1000\n",
      "Epoch 739/1000\n",
      "Epoch 740/1000\n",
      "Epoch 741/1000\n",
      "Epoch 742/1000\n",
      "Epoch 743/1000\n",
      "Epoch 744/1000\n",
      "Epoch 745/1000\n",
      "Epoch 746/1000\n",
      "Epoch 747/1000\n",
      "Epoch 748/1000\n",
      "Epoch 749/1000\n",
      "Epoch 750/1000\n",
      "Epoch 751/1000\n",
      "Epoch 752/1000\n",
      "Epoch 753/1000\n",
      "Epoch 754/1000\n",
      "Epoch 755/1000\n",
      "Epoch 756/1000\n",
      "Epoch 757/1000\n",
      "Epoch 758/1000\n",
      "Epoch 759/1000\n",
      "Epoch 760/1000\n",
      "Epoch 761/1000\n",
      "Epoch 762/1000\n",
      "Epoch 763/1000\n",
      "Epoch 764/1000\n",
      "Epoch 765/1000\n",
      "Epoch 766/1000\n",
      "Epoch 767/1000\n",
      "Epoch 768/1000\n",
      "Epoch 769/1000\n",
      "Epoch 770/1000\n",
      "Epoch 771/1000\n",
      "Epoch 772/1000\n",
      "Epoch 773/1000\n",
      "Epoch 774/1000\n",
      "Epoch 775/1000\n",
      "Epoch 776/1000\n",
      "Epoch 777/1000\n",
      "Epoch 778/1000\n",
      "Epoch 779/1000\n",
      "Epoch 780/1000\n",
      "Epoch 781/1000\n",
      "Epoch 782/1000\n",
      "Epoch 783/1000\n",
      "Epoch 784/1000\n",
      "Epoch 785/1000\n",
      "Epoch 786/1000\n",
      "Epoch 787/1000\n",
      "Epoch 788/1000\n",
      "Epoch 789/1000\n",
      "Epoch 790/1000\n",
      "Epoch 791/1000\n",
      "Epoch 792/1000\n",
      "Epoch 793/1000\n",
      "Epoch 794/1000\n",
      "Epoch 795/1000\n",
      "Epoch 796/1000\n",
      "Epoch 797/1000\n",
      "Epoch 798/1000\n",
      "Epoch 799/1000\n",
      "Epoch 800/1000\n",
      "Epoch 801/1000\n",
      "Epoch 802/1000\n",
      "Epoch 803/1000\n",
      "Epoch 804/1000\n",
      "Epoch 805/1000\n",
      "Epoch 806/1000\n",
      "Epoch 807/1000\n",
      "Epoch 808/1000\n",
      "Epoch 809/1000\n",
      "Epoch 810/1000\n",
      "Epoch 811/1000\n",
      "Epoch 812/1000\n",
      "Epoch 813/1000\n",
      "Epoch 814/1000\n",
      "Epoch 815/1000\n",
      "Epoch 816/1000\n",
      "Epoch 817/1000\n",
      "Epoch 818/1000\n",
      "Epoch 819/1000\n",
      "Epoch 820/1000\n",
      "Epoch 821/1000\n",
      "Epoch 822/1000\n",
      "Epoch 823/1000\n",
      "Epoch 824/1000\n",
      "Epoch 825/1000\n",
      "Epoch 826/1000\n",
      "Epoch 827/1000\n",
      "Epoch 828/1000\n",
      "Epoch 829/1000\n",
      "Epoch 830/1000\n",
      "Epoch 831/1000\n",
      "Epoch 832/1000\n",
      "Epoch 833/1000\n",
      "Epoch 834/1000\n",
      "Epoch 835/1000\n",
      "Epoch 836/1000\n",
      "Epoch 837/1000\n",
      "Epoch 838/1000\n",
      "Epoch 839/1000\n",
      "Epoch 840/1000\n",
      "Epoch 841/1000\n",
      "Epoch 842/1000\n",
      "Epoch 843/1000\n",
      "Epoch 844/1000\n",
      "Epoch 845/1000\n",
      "Epoch 846/1000\n",
      "Epoch 847/1000\n",
      "Epoch 848/1000\n",
      "Epoch 849/1000\n",
      "Epoch 850/1000\n",
      "Epoch 851/1000\n",
      "Epoch 852/1000\n",
      "Epoch 853/1000\n",
      "Epoch 854/1000\n",
      "Epoch 855/1000\n",
      "Epoch 856/1000\n",
      "Epoch 857/1000\n",
      "Epoch 858/1000\n",
      "Epoch 859/1000\n",
      "Epoch 860/1000\n",
      "Epoch 861/1000\n",
      "Epoch 862/1000\n",
      "Epoch 863/1000\n",
      "Epoch 864/1000\n",
      "Epoch 865/1000\n",
      "Epoch 866/1000\n",
      "Epoch 867/1000\n",
      "Epoch 868/1000\n",
      "Epoch 869/1000\n",
      "Epoch 870/1000\n",
      "Epoch 871/1000\n",
      "Epoch 872/1000\n",
      "Epoch 873/1000\n",
      "Epoch 874/1000\n",
      "Epoch 875/1000\n",
      "Epoch 876/1000\n",
      "Epoch 877/1000\n",
      "Epoch 878/1000\n",
      "Epoch 879/1000\n",
      "Epoch 880/1000\n",
      "Epoch 881/1000\n",
      "Epoch 882/1000\n",
      "Epoch 883/1000\n",
      "Epoch 884/1000\n",
      "Epoch 885/1000\n",
      "Epoch 886/1000\n",
      "Epoch 887/1000\n",
      "Epoch 888/1000\n",
      "Epoch 889/1000\n",
      "Epoch 890/1000\n",
      "Epoch 891/1000\n",
      "Epoch 892/1000\n",
      "Epoch 893/1000\n",
      "Epoch 894/1000\n",
      "Epoch 895/1000\n",
      "Epoch 896/1000\n",
      "Epoch 897/1000\n",
      "Epoch 898/1000\n",
      "Epoch 899/1000\n",
      "Epoch 900/1000\n",
      "Epoch 901/1000\n",
      "Epoch 902/1000\n",
      "Epoch 903/1000\n",
      "Epoch 904/1000\n",
      "Epoch 905/1000\n",
      "Epoch 906/1000\n",
      "Epoch 907/1000\n",
      "Epoch 908/1000\n",
      "Epoch 909/1000\n",
      "Epoch 910/1000\n",
      "Epoch 911/1000\n",
      "Epoch 912/1000\n",
      "Epoch 913/1000\n",
      "Epoch 914/1000\n",
      "Epoch 915/1000\n",
      "Epoch 916/1000\n",
      "Epoch 917/1000\n",
      "Epoch 918/1000\n",
      "Epoch 919/1000\n",
      "Epoch 920/1000\n",
      "Epoch 921/1000\n",
      "Epoch 922/1000\n",
      "Epoch 923/1000\n",
      "Epoch 924/1000\n",
      "Epoch 925/1000\n",
      "Epoch 926/1000\n",
      "Epoch 927/1000\n",
      "Epoch 928/1000\n",
      "Epoch 929/1000\n",
      "Epoch 930/1000\n",
      "Epoch 931/1000\n",
      "Epoch 932/1000\n",
      "Epoch 933/1000\n",
      "Epoch 934/1000\n",
      "Epoch 935/1000\n",
      "Epoch 936/1000\n",
      "Epoch 937/1000\n",
      "Epoch 938/1000\n",
      "Epoch 939/1000\n",
      "Epoch 940/1000\n",
      "Epoch 941/1000\n",
      "Epoch 942/1000\n",
      "Epoch 943/1000\n",
      "Epoch 944/1000\n",
      "Epoch 945/1000\n",
      "Epoch 946/1000\n",
      "Epoch 947/1000\n",
      "Epoch 948/1000\n",
      "Epoch 949/1000\n",
      "Epoch 950/1000\n",
      "Epoch 951/1000\n",
      "Epoch 952/1000\n",
      "Epoch 953/1000\n",
      "Epoch 954/1000\n",
      "Epoch 955/1000\n",
      "Epoch 956/1000\n",
      "Epoch 957/1000\n",
      "Epoch 958/1000\n",
      "Epoch 959/1000\n",
      "Epoch 960/1000\n",
      "Epoch 961/1000\n",
      "Epoch 962/1000\n",
      "Epoch 963/1000\n",
      "Epoch 964/1000\n",
      "Epoch 965/1000\n",
      "Epoch 966/1000\n",
      "Epoch 967/1000\n",
      "Epoch 968/1000\n",
      "Epoch 969/1000\n",
      "Epoch 970/1000\n",
      "Epoch 971/1000\n",
      "Epoch 972/1000\n",
      "Epoch 973/1000\n",
      "Epoch 974/1000\n",
      "Epoch 975/1000\n",
      "Epoch 976/1000\n",
      "Epoch 977/1000\n",
      "Epoch 978/1000\n",
      "Epoch 979/1000\n",
      "Epoch 980/1000\n",
      "Epoch 981/1000\n",
      "Epoch 982/1000\n",
      "Epoch 983/1000\n",
      "Epoch 984/1000\n",
      "Epoch 985/1000\n",
      "Epoch 986/1000\n",
      "Epoch 987/1000\n",
      "Epoch 988/1000\n",
      "Epoch 989/1000\n",
      "Epoch 990/1000\n",
      "Epoch 991/1000\n",
      "Epoch 992/1000\n",
      "Epoch 993/1000\n",
      "Epoch 994/1000\n",
      "Epoch 995/1000\n",
      "Epoch 996/1000\n",
      "Epoch 997/1000\n",
      "Epoch 998/1000\n",
      "Epoch 999/1000\n",
      "Epoch 1000/1000\n",
      "loss : 0.00010883081995416433\n",
      "accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "######## 실습1-7 학습\n",
    "\n",
    "# 학습데이터 준비 (Teacher Forcing)\n",
    "decoder_input_data = fra_padded[ : , : -1]  # <start> + 문장 --> (마지막 토큰 제외)\n",
    "decoder_target_data = fra_padded[ : , 1 : ]  # 문장 + <end>  --> (첫 토큰 제외)\n",
    "print(f'encoder input : {eng_padded.shape}')\n",
    "print(f'decoder input : {decoder_input_data.shape}')\n",
    "print(f'decoder target : {decoder_target_data.shape}')\n",
    "\n",
    "# 학습실행\n",
    "history = model.fit(\n",
    "    [eng_padded, decoder_input_data],\n",
    "    np.expand_dims(decoder_target_data, -1),\n",
    "    batch_size=2,\n",
    "    epochs=1000,\n",
    "    verbose=20\n",
    ")\n",
    "\n",
    "print(f\"loss : {history.history['loss'][-1]}\")\n",
    "print(f\"accuracy : {history.history['acc'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d01f23e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "번역 결과:\n",
      "\n",
      "영어: i love you\n",
      "원본: <start> je t aime <end>\n",
      "번역: je t aime\n",
      "----------------------------------------\n",
      "영어: he is a student\n",
      "원본: <start> il est etudiant <end>\n",
      "번역: il est etudiant\n",
      "----------------------------------------\n",
      "영어: she likes music\n",
      "원본: <start> elle aime la musique <end>\n",
      "번역: elle aime la musique\n",
      "----------------------------------------\n",
      "영어: we are learning attention\n",
      "원본: <start> nous apprenons l attention <end>\n",
      "번역: nous apprenons l attention\n",
      "----------------------------------------\n",
      "영어: you are amazing\n",
      "원본: <start> tu es incroyable <end>\n",
      "번역: tu es incroyable\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def translate(input_sentence):\n",
    "    \"\"\"\n",
    "    영어 문장을 프랑스어로 번역\n",
    "    \"\"\"\n",
    "    # 입력 문장 전처리\n",
    "    input_seq = eng_tokenizer.texts_to_sequences([input_sentence])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_eng_len, padding='post')\n",
    "    \n",
    "    # 디코더 입력 초기화 (<start> 토큰)\n",
    "    start_token_id = fra_tokenizer.word_index['<start>']\n",
    "    end_token_id = fra_tokenizer.word_index['<end>']\n",
    "    \n",
    "    # 디코더 입력: <start> + 패딩\n",
    "    decoder_input = np.zeros((1, max_decoder_len))\n",
    "    decoder_input[0, 0] = start_token_id\n",
    "    \n",
    "    # 번역 생성\n",
    "    output_sentence = []\n",
    "    \n",
    "    for t in range(1, max_decoder_len):\n",
    "        # 예측\n",
    "        predictions = model.predict([input_seq, decoder_input], verbose=0)\n",
    "        \n",
    "        # t-1 시점의 예측 결과에서 가장 높은 확률의 단어 선택\n",
    "        predicted_id = np.argmax(predictions[0, t-1, :])\n",
    "        \n",
    "        # <end> 토큰이면 종료\n",
    "        if predicted_id == end_token_id:\n",
    "            break\n",
    "        \n",
    "        # 단어 추가\n",
    "        predicted_word = fra_tokenizer.index_word.get(predicted_id, '')\n",
    "        if predicted_word not in ['<start>', '<end>', '']:\n",
    "            output_sentence.append(predicted_word)\n",
    "        \n",
    "        # 다음 입력으로 사용\n",
    "        decoder_input[0, t] = predicted_id\n",
    "    \n",
    "    return ' '.join(output_sentence)\n",
    "\n",
    "# 테스트\n",
    "print(\"\\n번역 결과:\\n\")\n",
    "for i in range(len(english_sentences)):\n",
    "    translation = translate(english_sentences[i])\n",
    "    print(f\"영어: {english_sentences[i]}\")\n",
    "    print(f\"원본: {french_sentences[i]}\")\n",
    "    print(f\"번역: {translation}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# for eng_sent in english_sentences:\n",
    "#     translation = translate(eng_sent)\n",
    "#     print(f\"영어: {eng_sent}\")\n",
    "#     print(f\"번역: {translation}\")\n",
    "#     print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
