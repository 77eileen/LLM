{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvUS5t7M-Cud"
      },
      "source": [
        "### Ko GPT2\n",
        "- https://huggingface.co/skt/kogpt2-base-v2\n",
        "--> https://github.com/SKT-AI/KoGPT2\n",
        "- ìƒê¸° git ì— ìˆëŠ” ë‚´ìš© ë³µë¶™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEfpodZX2IQt",
        "outputId": "9dcd3cb5-40a7-4aa6-909c-ba4c6003704a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21356518d4cc479fba5df1bd9ac64d4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\playdata2\\miniconda3\\envs\\P10\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\playdata2\\.cache\\huggingface\\hub\\models--skt--kogpt2-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76013350ea1a42aa9b3dcc3ed7b38f8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['â–ì•ˆë…•',\n",
              " 'í•˜',\n",
              " 'ì„¸',\n",
              " 'ìš”.',\n",
              " 'â–í•œêµ­ì–´',\n",
              " 'â–G',\n",
              " 'P',\n",
              " 'T',\n",
              " '-2',\n",
              " 'â–ì…',\n",
              " 'ë‹ˆë‹¤.',\n",
              " 'ğŸ˜¤',\n",
              " ':)',\n",
              " 'l^o']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "pad_token='<pad>', mask_token='<mask>')\n",
        "tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WhOUs246-v_9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c176443de3c4db28792c8018e053361",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f79809af6f8e4c56ae12aabd142432f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤.\n",
            "íŠ¹íˆ, ì•„ì¹¨ì‹ì‚¬ëŠ” ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•œ ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ë§ì´ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n",
            "ë˜í•œ í•˜ë£¨ 30ë¶„ ì´ìƒ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ë„ì›€ì´ ëœë‹¤.\n",
            "ì•„ì¹¨ ì‹ì‚¬ë¥¼ ê±°ë¥´ì§€ ì•Šê³  ê·œì¹™ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ë©´ í˜ˆì•¡ìˆœí™˜ì— ë„ì›€ì„ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ì§„ëŒ€ì‚¬ë¥¼ ì´‰ì§„í•´ ì²´ë‚´ ë…¸íë¬¼ì„ ë°°ì¶œí•˜ê³  í˜ˆì••ì„ ë‚®ì¶°ì¤€ë‹¤.\n",
            "ìš´ë™ì€ í•˜ë£¨ì— 10ë¶„ ì •ë„ë§Œ í•˜ëŠ” ê²Œ ì¢‹ìœ¼ë©° ìš´ë™ í›„ì—ëŠ” ë°˜ë“œì‹œ ìŠ¤íŠ¸ë ˆì¹­ì„ í†µí•´ ê·¼ìœ¡ëŸ‰ì„ ëŠ˜ë¦¬ê³  ìœ ì—°ì„±ì„ ë†’ì—¬ì•¼ í•œë‹¤.\n",
            "ìš´ë™ í›„ ë°”ë¡œ ì ìë¦¬ì— ë“œëŠ” ê²ƒì€ í”¼í•´ì•¼ í•˜ë©° íŠ¹íˆ ì•„ì¹¨ì— ì¼ì–´ë‚˜ë©´ ëª¸ì´ í”¼ê³¤í•´ì§€ê¸° ë•Œë¬¸ì— ë¬´ë¦¬í•˜ê²Œ ì›€ì§ì´ë©´ ì˜¤íˆë ¤ ì—­íš¨ê³¼ê°€ ë‚  ìˆ˜ë„ ìˆë‹¤.\n",
            "ìš´ë™ì„\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "gen_ids = model.generate(input_ids,\n",
        "                           max_length=128,\n",
        "                           repetition_penalty=2.0,\n",
        "                           pad_token_id=tokenizer.pad_token_id,\n",
        "                           eos_token_id=tokenizer.eos_token_id,\n",
        "                           bos_token_id=tokenizer.bos_token_id,\n",
        "                           use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8poPXgZl_dbP",
        "outputId": "b2096069-58c4-4ca3-aa98-c3dd1516dd47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "Both `max_new_tokens` (=100) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ìœ êµ¬í•œ ì—­ì‚¬ì™€ ì „í†µì— ë¹›ë‚˜ëŠ” ìš°ë¦¬ ëŒ€í•œë¯¼êµ­ì€ 3.1ìš´ë™ìœ¼ë¡œ ê±´ë¦½ëœ ëŒ€í•œë¯¼êµ­ì„ì‹œì •ë¶€ì™€ ë²•í†µê³¼ ë¶ˆì˜ì— í•­ê±°í•œ\n",
            "\n",
            "\n",
            " í•œì¤„ ìš”ì•½: êº½ì´ë“¤ì´ ì´ ë•…ê³¼ ë‚˜ë¼ì˜ ì£¼ì¸ì´ë‹¤\n",
            "ìš°ë¦¬ì˜ ë…ë¦½êµ°ì€ ë…ë¦½ìš´ë™ì˜ ì—­ì‚¬ë¥¼ ì´ì–´ë°›ì•„ ì˜¤ëŠ˜ì— ì´ë¥´ê³  ìˆë‹¤.\n",
            "ë…ë¦½êµ°ì´ íƒ„ìƒì‹œí‚¨ ë‚˜ë¼, ëŒ€í•œë¯¼êµ­ì˜ ë¯¸ë˜ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” ë°˜ë“œì‹œ ë…ë¦½ì„ ìŸì·¨í•´ì•¼ í•œë‹¤.\n",
            "ì§€ê¸ˆê¹Œì§€ì˜ ë…¸ë ¥ì€ í° ê²°ì‹¤ì„ ë§ºì„ ìˆ˜ ìˆì§€ë§Œ ê·¸ ê³¼ì •ì—ì„œ ìš°ë¦¬ê°€ ê²ªì–´ì•¼ í•  ì–´ë ¤ì›€ì´ ì ì§€ ì•Šë‹¤.\n",
            "ìš°ë¦¬ë‚˜ë¼ ì„ì‹œì •ë¶€ì˜ ë…ë¦½ìš´ë™ ì—­ì‚¬ëŠ” ê±´êµ­ ì´ˆê¸° ì´ìŠ¹ë§Œ ì •ë¶€ê°€ ì„¸ìš´ ëŒ€í•œë…ë¦½ì´‰ì„±êµ­ë¯¼íšŒë¼ëŠ” ìƒˆë¡œìš´ ë‹¨ì²´ê°€ ìˆì—ˆë‹¤.\n",
            "ê·¸ í›„ ì„ì‹œì •ë¶€ ìˆ˜ë¦½ì€ ëŒ€í•œë¯¼êµ­ ì„ì‹œì •ë¶€ì˜ ì •í†µì„±ê³¼ ì •í†µì„±ì„ íšŒë³µí•˜ê³ , ì •ë¶€ìˆ˜ë¦½ ì´í›„ ë¯¼ì¡±ì˜ ë¯¼ì£¼í™”ë¥¼ ì‹¤í˜„í•˜ê¸° ìœ„í•œ ì²«ê±¸ìŒìœ¼ë¡œ ì¶”ì§„ëœ ê²ƒì´ë‹¤.\n",
            "ì„ì‹œì •ë¶€ëŠ” ë¯¼ì¡±ì§€ë„\n"
          ]
        }
      ],
      "source": [
        "text = '''\n",
        "ìœ êµ¬í•œ ì—­ì‚¬ì™€ ì „í†µì— ë¹›ë‚˜ëŠ” ìš°ë¦¬ ëŒ€í•œë¯¼êµ­ì€ 3.1ìš´ë™ìœ¼ë¡œ ê±´ë¦½ëœ ëŒ€í•œë¯¼êµ­ì„ì‹œì •ë¶€ì™€ ë²•í†µê³¼ ë¶ˆì˜ì— í•­ê±°í•œ\n",
        "'''\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    \"skt/kogpt2-base-v2\",\n",
        "    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "    pad_token='<pad>', mask_token='<mask>')\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "model.eval()  # ì¶”ë¡ ëª¨ë“œë¡œ ë°”ê¿”ì„œ í•™ìŠµì´ ì•„ë‹Œ ë¬¸ì¥ ìƒì„± ì¤€ë¹„\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
        "# í”„ë¡¬í”„íŠ¸ê°€ ì¤‘ìš”í•¨!!!! í•œì¤„ìš”ì•½ì´ ì•„ë‹Œ, í•µì‹¬ë‹¨ì–´ ë“±ë“±ì˜ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ë‹¬ë¼ì§€ë¯€ë¡œ.\n",
        "prompt = f'{text}\\n\\n í•œì¤„ ìš”ì•½: '\n",
        "# ì¸ì½”ë”© (í”„ë¡¬í”„íŠ¸ë¥¼ ëª¨ë¸ì— ë„£ê¸° ìœ„í•´ì„œ í† í¬ë‚˜ì´ì§•)\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "# ëª¨ë¸ ì¶”ë¡  (ìƒì„±)\n",
        "with torch.no_grad():\n",
        "    gen_ids = model.generate(input_ids,\n",
        "                            max_length=256,\n",
        "                            max_new_tokens = 100,  #ìƒˆë¡œ ìƒì„±í•  ìµœëŒ€ í† í°ìˆ˜ \n",
        "                            repetition_penalty=2.0, # ê°™ì€ ë§ì„ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ ë²Œì ì„ ì¤Œ\n",
        "                            pad_token_id=tokenizer.pad_token_id, # ì›ë˜ ì´ ëª¨ë¸ì´ ê°–ê³  ìˆë˜ê²ƒ? ì¤‘ìš”í•˜ì§€ì•ŠìŒ..\n",
        "                            eos_token_id=tokenizer.eos_token_id, # ì›ë˜ ì´ ëª¨ë¸ì´ ê°–ê³  ìˆë˜ê²ƒ? ì¤‘ìš”í•˜ì§€ì•ŠìŒ..\n",
        "                            bos_token_id=tokenizer.bos_token_id, # ì›ë˜ ì´ ëª¨ë¸ì´ ê°–ê³  ìˆë˜ê²ƒ? ì¤‘ìš”í•˜ì§€ì•ŠìŒ..\n",
        "                            use_cache=True, \n",
        "                            do_sample=True, #í™•ë¥ ì  ìƒ˜í”Œë§(ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ë§Œë“¤ì–´ì¤Œ)\n",
        "                            temperature=0.7, #ì°½ì˜ì„± ì¡°ì • (ë‚®ì„ìˆ˜ë¡ ë³´ìˆ˜ì , ë†’ì„ìˆ˜ë¡ í™˜ê° ê°€ëŠ¥ì„± ë†’ìŒ)\n",
        "                            top_k=50 #í™•ë¥ ì ìœ¼ë¡œ ìƒìœ„ 50ê°œ ë‹¨ì–´ì¤‘ì—ì„œ ì„ íƒ(ë‹¤ì–‘ì„± ì¡°ì •)\n",
        "                            )\n",
        "    \n",
        "# ê²°ê³¼ ë””ì½”ë”© : ëª¨ë¸ì´ ìƒì„±í•œ í† í°ì„ ë‹¤ì‹œ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ ì´í›„ì— ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
        "summary = generated[len(prompt):].strip()\n",
        "print(generated)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "undp1p51DUnA"
      },
      "source": [
        "- ê¸°ì¡´ ë¬¸ì¥ì— ìˆëŠ”ê²ƒì— ëŒ€í•´ í•˜ê³  ì‹¶ë‹¤ë©´... BERTëª¨ë¸ë¡œ. ìƒˆë¡œ ìƒì„±ë˜ëŠ”ê²Œ ì‹«ë‹¤ë©´.. (í—¤ë“œì •ë³´ê°€ ìˆì–´ì•¼í•¨)\n",
        "\n",
        "- BERTëŠ” encode\n",
        "- gptëŠ” decode\n",
        "- KobartëŠ” encode, decode ë‘˜ë‹¤ ê°€ì§€ê³  ìˆìŒ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "P10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
