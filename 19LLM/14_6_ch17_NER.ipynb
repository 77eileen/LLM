{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBXzbkDgJVBx"
      },
      "source": [
        "### 개체명 인식: NER\n",
        "- NER(Named Entity Recognition, 개체명 인식)에서는 문장 속에서 사람, 장소, 조직, 날짜 같은 의미 있는 단어들을 찾아내는 작업을 함.\n",
        "- 텍스트에서 특정 의미를 가진 단어나 구절을 찾아내고 분류하는 작업\n",
        "- 이때 단어(token)가 개체인지 표시하는 형식이 BIO 방식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Dz6uYEGJOa3"
      },
      "outputs": [],
      "source": [
        "# 홍길동은 2025년 11월 19일 서울시청에서 삼성전자 직원을 만났다\n",
        "# 홍길동 - [인명]\n",
        "# 2024년 1월 15일 - [날짜]\n",
        "# 서울시청 - [지명]\n",
        "# 삼성전자 - [기관명]\n",
        "\n",
        "# 활용분야\n",
        "    # 뉴스기사 : 기사에서 인물, 장소, 기관 자동 추출\n",
        "    # 의료문서 : 병명, 약물명, 증상\n",
        "    # 계약서 : 회사명, 날짜, 금액\n",
        "    # 챗봇 : 사용자 질문에 핵심정보 파악\n",
        "\n",
        "# BIO 태깅\n",
        "    # B(Begin) 개체 시작\n",
        "    # I(Inside) 개체 내부\n",
        "    # O(Outside) 개체가 아님"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "STsMPcs9Lnpm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgvLJHHzMaQF",
        "outputId": "ff43fbe2-2589-48af-d4a5-1b2426c93010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 김철수는         | B-PER    | 'PER' 개체의 시작\n",
            " 2024년        | B-DAT    | 'DAT' 개체의 시작\n",
            " 1월           | I-DAT    | 'DAT' 개체의 내부\n",
            " 15일          | I-DAT    | 'DAT' 개체의 내부\n",
            " 서울시청에서       | B-LOC    | 'LOC' 개체의 시작\n",
            " 삼성전자         | B-ORG    | 'ORG' 개체의 시작\n",
            " 직원을          | O        | 개체가 아님\n",
            " 만났다          | O        | 개체가 아님\n"
          ]
        }
      ],
      "source": [
        "# BIO 태깅\n",
        "tokens = [\"김철수는\", \"2024년\", \"1월\", \"15일\", \"서울시청에서\", \"삼성전자\", \"직원을\", \"만났다\"]\n",
        "bio_tags = [\"B-PER\", \"B-DAT\", \"I-DAT\", \"I-DAT\", \"B-LOC\", \"B-ORG\", \"O\", \"O\"]  #B-PER person's begin, B-DAT date's begin, B-LOC location's begin, ... # 직원을 만났다는 개체명이 아님. \n",
        "for token, tag in zip(tokens, bio_tags):\n",
        "    if tag.startswith('B-'):\n",
        "        desc = f\"'{tag[2:]}' 개체의 시작\"\n",
        "    elif tag.startswith('I-'):\n",
        "        desc = f\"'{tag[2:]}' 개체의 내부\"\n",
        "    else :\n",
        "        desc = \"개체가 아님\"\n",
        "    print(f\" {token:12} | {tag:8} | {desc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "sz3--ijTNMc7"
      },
      "outputs": [],
      "source": [
        "# 학습데이터\n",
        "train_sentences = [\n",
        "    [\"김철수는\", \"서울에\", \"산다\"],\n",
        "    [\"이영희는\", \"2024년에\", \"부산으로\", \"이사했다\"],\n",
        "    [\"삼성전자는\", \"대한민국의\", \"대기업이다\"],\n",
        "    [\"박지성은\", \"축구선수다\"],\n",
        "    [\"2025년\", \"1월\", \"1일은\", \"새해다\"],\n",
        "]\n",
        "\n",
        "train_labels = [\n",
        "    [\"B-PER\", \"B-LOC\", \"O\"],\n",
        "    [\"B-PER\", \"B-DAT\", \"B-LOC\", \"O\"],\n",
        "    [\"B-ORG\", \"B-LOC\", \"O\"],\n",
        "    [\"B-PER\", \"O\"],\n",
        "    [\"B-DAT\", \"I-DAT\", \"I-DAT\", \"O\"],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63WjHhp6N1c_",
        "outputId": "1f46cd1f-5f72-406f-9780-b72f8261ea5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[517, 490, 494,   0, 517,   0, 491,   0, 491,   0, 517,   0,   0,   0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ast import mod\n",
        "# 토크나이저\n",
        "MODEL_NAME = 'skt/kobert-base-v1'\n",
        "tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "text = '김철수는 서울에 산다'\n",
        "# 토크나이저\n",
        "tokens = tokenizer.tokenize(text)\n",
        "# 인코딩\n",
        "encoded = tokenizer(text, return_tensors='pt') # 객체를 생성자에 넣으면.. encode\n",
        "encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K_Hf6DP0PC0C"
      },
      "outputs": [],
      "source": [
        "# NER 모델 4단계로 구성\n",
        "# 1. 입력 테스트\n",
        "# 2. koBERT 인코더 : 문장의 의미를 이해\n",
        "# 3. 분류기(Linear) : 예측\n",
        "# 4. 출력 라벨 : B-PER, O, B-LOC 이렇게 나옴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "TQ6gQnz2UCjM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "테스트 문장 : 김철수는 서울에 산다\n",
            "정답 라벨 : B-PER B-LOC O\n",
            "김철수는       -> O        정답 : B-PER\n",
            "김철수는       -> O        정답 : B-PER\n",
            "김철수는       -> O        정답 : B-PER\n",
            "김철수는       -> O        정답 : B-PER\n",
            "서울에        -> O        정답 : B-LOC\n",
            "서울에        -> O        정답 : B-LOC\n",
            "서울에        -> O        정답 : B-LOC\n",
            "서울에        -> O        정답 : B-LOC\n",
            "서울에        -> O        정답 : B-LOC\n",
            "서울에        -> O        정답 : B-LOC\n",
            "산다         -> O        정답 : O\n",
            "산다         -> O        정답 : O\n"
          ]
        }
      ],
      "source": [
        "# 상기 내용 시현을 위한 함수 작성\n",
        "\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "class SimpleNERModel(nn.Module):\n",
        "  def __init__(self, num_labels) -> None:\n",
        "    super(SimpleNERModel, self).__init__()\n",
        "    self.num_labels = num_labels\n",
        "    self.bert = AutoModel.from_pretrained(MODEL_NAME)   #AutoModel 출력이 없음. 그래서 마지막 은닉상태지정해주고 연결 해주는게 필요함.\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.clf = nn.Linear(self.bert.config.hidden_size,  self.num_labels) #config.hidde_size ==> last_hidden_size에 해당\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    # kobert로 문자 인코딩\n",
        "    outputs = self.bert(input_ids, attention_mask=attention_mask) # inputs_ids, attention_mask를 forward가 넘겨받음 \n",
        "    # 마지막 은닉상태 추출 : AutoModel 출력이 없음. 그래서 마지막 은닉상태지정해주고 연결 해주는게 필요함.\n",
        "    sequence_output =  outputs.last_hidden_state\n",
        "    # Dropout 적용 - 과적합 방지\n",
        "    sequence_output = self.dropout(sequence_output)\n",
        "    # 분류기\n",
        "    logits = self.clf(sequence_output)\n",
        "    return logits\n",
        "# 라벨의 갯수 : 왜 확인? SimpleNERMODEL에 라벨갯수를 받아야 하므로. 클래스의 갯수!  \n",
        "label_list = sorted(list(set([data for i in train_labels for data in i])))\n",
        "label_list\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i : label for i, label in enumerate(label_list)}\n",
        "model = SimpleNERModel(num_labels=len(label_list))\n",
        "\n",
        "\n",
        "# 모델 학습\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "# 순전파 테스트 forward\n",
        "sample_sentence = train_sentences[0]\n",
        "sample_label = train_labels[0]\n",
        "print(f\"테스트 문장 : {' '.join(sample_sentence)}\")   #sample_sentence 리스트라서 붙이기 위해 .join\n",
        "print(f\"정답 라벨 : {' '.join(sample_label)}\")\n",
        "\n",
        "\n",
        "encoding = tokenizer(sample_sentence, return_tensors='pt', truncation=True, padding=True, max_length=32, is_split_into_words=True)\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  logits = model(input_ids, attention_mask)\n",
        "  predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "\n",
        "# 예측결과\n",
        "word_ids = encoding.word_ids(batch_index=0)\n",
        "pred_label = []\n",
        "for i, word_idx in enumerate(word_ids):\n",
        "  if word_idx is not None and i < len(predictions[0]):\n",
        "    pred_label = id2label[ predictions[0][i].item() ]\n",
        "    if word_idx < len(sample_sentence):\n",
        "      print(f\"{sample_sentence[word_idx]:10} -> {pred_label:8} 정답 : {sample_label[word_idx]}\")\n",
        "\n",
        "# 포맷코드 정리\n",
        "# :<10 : 왼쪽정렬(문자열/숫자모두가능)\n",
        "# :>10 : 오른쪽 정렬\n",
        "# :^10 : 가운데 정렬\n",
        "# :10  : 숫자형일 때만 적용 가능 → 문자열에 쓰면 오류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 DataSet\n",
        "from torch.utils.data import Dataset\n",
        "class NerDataSet(Dataset):\n",
        "  def __init__(self,sentences,labels, tokenizer,max_len=64) -> None:\n",
        "    self.sentences = sentences\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "  def __getitem__(self,idx):\n",
        "    words = self.sentences[idx]\n",
        "    lbls = self.labels[idx]\n",
        "    encoding = self.tokenizer(\n",
        "        words,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=self.max_len,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "    word_ids = encoding.word_ids(batch_index = 0)\n",
        "    label_ids = []\n",
        "    for w in word_ids:\n",
        "      if w is None:\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        label_ids.append(label2id[lbls[w]])\n",
        "    return {\n",
        "        'input_ids' : encoding['input_ids'].squeeze(),\n",
        "        'attention_mask' : encoding['attention_mask'].squeeze(),\n",
        "        'labels' : torch.tensor(label_ids)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 DataSet\n",
        "from torch.utils.data import Dataset\n",
        "class NerDataSet(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer, max_len=4) -> None:\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "    def __getitem__(self, idx):\n",
        "        words = self.sentences[idx]\n",
        "        ibls = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            words,\n",
        "            return_tensors='pt',\n",
        "            truncation= True,\n",
        "            padding=True,\n",
        "            max_length=self.max_len,\n",
        "            is_split_into_words=True\n",
        "        )\n",
        "        word_ids = encoding.word_ids(batch_index=0)\n",
        "        label_ids=[]\n",
        "        for w in word_ids:\n",
        "            if w is None:\n",
        "                label_ids.append(-100)\n",
        "            else:\n",
        "                label_ids.append(label2id[ibls[w]])\n",
        "        return {\n",
        "            'input_ids' : encoding['input_ids'].squeeze(),\n",
        "            'attention_mask' : encoding['attention_mask'].squeeze(),\n",
        "            'labels' : torch.tensor(label_ids)\n",
        "        }       \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[517, 491,   0,   0],\n",
              "         [517,   0,   0,   0]]),\n",
              " 'attention_mask': tensor([[1, 1, 1, 1],\n",
              "         [1, 1, 1, 1]]),\n",
              " 'labels': tensor([[   3,    3, -100, -100],\n",
              "         [   2,    2, -100, -100]])}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataset = NerDataSet(train_sentences, train_labels, tokenizer, max_len=4)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 선언 및 학습\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "model=SimpleNERModel(num_labels=len(label_list))\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch1 loss: 1.6340\n",
            " epoch2 loss: 1.4871\n",
            " epoch3 loss: 1.3171\n",
            " epoch4 loss: 1.2291\n",
            " epoch5 loss: 1.1002\n",
            " epoch6 loss: 1.2710\n",
            " epoch7 loss: 0.9641\n",
            " epoch8 loss: 0.9119\n",
            " epoch9 loss: 0.9528\n",
            " epoch10 loss: 0.7656\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "       optimizer.zero_grad()\n",
        "       input_ids = batch['input_ids'].to(device) \n",
        "       attention_mask = batch['attention_mask'].to(device)\n",
        "       labels = batch['labels'].to(device)\n",
        "       logits = model(input_ids, attention_mask) #skt/ ~ 모델은 input_ids, attention_mask만 받음. logit 객체는 갖고 있음\n",
        "       loss = criterion(logits.view(-1, model.num_labels), labels.view(-1))\n",
        "       total_loss += loss.item()\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "    print(f' epoch{epoch+1} loss: {total_loss/len(train_loader):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['김철수는', '서울에', '산다'], ['B-PER', 'B-LOC', 'O'])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_sentences[0], train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['김철수는', '서울에', '산다'] ['B-PER', 'B-LOC', 'O']\n",
            "{'input_ids': tensor([[517, 490, 494,   0, 517,   0, 491,   0, 491,   0, 517,   0,   0,   0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "결과\n",
            "김철수는 -> B-PER 정답 : B-PER\n",
            "서울에 -> B-PER 정답 : B-LOC\n",
            "산다 -> B-PER 정답 : O\n"
          ]
        }
      ],
      "source": [
        "# 평가\n",
        "sample_sentence, sample_label =  train_sentences[0], train_labels[0]\n",
        "print(sample_sentence, sample_label)\n",
        "encoding = tokenizer(\n",
        "        sample_sentence,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=20,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "print(encoding)\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  logits = model(input_ids, attention_mask)\n",
        "  predictions = torch.argmax(logits, dim=-1)[0]\n",
        "print('결과')\n",
        "word_ids = encoding.word_ids(batch_index=0)\n",
        "printed_word = set()\n",
        "\n",
        "for i ,word_idx in enumerate(word_ids):\n",
        "  if word_idx is not None:\n",
        "    if word_idx not in printed_word:\n",
        "      printed_word.add(word_idx)\n",
        "      pred_label = id2label[predictions[i].item()]\n",
        "      print(f'{sample_sentence[word_idx]} -> {pred_label} 정답 : {sample_label[word_idx]}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "P10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
