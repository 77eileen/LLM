{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTVfdYdqWzFy"
      },
      "source": [
        "#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric\n",
        "- 문서를 요약했는데, 어떻게 품질을 측정하나?\n",
        "- 겹침정도를 측정 (ex. 문장내에서 핵심적인 단어가 요약문제 없다? 그럼 요약이 잘 안된거지 않을까?)\n",
        "- ROUGE-1 : 1개 단어단위로 겹침정도를 판단\n",
        "    - 정답 : 고양이가 생선을 먹었다\n",
        "    - 생성 : 고양이가 물고기를 먹었다\n",
        "    - 겹침 : 고양이가, 먹었다\n",
        "    - =====> 2/4 = 0.5\n",
        "- ROUGE-2 : 2개 단어단위로 겹침정도를 판단 : 순서도 고려 대상이 됨\n",
        "- ROUGE-L : 가장 긴 공통 부분수열로 겹침정도 판단\n",
        "\n",
        "- 단점: 의미는 같지만, 다른표현을 쓰면 점수가 낮음.\n",
        "    - 자동차 vs 차량 : 겹침없음으로 판단함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eQ9JR7tlpzqE"
      },
      "outputs": [],
      "source": [
        "# 토크나이저"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ruXy2B8N0tRY"
      },
      "outputs": [],
      "source": [
        "# 배치처리, 데이터 콜레이터 (배치를 만들때 길이를 맞춰주는 작업)\n",
        "# 효율적인 데이터 처리방법을 알아보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0-0C8oh0x-Y"
      },
      "source": [
        "- 개별 처리:\n",
        "    - [문장1] -> 모델 -> [출력1]\n",
        "    - [문장2] -> 모델 -> [출력2]\n",
        "\n",
        "- 배치 처리:   [문장1, 문장2, ...] -> 모델 -> [출력1, 출력2, ...]\n",
        "\n",
        "- DataCollator:  토큰화 + 패딩 + 라벨 시프트 + tensor 변환 -> 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4JWij1SYbvW",
        "outputId": "2a525615-625a-4206-d1a0-31214dabaafb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aac52253ae144fabef07c159a88e711",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\playdata2\\miniconda3\\envs\\P10\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\playdata2\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92df2129862a46c691f6f02df01ec40b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d1aca7bf4dc4192b6189052e37fa745",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a61a2bd05e54ca18b3784d3e93c41c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8daada9c6694941bcb7fa0d11da71fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch, time\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
        "    # DataCollatorForSeq2Seq :\n",
        "    # seq2seq(Encoder-Decoder) 모델 학습용 데이터 배치 전처리를 자동화해주는 유틸\n",
        "    # 배치 단위에서 모델 입력과 라벨을 자동으로 맞춰주는 도우미\n",
        "    # seq2seq 학습시, 배치단위로 \"패딩-정렬-라벨시프트\" 등을 자동처리하는 데이터 정렬도구\n",
        "        # 배치생성 - 길이가 다른 문장들을 동일길이로 패딩하는 것을 일컫음\n",
        "        # 라벨 시프트 - 디코더 입력과 라벨이 한칸씩 밀리도록 자동 변환 (teacher forcing에 필요한 작업)\n",
        "        # 배치단위 tensor 변환 - list of dict 형태를 torch.Tensor로 변환\n",
        "        # DataLoader 안에서 사용\n",
        "\n",
        "    # AutoModelForSeq2SeqLM :\n",
        "    # 입력을 받아서 다른 텍스트를 생성하는 seq2seq 모델을 자동로드\n",
        "        # ---> Seq2Seq이므로 Encoder-Decoder 모델을 자동으로\n",
        "        # ---> 이 라이브러리 안에는 T5, BART, MarianMT 모델이 있음\n",
        "        # ---> 번역/요약/Q&A/문장변환에 최적화 (입력들어오면 출력하는 형태에 최적화)\n",
        "\n",
        "# 모델 : t5-small을 갖는 생성형 모델\n",
        "MODEL_NAME = 't5-small'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "texts = [\n",
        "    \"summarize: The cat sat on the mat.\",\n",
        "    \"summarize: Python is a popular programming language.\",\n",
        "    \"summarize: Machine learning is a subset of artificial intelligence.\",\n",
        "    \"summarize: The weather is nice today.\",\n",
        "    \"summarize: I love reading books in my free time.\",\n",
        "    \"summarize: Coffee is one of the most popular beverages worldwide.\",\n",
        "    \"summarize: Regular exercise is important for health.\",\n",
        "    \"summarize: The Internet has changed how we communicate.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fcfj3YsrWX9",
        "outputId": "377993c2-b3cc-4f0c-8f4e-0a86f8aab6c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "개별처리 소요시간: 3.88초\n",
            "개별처리 문장당 소요:  0.4847초\n"
          ]
        }
      ],
      "source": [
        "# 개별처리 vs 배치처리\n",
        "\n",
        "\n",
        "### 개별처리\n",
        "# 특징: 문장 하나씩 모델에 입력 -> 출력 -> 다음문장 처리\n",
        "# 장점: 단순, 직관적\n",
        "# 단점: 속도 느림, GPU활용 비효율\n",
        "\n",
        "start_time = time.time()\n",
        "result_individual = []\n",
        "with torch.no_grad():\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
        "        # print(inputs) # inputs 안에 들어있는거 확인 : input_ids, attention_mask 2개 들어있음\n",
        "        outputs = model.generate(**inputs, max_length=30)\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        result_individual.append(summary)\n",
        "time_indivisual = time.time() - start_time\n",
        "print(f\"개별처리 소요시간: {time_indivisual:.2f}초\")\n",
        "print(f\"개별처리 문장당 소요: {time_indivisual/ len(texts): .4f}초\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbGody38u6IO",
        "outputId": "baa04ca8-db8b-4e66-b38d-1400d5f1f06e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "배치처리 소요시간: 0.58초\n",
            "배치처리 문장당 소요:  0.0727초\n"
          ]
        }
      ],
      "source": [
        "### 배치처리\n",
        "# 특징: 여러문장을 모아서 한번에 모델에 입력\n",
        "# 장점: GPU 병렬 처리로 속도 빠름, 효율적\n",
        "# 단점: 문장 길이가 다르면 패딩 필요\n",
        "\n",
        "start_time = time.time()\n",
        "result_batch = []\n",
        "batch_inputs = tokenizer(\n",
        "    texts,\n",
        "    return_tensors='pt',\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ").to(device)\n",
        "outputs = model.generate(**batch_inputs, max_length=30)\n",
        "for output in outputs:\n",
        "    summary = tokenizer.decode(output, skip_sepcial_tokens=True)\n",
        "    result_batch.append(summary)\n",
        "time_batch = time.time() - start_time\n",
        "print(f\"배치처리 소요시간: {time_batch:.2f}초\")\n",
        "print(f\"배치처리 문장당 소요: {time_batch/ len(texts): .4f}초\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "489srmZ4veZW",
        "outputId": "64c9fcca-9978-49ee-8d4c-85d91ab91bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[21603,    10,    37,  1712,     3,     7,   144,    30,     8,  6928,\n",
            "             5,     1,     0,     0],\n",
            "        [21603,    10, 20737,    19,     3,     9,  1012,  6020,  1612,     5,\n",
            "             1,     0,     0,     0],\n",
            "        [21603,    10,  5879,  1036,    19,     3,     9,   769,  2244,    13,\n",
            "          7353,  6123,     5,     1],\n",
            "        [21603,    10,    37,  1969,    19,  1245,   469,     5,     1,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [21603,    10,    27,   333,  1183,  1335,    16,    82,   339,    97,\n",
            "             5,     1,     0,     0],\n",
            "        [21603,    10, 10429,    19,    80,    13,     8,   167,  1012, 18928,\n",
            "          4388,     5,     1,     0],\n",
            "        [21603,    10, 17116,  2510,    19,   359,    21,   533,     5,     1,\n",
            "             0,     0,     0,     0],\n",
            "        [21603,    10,    37,  1284,    65,  2130,   149,    62,  4521,     5,\n",
            "             1,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]), 'labels': None}\n",
            "소요시간: 0.01초\n",
            "문장당:  0.0007초\n"
          ]
        }
      ],
      "source": [
        "# DataCollatorForSeq2Seq  seq2se1 학습용 배치 생성\n",
        "# 특징 : 배치 생성, 패딩, 라벨 시프트 등 seq2seq 학습 전처리 자동화\n",
        "# 장점:\n",
        "    # 여러 문장을 한 번에 배치로 만들어 줌\n",
        "    # 길이가 서로 다른 문장 자동 패딩\n",
        "    # 라벨 시프트(teacher forcing) 자동 처리\n",
        "    # PyTorch Tensor로 변환 → 바로 모델 입력 가능\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
        "tokenized = []\n",
        "for text in texts:\n",
        "    encoded = tokenizer(text,truncation=True)\n",
        "    tokenized.append(encoded)\n",
        "batch = data_collator(tokenized)\n",
        "time_indivisual = time.time() - start_time\n",
        "print(batch)\n",
        "print(f\"소요시간: {time_indivisual:.2f}초\")\n",
        "print(f\"문장당: {time_indivisual/ len(texts): .4f}초\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "P10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
