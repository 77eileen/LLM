{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AMglx-w2x5oX",
        "outputId": "ad33a2d1-67d7-42cf-935b-f6bd013ee034"
      },
      "outputs": [],
      "source": [
        "# %pip install transformers torch datasets evaluate rouge-score sentencepiece accelerate pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kHOQJ8xrz9TB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\playdata2\\miniconda3\\envs\\P10\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    pipeline,                          # 고수준 API - 가장 쉬운 방법\n",
        "    AutoTokenizer,                     # 자동 토크나이저\n",
        "    AutoModelForSeq2SeqLM,            # 자동 모델 로더 (Seq2Seq 작업용)\n",
        "    T5TokenizerFast,                  # T5 전용 고속 토크나이저\n",
        "    T5ForConditionalGeneration,       # T5 모델 클래스\n",
        "    BartForConditionalGeneration,     # BART 모델 클래스\n",
        "    PreTrainedTokenizerFast,          # 사전학습 토크나이저 기본 클래스\n",
        "    DataCollatorForSeq2Seq,           # Seq2Seq 학습용 데이터 콜레이터\n",
        "    Seq2SeqTrainingArguments,         # Seq2Seq 학습 하이퍼파라미터\n",
        "    Seq2SeqTrainer,                   # Seq2Seq 전용 트레이너\n",
        ")\n",
        "\n",
        "# 경고메세지 숨김\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I_QOvARwFIhv"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 라이브러리\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "260PI7Q3FMDq",
        "outputId": "3499f14d-77e4-4913-b705-b88a0dab9b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pytorch 버전 : 2.9.1+cpu\n",
            "transformer 버전 : 4.57.1\n"
          ]
        }
      ],
      "source": [
        "# 버전확인\n",
        "print(f\"pytorch 버전 : {torch.__version__}\")\n",
        "print(f\"transformer 버전 : {__import__('transformers').__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0R3alM0M3A9"
      },
      "source": [
        "### pipeline을 이용한 간단한 문서 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vs85XzTFo8m",
        "outputId": "8da9d5d4-e16f-4c1f-caa0-cc6ab2f5f829"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23b8e4ec28024e16a5cdc7aa37e71cb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2857e4041c2f48aa894587c22ba540ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ae8ac84e1ac4df6bc22977e8f580f58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "645e8e1b0d49480a8645e89147548dfc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "380bc98ff6dd44cda31909719dc8fa3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "978877a038634637b0afa202e361b507",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.\n"
          ]
        }
      ],
      "source": [
        "# pipeline을 이용한 간단한 문서 요약\n",
        "'''\n",
        "모델 다운로드 및 로딩\n",
        "토크나이제이션(문자-> 숫자)\n",
        "모델추론(요약생성)\n",
        "디코딩(숫자-> 문자)\n",
        "\n",
        "장점: 적으 코드로 실행가능(2~3줄)\n",
        "단점: 세밀한 제어 어려)\n",
        "\n",
        "언제:\n",
        "    빠른 프로토타입 제작\n",
        "    간단한 데모\n",
        "    성능테스트\n",
        "'''\n",
        "summarizer = pipeline (\n",
        "    'summarization', #작업유형\n",
        "    model = \"facebook/bart-large-cnn\", #facebook 에서 개발한 요약모델(생성형)\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        ")\n",
        "\n",
        "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
        "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
        "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
        "2010 marriage license application, according to court documents.\n",
        "Prosecutors said the marriages were part of an immigration scam.\n",
        "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
        "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
        "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
        "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
        "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
        "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
        "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
        "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
        "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
        "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
        "\"\"\"\n",
        "\n",
        "summary_result = summarizer(ARTICLE,\n",
        "                            max_length=130, #요약문 최대길이\n",
        "                            min_length=30, #요약문 최소길이\n",
        "                            do_sample=False) #매번 동일한 결과(재현성)\n",
        "print(summary_result[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MNHuLVAI-ba",
        "outputId": "6b7cecd5-96b9-4759-e70f-fcf16af1630e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "원문길이 : 2298 문자 (369 단어)\n",
            "요약문 길이 :  262 문자 ( 47 단어)\n",
            "압축비율: 11.40%\n",
            "단어감소: 322 단어절감\n"
          ]
        }
      ],
      "source": [
        "summary_text = summary_result[0]['summary_text']\n",
        "# 통계분석\n",
        "compression_ratio = len(summary_text) / len(ARTICLE)*100\n",
        "word_reduced = len(ARTICLE.split()) - len(summary_text.split())\n",
        "print(f\"원문길이 : {len(ARTICLE):4d} 문자 ({len(ARTICLE.split()):3d} 단어)\")\n",
        "print(f\"요약문 길이 : {len(summary_text):4d} 문자 ({len(summary_text.split()):3d} 단어)\")\n",
        "print(f\"압축비율: {compression_ratio:.2f}%\")\n",
        "print(f\"단어감소: {word_reduced} 단어절감\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UcOT8fQMxqG"
      },
      "source": [
        "### T5 모델과 AutoModel 을 이용한 문서요약\n",
        "\n",
        "- 모든 nlp작업을 텍스트 -> 텍스트 형식으로 통일\n",
        "- 요약 : \"summarize : [원문] --> [요약문]\"\n",
        "- 번역 : \"translate English to French : [원문] --> [번역문]\"\n",
        "- 분류 : \"sentiment [원문] -> [클래스]\" i love this! --> positive\n",
        "\n",
        "- 단점 : Task Prefix 필수! (없으면 급격한 성능저하) ---> 위의 예시처럼 문제의 유형을 알려주는 # 짧은 문장을 입력앞에 붙임\n",
        "\n",
        "- Task Prefix는 T5(T5-small, T5-base, T5-large 등) 모델이 어떤 작업을 수행해야 하는지 알려주는 짧은 “지시문” 입니다\n",
        "- T5는 Multi-Task 방식으로 학습\n",
        "  - 요약) summarize: <문장>\n",
        "  - 번역) translate English to German: <문장>\n",
        "  - 감정분류) sentiment: <문장>\n",
        "  - 문법교정) grammar check: <문장>\n",
        "  - 질문생성) generate question: <문장>\n",
        "\n",
        "- 모델\t파라미터 수\t특징\n",
        "|모델|파라미터수|특징|\n",
        "|---|------|---|\n",
        "|t5-small |\t약 6억 2천만 |\t작고 가벼움, 메모리/속도 빠름, 성능 낮음, 긴 문장/복잡한 task에 약함 |\n",
        "|t5-base |\t약 22억\t| t5-small보다 크고 성능 좋음, 긴 문장/복잡한 task 처리 가능, 속도 느림|\n",
        "|t5-large\t| 약 77억\t| 더 크고 정확도 높음, 큰 GPU 필요|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTd9GxwyJuRl",
        "outputId": "c80f95ac-9315-4cee-aa4c-39b952546860"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdc8e9780a5d49a48797488f0e87b186",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05a6fc6add6b4277ad3582c5b9217d0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b076b7bb2a0408d9ea3652ba2454a9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f44882b5cf3c43ad852878c7fc056704",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "토큰수 : 512\n",
            "텐서형태 : torch.Size([1, 512]), 배치크기=1, 시퀀스길이=512\n",
            "첫 10개토큰 : [21603, 3, 10, 368, 1060, 41, 254, 17235, 61, 10555]\n",
            "첫 20개토큰을 디코딩 : summarize : New York (CNN)When Liana Barrientos was 23 years old\n"
          ]
        }
      ],
      "source": [
        "#T5 “입력 준비 + 토크나이즈 확인” 과정만 보여주는 코드 (입력전처리 과정을 설명하는 예)\n",
        "\n",
        "MODEL_NAME = 't5-small'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# gpu 이동 (가능하면)\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "model=model.to(device)\n",
        "\n",
        "# 전처리\n",
        "# 1. 공백제거\n",
        "# 2. 줄바꿈을 공백으로 변환(모델은 줄바꿈을 잘 처리 못함)\n",
        "# 3. Task Prefix 추가 !!필수!!\n",
        "preprocess_text = ARTICLE.strip().replace(\"\\n\", \" \")\n",
        "input_text = f\"summarize : {preprocess_text}\"  # Task Prefix : summarize\n",
        "\n",
        "# 토크나이제이션\n",
        "tokenized_text = tokenizer.encode(\n",
        "    input_text, return_tensors='pt', truncation=True, max_length=512\n",
        ").to(device)\n",
        "\n",
        "print(f\"토큰수 : {tokenized_text.shape[1]}\")\n",
        "print(f\"텐서형태 : {tokenized_text.shape}, 배치크기=1, 시퀀스길이={tokenized_text.shape[1]}\")\n",
        "print(f\"첫 10개토큰 : {tokenized_text[0][:10].tolist()}\")\n",
        "\n",
        "# 디코딩 (숫자->텍스트)\n",
        "decoded_sample = tokenizer.decode(tokenized_text[0][:20], skip_special_tokens=False)\n",
        "print(f\"첫 20개토큰을 디코딩 : {decoded_sample }\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARmckxf8T8LG"
      },
      "source": [
        "### BEAM 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu5TCa8nTHEo",
        "outputId": "eebd8b34-d132-4e80-d263-667a7fc63c78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a year later, she got married again in westchester county, new york. she was\n",
            "요약통계\n",
            "원문의 길이 : 2298 문자\n",
            "요약문의 길이 :  201문자\n",
            "압축률 : 8.75%\n",
            "생성된 토큰수  : 51개\n",
            "요약문 : in 2010, she married once more, this time in the Bronx. she is facing two criminal counts of \"offering a false instrument for filing in the first degree\" the marriages were part of an immigration scam.\n"
          ]
        }
      ],
      "source": [
        "# 실제 요약생성1. T5 만으로만 요약\n",
        "# T5+기본 Greedy(단순argmax)\n",
        "# 장점: 코드 간단, 속도 빠름\n",
        "# 단점: 매번 가장 가능성 높은 토큰만 선택 → 반복적이거나 단순한 문장 가능\n",
        "# 길고 복잡한 문장 요약에는 자연스러운 문장 생성이 제한될 수 있음\n",
        "\n",
        "input_text_t5 = f\"summarize: {ARTICLE}\"\n",
        "inputs_t5 = tokenizer(input_text_t5, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "summary_ids_t5 = model.generate(inputs_t5[\"input_ids\"])\n",
        "summary_t5 = tokenizer.decode(summary_ids_t5[0], skip_special_tokens=True)\n",
        "print(summary_t5)\n",
        "\n",
        "\n",
        "# 실제 요약생성2. T5 + Beam Search\n",
        "# Beam Search 사용 : 여러가능성을 동시에 탐색하면서 최적의 요약을 찾기\n",
        "# 장점: 여러 후보 시퀀스를 동시에 고려 → 더 자연스럽고 정보가 잘 담긴 요약 가능\n",
        "# 반복되는 n-gram 방지, 길이 제한, early stopping 등 세밀하게 제어 가능\n",
        "# 단점: Greedy보다 느림, Beam 수가 크면 메모리/연산 부담 ↑\n",
        "\n",
        "# beam=4, min_length=30, max_length=100\n",
        "summary_ids = model.generate(\n",
        "    tokenized_text,\n",
        "    num_beams=4,      # Beam Search\n",
        "    no_repeat_ngram_size=3, #3-gram 반복 방지\n",
        "    min_length=30,\n",
        "    max_length=100,\n",
        "    early_stopping=True  #EOS 만나면 종료\n",
        ")\n",
        "# 생성완료\n",
        "# 디코딩\n",
        "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(f'요약통계')\n",
        "print(f\"원문의 길이 : {len(ARTICLE):4d} 문자\")\n",
        "print(f\"요약문의 길이 : {len(output):4d}문자\")\n",
        "print(f\"압축률 : {len(output)/len(ARTICLE)*100:.2f}%\")\n",
        "print(f\"생성된 토큰수  : {summary_ids.shape[1]}개\")\n",
        "print(f\"요약문 : {output}\")\n",
        "\n",
        "\n",
        "\n",
        "# 결론\n",
        "# 짧고 간단한 요약이면 (T5)Greedy만으로도 충분\n",
        "# 길고 중요한 정보를 놓치면 안 되는 뉴스 기사, 논문 요약 등은 Beam Search 권장\n",
        "# Beam 수(num_beams)는 보통 2~5 정도가 적절\n",
        "# 필요에 따라 no_repeat_ngram_size, min_length, max_length 같은 옵션으로 제어 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Zp_9xuV868"
      },
      "outputs": [],
      "source": [
        "# T5모델\n",
        "# Task prefix --> 다양하게 적용가능함\n",
        "# T5는 Multi-Task 방식으로 학습\n",
        "# 요약) summarize: <문장>\n",
        "# 번역) translate English to German: <문장>\n",
        "# 감정분류) sentiment: <문장>\n",
        "# 문법교정) grammar check: <문장>\n",
        "# 질문생성) generate question: <문장>\n",
        "\n",
        "# 모델\t파라미터 수\t특징\n",
        "# t5-small\t약 6억 2천만\t작고 가벼움, 메모리/속도 빠름, 성능 낮음, 긴 문장/복잡한 task에 약함\n",
        "# t5-base\t약 22억\tt5-small보다 크고 성능 좋음, 긴 문장/복잡한 task 처리 가능, 속도 느림\n",
        "# t5-large\t약 77억\t더 크고 정확도 높음, 큰 GPU 필요\n",
        "\n",
        "# T5 Task prefix 참조\n",
        "https://www.reddit.com/r/MachineLearning/comments/st97z9/d_where_can_i_find_a_list_of_t5_tasksprefixes/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\playdata2\\miniconda3\\envs\\P10\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "from transformers import BertModel\n",
        "\n",
        "print(inspect.getfile(BertModel))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "P10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
