{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1-1 ë°ì´í„°\n",
    "\n",
    "# í•œêµ­ì–´ ì˜ì–´ ë²ˆì—­ ë°ì´í„°ì…‹\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# ì‹¤ì œí”„ë¡œì íŠ¸ AI hub ê¸°íƒ€ ë“±ë“±\n",
    "korean_sentences = [\n",
    "    \"ì•ˆë…•í•˜ì„¸ìš”\",\n",
    "    \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ìš”\",\n",
    "    \"ì €ëŠ” í•™ìƒì…ë‹ˆë‹¤\",\n",
    "    \"ì´ê²ƒì€ ì‚¬ê³¼ì…ë‹ˆë‹¤\",\n",
    "    \"ê³ ì–‘ì´ê°€ ìê³  ìˆì–´ìš”\",\n",
    "    \"ë‚´ì¼ ë¹„ê°€ ì˜¬ê¹Œìš”\",\n",
    "    \"ì €ëŠ” ì»¤í”¼ë¥¼ ì¢‹ì•„í•´ìš”\",\n",
    "    \"ê·¸ëŠ” ì˜ì‚¬ì…ë‹ˆë‹¤\",\n",
    "    \"ì´ ì±…ì€ ì¬ë¯¸ìˆì–´ìš”\",\n",
    "    \"ìš°ë¦¬ëŠ” ì¹œêµ¬ì˜ˆìš”\"\n",
    "]\n",
    "\n",
    "english_sentences = [\n",
    "    \"hello\",\n",
    "    \"the weather is nice today\",\n",
    "    \"i am a student\",\n",
    "    \"this is an apple\",\n",
    "    \"the cat is sleeping\",\n",
    "    \"will it rain tomorrow\",\n",
    "    \"i like coffee\",\n",
    "    \"he is a doctor\",\n",
    "    \"this book is interesting\",\n",
    "    \"we are friends\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69f332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"e'\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "# unicode ì •ê·œí™”, íŠ¹ìˆ˜ë¬¸ìì²˜ë¦¬\n",
    "# NFD : Normalization form Decomposition --> ë¶„í•´ê°€ëŠ¥í•œ ëª¨ë“  ë¬¸ìë¥¼ ë¶„í•´í•œë‹¤\n",
    "import unicodedata\n",
    "unicodedata.normalize('NFD', \"e'\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2d7e8",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px\">\n",
    "\n",
    "- $\\acute{e}$  --->  ë‹¨ì¼ë¬¸ì U+00E9\n",
    "- $e$ + ' \n",
    "- $e$ + ì•…ì„¼íŠ¸ \n",
    "- unicode ì •ê·œí™” : ê°™ì€ ë¬¸ì¥ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ë¥¸ ë°”ì´íŠ¸ ì¡°í•©ì„ ê°–ëŠ” ê²½ìš° --> ë™ì¼í•œ ë‚´ë¶€ í‘œí˜„ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê³¼ì • \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1-2 í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "### ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_stentence(sentence, is_korean = False):\n",
    "    '''\n",
    "    Unicode ì •ê·œí™”, íŠ¹ìˆ˜ë¬¸ìì²˜ë¦¬\n",
    "    Arg : \n",
    "        sentence : ì›ë³¸\n",
    "        is_korean : í•œêµ­ì–´ ì—¬ë¶€\n",
    "    return : \n",
    "        ì „ì²˜ë¦¬ëœ ë¬¸ì¥\n",
    "    '''\n",
    "    sentence = unicodedata.normalize('NFD', sentence)\n",
    "    if not is_korean:\n",
    "        sentence = sentence.lower()\n",
    "    sentence = sentence.strip() #ë¬¸ìì—´ ì‹œì‘, ë ë¶€ë¶„, ì¤„ë°”ê¿ˆ ë¬¸ì(\\n), íƒ­(\\t) ì œê±°\n",
    "    # ì •ê·œì‹ìœ¼ë¡œ íŠ¹ìˆ˜ë¬¸ì ì „í›„ì— ê³µë°±ì¶”ê°€\n",
    "    # ì•ˆë…•í•˜ì„¸ìš”! --> ì•ˆë…•í•˜ì„¸ìš” !\n",
    "    # r\"([? ! , .])\" # --> ë¬¸ìì¤‘ì— ? ! , . ë‚˜ì˜¤ë©´ ë¬¸ìë¥¼ ê·¸ë£¹ìœ¼ë¡œ ìº¡ì³\n",
    "    # r\"\\1\" ìº¡ì³í•œ ë¬¸ì(\\1) ì•ë’¤ë¡œ ê³µë°±ì„ í•˜ë‚˜ì”© ë„£ëŠ”ë‹¤\n",
    "    sentence = re.sub(r\"([? ! , .])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]', \" \", sentence)\n",
    "    # ì‹œì‘ ì¢…ë£Œ í† í° ì¶”ê°€\n",
    "    sentence = '<start>' + sentence + '<end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a255e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì–´ ì „ì²˜ë¦¬ ê²°ê³¼ : <start>á„‹á…¡á†«á„‚á…§á†¼á„’á…¡á„‰á…¦á„‹á…­<end> \n",
      "ì˜ì–´ ì „ì²˜ë¦¬ ê²°ê³¼ : <start>hello<end>\n"
     ]
    }
   ],
   "source": [
    "##### 1-2 í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©\n",
    "kor_processed = [ preprocess_stentence(kor) for kor in korean_sentences]  \n",
    "eng_processed =[ preprocess_stentence(eng) for eng in english_sentences]\n",
    "\n",
    "print (f'í•œêµ­ì–´ ì „ì²˜ë¦¬ ê²°ê³¼ : {kor_processed[0]} \\nì˜ì–´ ì „ì²˜ë¦¬ ê²°ê³¼ : {eng_processed[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f7e6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1-3 í† í¬ë‚˜ì´ì €\n",
    "# ë¬¸ì¥ì„ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "def create_tokenizer (sentence):\n",
    "    '''\n",
    "    ë‹¨ì–´ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "    vocabulary êµ¬ì¶•, word to index mapping\n",
    "    Args:\n",
    "        sentence : ë¬¸ì¥ë¦¬ìŠ¤íŠ¸\n",
    "    returns:\n",
    "        Tokenizer : keras Tokenizer ê°ì²´\n",
    "    '''\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters='', #í•„í„° ë¹„í™œì„±í™”\n",
    "        oov_token='<unk>'\n",
    "    )\n",
    "    # ëª¨ë“  ë¬¸ì¥ìœ¼ë¡œ ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•\n",
    "    tokenizer.fit_on_texts(sentence)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ec1b619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì–´ ì‚¬ì „ í¬ê¸° : 25 \n",
      "ì˜ì–´ ì‚¬ì „ í¬ê¸° : 30\n"
     ]
    }
   ],
   "source": [
    "##### 1-3 í† í¬ë‚˜ì´ì € ì ìš©\n",
    "# í•œêµ­ì–´ ì˜ì–´ ê°ê°ì˜ í† í¬ë‚˜ì´ì € ìƒì„±\n",
    "kor_tokenizer = create_tokenizer(kor_processed)\n",
    "eng_tokenizer = create_tokenizer(eng_processed)\n",
    "kor_tokenizer\n",
    "eng_tokenizer\n",
    "\n",
    "# ë‹¨ì–´ ì‚¬ì „ í¬ê¸° í™•ì¸\n",
    "kor_vocab_size = len(kor_tokenizer.word_index)+1\n",
    "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
    "print(f'í•œêµ­ì–´ ì‚¬ì „ í¬ê¸° : {kor_vocab_size} \\nì˜ì–´ ì‚¬ì „ í¬ê¸° : {eng_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1-4 ì •ìˆ˜ ì‹œí€€ìŠ¤ ë³€ê²½ (encode)\n",
    "def encode_sentence(tokenizer, sentences, maxlen):\n",
    "    '''\n",
    "    ë¬¸ì¥ì„ ê³ ì •ê¸¸ì´ì˜ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "    padding : ì§§ì€ ë¬´ì¥ì„ ë™ì¼ ê¸¸ì´ë¡œ ë§ì¶”ëŠ”\n",
    "    Arg:\n",
    "        tokenizer \n",
    "        sentence : ë¬¸ì¥ë¦¬ìŠ¤íŠ¸\n",
    "        maxlen : ê°€ì¥ ê¸´ ë¬¸ì¥\n",
    "    returns:\n",
    "        íŒ¨ë”©ëœ ì ìˆ˜ ì‹œí€€ìŠ¤ ë°°ì—´ tfì˜ pad_sequence\n",
    "    '''\n",
    "    sequence = tokenizer.texts_to_sequences(sentences) # ì˜ˆ) [i love you] --> [1,5,7]\n",
    "\n",
    "    # ê¸¸ì´ ë§ì¶”ê¸° (íŒ¨ë”© ì¶”ê°€)\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequence,\n",
    "            maxlen = maxlen,\n",
    "            padding = 'post'\n",
    "    )\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95573d0e",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px\">\n",
    "\n",
    "#### íŒ¨ë”©ë°©ì‹\n",
    "#### ğŸ”¹ RNN/LSTM/GRU vs Transformer íŒ¨ë”© ë°©ì‹ ì •ë¦¬\n",
    "##### âœ… 1) RNN / LSTM / GRU â€” `padding='pre'`\n",
    "- RNN ê³„ì—´ì€ **ì™¼ìª½ â†’ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì½ëŠ” êµ¬ì¡°**.\n",
    "- ë§ˆì§€ë§‰ ì‹œì ì˜ hidden state(`h_last`)ê°€ **ì „ì²´ ë¬¸ì¥ì„ ìš”ì•½í•˜ëŠ” í•µì‹¬ ì •ë³´**ê°€ ë¨.\n",
    "- ê·¸ë˜ì„œ **ì§„ì§œ ë‹¨ì–´ë“¤ì´ ë’¤ìª½ì— ìœ„ì¹˜í•˜ë„ë¡** ì•ìª½ì— íŒ¨ë”©ì„ ë„£ëŠ” `pre-padding`ì„ ì‚¬ìš©.\n",
    "  - ì˜ˆ: `[PAD, PAD, ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3]`\n",
    "- ì´ë ‡ê²Œ í•˜ë©´ RNNì´ **ë§ˆì§€ë§‰ì— ì½ëŠ” ë¶€ë¶„ì´ ì‹¤ì œ ë‹¨ì–´**ê°€ ë˜ì–´  \n",
    "  `h_last`ê°€ ë¬¸ë§¥ ì •ë³´ë¥¼ ì •í™•íˆ ë‹´ì„ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "\n",
    "##### âœ… 2) Transformer â€” `padding='post'`\n",
    "- TransformerëŠ” RNNì²˜ëŸ¼ ìˆœì°¨ì ìœ¼ë¡œ ì½ì§€ ì•Šê³ ,  \n",
    "  **ì „ì²´ ë‹¨ì–´ë¥¼ ë™ì‹œì— ë³´ë©´ì„œ Attentionì„ ê³„ì‚°**.\n",
    "- ë¬¸ë§¥ ìš”ì•½ì´ \"ë§ˆì§€ë§‰ ë‹¨ì–´\"ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ.\n",
    "- ìœ„ì¹˜ ì •ë³´ëŠ” **positional encoding**ìœ¼ë¡œ í•´ê²°ë˜ë¯€ë¡œ  \n",
    "  ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ìˆœì„œë¥¼ ìœ ì§€í•˜ëŠ” `post-padding`ì´ ì í•©.\n",
    "  - ì˜ˆ: `[ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3, PAD, PAD]`\n",
    "- ë’¤ìª½ íŒ¨ë”©ì€ self-attention ê³„ì‚° ì‹œ **maskingìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ ë¬´ì‹œë¨**.\n",
    "\n",
    "\n",
    "##### ğŸ¯ í•œì¤„ ìš”ì•½\n",
    "- **RNN ê³„ì—´ â†’ ë§ˆì§€ë§‰ hidden stateê°€ ì¤‘ìš” â†’ `pre-padding`**  \n",
    "- **Transformer â†’ ìˆœì°¨ ì²˜ë¦¬ ì•„ë‹˜ + masking ê°€ëŠ¥ â†’ `post-padding`**\n",
    "\n",
    "\n",
    "\n",
    "| ëª¨ë¸               | íŒ¨ë”© ë°©ì‹            | ì´ìœ                                                                |\n",
    "| ---------------- | ---------------- | ---------------------------------------------------------------- |\n",
    "| **RNN/LSTM/GRU** | **pre-padding**  | ì¤‘ìš”í•œ ë‹¨ì–´ê°€ ë’¤ìª½ì— ì˜¤ë„ë¡ í•´ì„œ **ë§ˆì§€ë§‰ hidden state**ê°€ ë¬¸ë§¥ ìš”ì•½ì„ ì˜ ê°–ë„ë¡            |\n",
    "| **Transformer**  | **post-padding** | ìˆœì°¨ ì²˜ë¦¬ ì—†ìŒ. ìœ„ì¹˜ ì •ë³´ëŠ” positional embeddingìœ¼ë¡œ í•´ê²°ë˜ë¯€ë¡œ ì•/ë’¤ ìˆœì„œë§Œ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ ë¨ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4979e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì–´ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ : 5 \n",
      "ì˜ì–´ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ : 5\n",
      "====================================\n",
      "í•œêµ­ì–´ ì „ì²˜ë¦¬ ê²°ê³¼ : <start>á„‹á…¡á†«á„‚á…§á†¼á„’á…¡á„‰á…¦á„‹á…­<end> \n",
      "ì˜ì–´ ì „ì²˜ë¦¬ ê²°ê³¼ : <start>hello<end>\n",
      "í•œêµ­ì–´ ì¸ì½”ë”© ì²«ë²ˆì§¸ ë¬¸ì¥ : [3 0 0] \n",
      "ì˜ì–´ ì¸ì½”ë”© ì²«ë²ˆì§¸ ë¬¸ì¥: [7 0 0 0 0]\n",
      "====================================\n",
      "ë°ì´í„°ì…‹ ì¤€ë¹„ì™„ë£Œ\n",
      "ì „ì²´ ìƒ˜í”Œìˆ˜ : 10\n",
      "ë°°ì¹˜ í¬ê¸° : 2\n",
      "ë°°ì¹˜ ìˆ˜ : 5\n"
     ]
    }
   ],
   "source": [
    "# ìµœëŒ€ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ ê²°ì •(ê°€ì¥ ê¸´ ë¬¸ì¥ê¸°ì¤€)\n",
    "max_kor_len = max( len(s.split()) for s in kor_processed)\n",
    "max_eng_len = max( len(s.split()) for s in eng_processed)\n",
    "print(f'í•œêµ­ì–´ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ : {max_eng_len} \\nì˜ì–´ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ : {max_eng_len}')\n",
    "print('====================================')\n",
    "\n",
    "\n",
    "##### 1-4 ì •ìˆ˜ ì‹œí€€ìŠ¤ ë³€ê²½ (encode) ì¸ì½”ë”© ìˆ˜í–‰\n",
    "kor_tensor = encode_sentence(kor_tokenizer, kor_processed, max_kor_len)\n",
    "eng_tensor = encode_sentence(eng_tokenizer, eng_processed, max_eng_len)\n",
    "print (f'í•œêµ­ì–´ ì „ì²˜ë¦¬ ê²°ê³¼ : {kor_processed[0]} \\nì˜ì–´ ì „ì²˜ë¦¬ ê²°ê³¼ : {eng_processed[0]}')\n",
    "print(f'í•œêµ­ì–´ ì¸ì½”ë”© ì²«ë²ˆì§¸ ë¬¸ì¥ : {kor_tensor[0]} \\nì˜ì–´ ì¸ì½”ë”© ì²«ë²ˆì§¸ ë¬¸ì¥: {eng_tensor[0]}')\n",
    "print('====================================')\n",
    "\n",
    "\n",
    "# tensorflow Dataset ê°ì²´\n",
    "# ë°°ì¹˜ì²˜ë¦¬ ì™€ ì…”í”Œ\n",
    "BUFFER_SIZE = len(kor_tensor)\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices( (kor_tensor, eng_tensor))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print('ë°ì´í„°ì…‹ ì¤€ë¹„ì™„ë£Œ')\n",
    "print(f'ì „ì²´ ìƒ˜í”Œìˆ˜ : {len(kor_tensor)}')\n",
    "print(f'ë°°ì¹˜ í¬ê¸° : {BATCH_SIZE}')\n",
    "print(f'ë°°ì¹˜ ìˆ˜ : {len(kor_tensor) // BATCH_SIZE}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f758eb7",
   "metadata": {},
   "source": [
    "<span style=\"font-size:12px\">\n",
    "\n",
    "\n",
    "----------------- step2 -------------------------------------\n",
    "\n",
    "- seq2seq êµ¬ì¡°\n",
    "- Encoder\n",
    "    - ì…ë ¥ë¬¸ì¥ --> Embedding --> LSTM + Hidden States\n",
    "- Decoder\n",
    "    - <start> í† í° --> Embedding --> LSTM + context --> ë‹¨ì–´ì˜ˆì¸¡\n",
    "<br>    ì˜ˆì¸¡ë‹¨ì–´ --> ë‹¤ìŒì…ë ¥ --> ë°˜ë³µ(<end> ë‚˜ì˜¬ë•Œê¹Œì§€)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5532c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder í´ë˜ìŠ¤\n",
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    ì…ë ¥ë¬¸ì¥ì„ ê³ ì°¨ì› ë²¡í„°ë¡œ ì••ì¶•\n",
    "    Embedding --> LSTM --> Hidden States ì¶œë ¥\n",
    "    êµ¬ì¡° : \n",
    "        ì…ë ¥(ì •ìˆ˜ì‹œí€€ìŠ¤)  --> Embedding --> LSTM --> ëª¨ë“  íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥\n",
    "    '''\n",
    "    def __init__ (self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        '''\n",
    "        Arg:\n",
    "            vocab_size : ë‹¨ì–´ì‚¬ì „ í¬ê¸°(ì„ë² ë”© í…Œì´ë¸” í¬ê¸°)\n",
    "            embedding_dim : ì„ë² ë”© ë²¡í„° ì°¨ì› (ë°©í–¥ì´ ìˆëŠ” ë²¡í„°ë¡œ ë§Œë“¬)\n",
    "            enc_units : LMS ìœ ë‹‰ ìˆ˜ (hidden state ì°¨ì›)\n",
    "            batch_size : ë°°ì¹˜í¬ê¸°\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "\n",
    "        # Embedding Layer : ì •ìˆ˜ --> ë°€ì§‘ë²¡í„°(Denserf Vector) \n",
    "        # ì‘ì› ì°¨ì› ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0ì´ ì•„ë‹˜. ì—°ì†ëœ ì‹¤ìˆ˜ê°’ ê³µê°„ íš¨ìœ¨ì„±\n",
    "        # í•™ìŠµ ê³¼ì •ì„ í†µí•´ ë‹¨ì–´ê°„ ì˜ë¯¸ì  ê´€ê³„ í•™ìŠµ --> ìœ ì‚¬ ë‹¨ì–´ëŠ” ê°€ê¹Œìš´ ë²¡í„° ê³µê°„ì— ìœ„ì¹˜\n",
    "        # ë‹¨ì : í›ˆë ¨í•„ìš”(ì‚¬ì „í•™ìŠµ ë˜ëŠ” ì„ë² ë”©ì„ í•™ìŠµ)\n",
    "\n",
    "        # í¬ì†Œë²¡í„° : ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0 (ëŒ€í‘œì ìœ¼ë¡œ ì›-í•«ë²¡í„°) ì°¨ì›ì´ í¬ë‹¤/ ë‹¨ì–´ê°„ ì¶©ëŒì´ ì—†ë‹¤/ë‹¨ì–´ê°„ ìœ ì‚¬ë„í‘œí˜„ ã…‡ëª»í•¨ \n",
    "        # ë°€ì§‘ë²¡í„° : í•™ìŠµê¸°ë°˜ (Word2Vec Glove Embedding)\n",
    "        # í¬ì†Œë²¡í„° : ê·œì¹™ê¸°ê°„ (ì›í•« Bow) --> ì´ˆì°½ê¸° ìì—°ì–´ ëª¨ë¸\n",
    "\n",
    "        # mask_zero = True íŒ¨ë”©ì— ë§ˆìŠ¤í¬ì²˜ë¦¬ë¥¼ í•´ì„œ ëª¨ë¸ì´ í•´ì„í•˜ì§€ ì•Šê²Œ í•œë‹¤\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, mask_zero = True\n",
    "        )\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            enc_units,\n",
    "            return_sequence = True, #[batch, seq_len, units]\n",
    "            return_state =True, #(output, h, c)\n",
    "            recurrent_initializer = 'glorot_uniform' #ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        )\n",
    "    def call (self, x, hidden):\n",
    "        '''\n",
    "        ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•´ì„œ hidden states ìƒì„±\n",
    "        Args:\n",
    "            x: ì…ë ¥ ì‹œí€€ìŠ¤ [ batch_size, seq_len ]\n",
    "            hidden: ì´ˆê¸° hidden state (ì²« í˜¸ì¶œì‹œ 0 ë²¡í„°)\n",
    "        Returns :\n",
    "            output: ëª¨ë“ íƒ€ì„ìŠ¤í…œí”„ì˜ ì¶œë ¥  [batch, seq_len, enc_units]\n",
    "            state_h : ë§ˆì§€ë§‰ hidden state [batch, enc_units]\n",
    "            state_c : ë§ˆì§€ë§‰ cell state [batch, enc_units]\n",
    "        '''\n",
    "\n",
    "        # 1ë‹¨ê³„ emb\n",
    "        x= self.embedding(x)\n",
    "        output,  state_h, state_c = self.lstm(x,initial_state=hidden)\n",
    "        return output, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00494d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜¸ì¶œ 100\n"
     ]
    }
   ],
   "source": [
    "class ABC():\n",
    "    def __call__(self, data):\n",
    "        self.call(data)\n",
    "\n",
    "    def call(self,data):\n",
    "        print('í˜¸ì¶œ', data)\n",
    "\n",
    "a= ABC()\n",
    "a(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb580449",
   "metadata": {},
   "source": [
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
